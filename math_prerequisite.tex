%\documentclass[12pt,letterpaper]{report} % when using this, pages do not match
\documentclass[12pt,letterpaper]{book}
\usepackage{amsmath,amssymb,mathrsfs,amscd,array,amsthm}
\usepackage[all]{xy}
\usepackage{makeidx}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{soul} % using \ul{blah blah}, an underlined sentence can get wrapped
\usepackage{datetime} % Needed for \currenttime
\usepackage[colorinlistoftodos]{todonotes} % Needed for \todo comments
\usepackage{pgfplots} % Needed for TikZ
\usepackage{bm} %% Needed for bold italic in math mode
\usepackage{tikz} %% For TikZ code
%\usepackage{environ} % Needed for \NewEnviron
\usepackage{comment} % Needed for \excludecomment{environ_name} : any text inside environ_name will be commented out
%\excludecomment{proof}% toggle: shows proof or not: See \usepackage{comment}

\usetikzlibrary{arrows} % Needed for tikz axis arrow shape

%%%% The following is for hyperref in pdf output %%%%
%%%% hyperref package requires pdflatex %%%%
%%%% and it must be the ***** LAST ***** package loaded with few exceptions %%%%
%%%% Use colorlinks=false below when printing %%%%
\usepackage[colorlinks=true, pdfborder={1 1 0}]{hyperref}
\hypersetup
	{
    	colorlinks,%
    	citecolor=red,%
    	filecolor=black,%
    	linkcolor=blue,%
    	urlcolor=green
	}
%%%% end of hyperref package &&&&&

%%% pdfLaTeX margins %%%
\hoffset= -0.5in
\textwidth 6.4in
\textheight 8.4in
\topmargin -0.2in

%%% YAP dvi margin %%%
%\hoffset= -0.5in
%\textwidth 6.15in
%\textheight 8.6in
%\footskip 2in
%\parindent=0pt

\def\red{\textcolor{red}}
\def\green{\textcolor{green}}
\def\blue{\textcolor{blue}}
\def\cyan{\textcolor{cyan}}
\def\yellow{\textcolor{yellow}}

\numberwithin{equation}{section}

\newtheorem{thm}{\textbf{Theorem}}[section]
\newtheorem{prop}[thm]{\textbf{Proposition}}
\newtheorem{coro}[thm]{\textbf{Corollary}}
\newtheorem{lemma}[thm]{\textbf{Lemma}}

\theoremstyle{definition}
\newtheorem{defi}[thm]{\textbf{Definition}}
\newtheorem{problem}[thm]{\textbf{Problem}}
\newtheorem{example}[thm]{\textbf{Example}}
\newtheorem{remark}[thm]{\textbf{Remark}}
\newtheorem{exercise}{\textbf{Exercise}}[chapter]

%\newtheorem*{answer}{\textbf{Answer}}
\newenvironment{answer}{\noindent\textbf{Answer}}{\hfill$\blacksquare$\vspace{0.1in}}

%\newenvironment{remark}{\textbf{Remark}}{\vspace{0.1in}}
%\newenvironment{question}{\textbf{Question}}{\vspace{0.1in}}
%\newenvironment{answer}{\textbf{Example}}{\vspace{0.1in}}


\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\labelenumi}{\theenumi}
%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{theenumi} % suppress the period (.) after the heading
%\renewcommand{\labelenumii}{\roman{enumii}.}

%\newcommand{\tr}{\operatorname{tr}}
\newcommand{\spann}{\operatorname{span}}
\newcommand{\vt}{\bm{t}}
\newcommand{\vu}{\bm{u}}
\newcommand{\vv}{\bm{v}}
\newcommand{\vw}{\bm{w}}
\newcommand{\vx}{\bm{x}}
\newcommand{\vy}{\bm{y}}
\newcommand{\vz}{\bm{z}}
\newcommand{\vb}{\bm{b}}
\newcommand{\vc}{\bm{c}}
\newcommand{\ve}{\bm{e}}
\newcommand{\vp}{\bm{p}}
\newcommand{\vxi}{\bm{\xi}}
\newcommand{\vmu}{\bm{\mu}}
\newcommand{\veta}{\bm{\eta}}
\newcommand{\veczero}{\bm{0}}
\newcommand{\Nul}{\operatorname{Nul}}
\newcommand{\Coll}{\operatorname{Col}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\nullity}{\operatorname{nullity}}
\newcommand{\vA}{\bm{A}}
\newcommand{\vB}{\bm{B}}
\newcommand{\vC}{\bm{C}}
\newcommand{\vQ}{\bm{Q}}
\newcommand{\vI}{\bm{I}}
\newcommand{\vL}{\bm{L}}
\newcommand{\vV}{\bm{V}}
\newcommand{\vS}{\bm{S}}
\newcommand{\vT}{\bm{T}}
\newcommand{\vR}{\bm{R}}
\newcommand{\vW}{\bm{W}}
\newcommand{\vX}{\bm{X}}
\newcommand{\vZ}{\bm{Z}}

\pagestyle{headings}

\makeindex

\begin{document}
\noindent
LAST REVISED: \today \quad \currenttime\\
THE LATEST VERSION IS AVAILABLE AT \url{http://www.mtholyoke.edu/~jjlee/Teaching/math_prerequisite.pdf}
\vspace{3cm}
\begin{center}
{\Huge\textbf{ESSENTIALS}} \bigskip \\ {\large \texttt{OF}} \bigskip \\
{\Huge\textbf{CALCULUS}} \bigskip \\ {\large \texttt{AND}} \bigskip \\
{\Huge\textbf{LINEAR ALGEBRA}} \bigskip \\ {\large \texttt{FOR}} \bigskip \\
{\Huge\textbf{STATISTICS}}
\end{center}

\tableofcontents

\chapter*{Preface}
This monograph was designed as a review of standard results in mathematics for those who are planning to take courses in mathematical statistics. I tried to make it self-contained, but no proofs are given, since almost all results given in the monograph are well known and  proofs are easily accessible on/off line.\\\\
Readers are assumed to have been introduced to calculus, linear algebra, and basic probability to some extent. Since the goal of this monograph is to provide the readers with necessary mathematical background that can help them understand and develop statistical tools, more emphasis was given to intuition and application rather than rigorous treatment of the topics. Whenever appropriate, each section contains examples in statistics that use the material in the section. \\\\
To enhance the understanding of the content, \textbf{Problem} and \textbf{Exercise} are included in addition to \textbf{Theorem}, \textbf{Remark}, and \textbf{Example}. The format of \textbf{Problem} is the same as \textbf{Exercise}, except that the former is followed by a completely worked out \textbf{Answer} ($\blacksquare$ indicates the end of \textbf{Answer}). The readers are strongly encouraged to work on \textbf{Exercise}.

\bigskip
\begin{flushright}
J.-J. Lee
\end{flushright}

\chapter{Basics of Discrete Mathematics}

\section{Terminologies in Set Theory}

Probability is a function defined on a certain collection of sets called events, and set theory lays the foundation for probability theory. In this section, we review basic terminologies of set theory.

\begin{defi}
A \underline{set}\index{set} is an unordered collection of objects. The objects in a set are called the \underline{elements}\index{element}, or \underline{members}\index{member}, of the set.
\end{defi}

\begin{example}\label{set O} The set consisting of all positive odd integers less than $10$ can be expressed by $\{1,3,5,7,9\}$ or $\{1,5,7,9,3\}$. Let $O$ denote this set. To indicate that $1$ is an element of $O$, we use the notation $1\in O$. To indicate that $2$ is not an element of $O$, we write $2\notin O$.
\end{example}

\begin{example} The set consisting of all positive integers less than $10$ can be denoted by $\{1,2,3,4,5,6,7,8,9\}$. $\{1,2,3,\ldots,9\}$ is also used since the general pattern is obvious.
\end{example}

Another way to describe a set is to use \textit{set builder} notation: elements in a given set are characterized by the property or properties they must have to be members of the set. For example, $O=\{1,3,5,7,9\}$ in Example \ref{set O} can also be written as $$O=\{x: x \text{ is a positive odd integer less than } 10\}.$$

\begin{problem} Describe the following sets by listing all the elements or giving general pattern.
\begin{enumerate}
\item $A=\{x: x \text{ is a prime number less than }20\}$.
\item $B=\{n: n \text{ is a positive integer}\}$.
\item $C=\{x: x^2-2x-3=0\}$.
\item $D=\{2n: n \text{ is an integer and }1\leq n\leq 4\}$.
\end{enumerate}
\end{problem}

\begin{answer}\quad
\begin{enumerate}
\item $A=\{2,3,5,7,11,13,17,19\}$.
\item $B=\{1,2,3,4,\ldots\}$.
\item The roots of the equation $x^2-2x-3=0$ are $-1$ and $3$, so $C=\{-1,3\}$.
\item Integers $n$ such that $1\leq n\leq 4$ are $1,2,3,$ and $4$. Since $D$ is the collection of 2 times such $n$'s, $D=\{2,4,6,8\}$.
\end{enumerate}
\end{answer}

Some common notations for basic sets are given below.

\begin{defi}\label{common sets}\quad
\begin{enumerate}
\item $\emptyset$, The \underline{empty set}\index{empty set} (a set with \textit{no} elements). $\{\quad\}$ is also used.
\item $\mathbb{N}=\{1,2,3,\ldots\}$, the set of natural numbers.\index{natural number}\index{$\mathbb{N}$}
\item $\mathbb{Z}=\{\ldots,-2,-1,0,1,2,\ldots\}$, the set of integers. \index{integer}\index{$\mathbb{Z}$}
\item $\mathbb{Q}=\{p/q: p\in \mathbb{Z}, q\in \mathbb{Z}, q\neq 0\}$, the set of rational numbers.\index{rational number}\index{$\mathbb{Q}$}
\item $\mathbb{R}$ or $(-\infty,\infty)$, the set of real numbers. \index{real number}\index{$\mathbb{R}$}
\item $\mathbb{R^+}=\{x: x\in \mathbb{R} \text{ and } x>0\}$, the set of \textit{positive} real numbers.
\item $[a,b]$, the set of all real numbers $x$ such that $a\leq x \leq b$.
\item $(a,b)$, the set of all real numbers $x$ such that $a< x < b$.
\item $(a,b]$, the set of all real numbers $x$ such that $a< x \leq  b$.
\item $[a,b)$, the set of all real numbers $x$ such that $a\leq x< b$.
\item $(-\infty,a)$, the set of all real numbers $x$ such that $x<a$.
\item $[a,\infty)$, the set of all real numbers $x$ such that $x\geq a$.
\end{enumerate}
\end{defi}

One of important notions in set theory is the equality of sets, which is defined in terms of a two-way containment.

\begin{defi}\label{equal set defi} A set $A$ is said to be a \underline{subset}\index{subset} of another set $B$ if and only if every element of $A$ is also an element of $B$. We use the notation $A\subseteq B$. Two sets $A$ and $B$ are said to be \underline{equal} if they have the same elements. In other words, $A$ and $B$ are equal if $A\subseteq B$ and $B\subseteq A$.
\end{defi}

\begin{remark} $\emptyset$ is a subset of $A$ for any set $A$. $A$ is a subset of $A$ itself, for any set $A$.
\end{remark}

\begin{example} By Definition \ref{common sets}, $\mathbb{N}\subseteq \mathbb{Z}\subseteq \mathbb{Q} \subseteq \mathbb{R}$.
\end{example}

\begin{example} $\{1,2,3,4\} \nsubseteq \{1,3,4,5,6,7\}$, because $2\notin \{1,3,4,5,6,7\}$.
\end{example}

\begin{problem}\label{num of subsets} How many distinct subsets does the set $A=\{x,y\}$ have? How about $B=\{p,q,r\}$?
\end{problem}

\begin{answer}
Subsets of $A$ are: $\emptyset,\{x\}, \{y\}$, and $A$ itself, so $A$ has $4$ distinct subsets. \\
Subsets of $B$ are: $\emptyset,\{p\}, \{q\}, \{r\}, \{p,q\}, \{p,r\}, \{q,r\}$, and $B$ itself, so $B$ has $8$ distinct subsets.
\end{answer}

In many cases, Definition \ref{equal set defi} is the only tool available to identify inclusion relation of two sets.

\begin{problem} Let $$A=\{n: n=2k+1 \text{ for some } k\in \mathbb{Z}\}$$
and
$$B=\{m: m \text{ is the difference of two consecutive squares of integers}\}$$
Show that $A=B$.
\end{problem}

\begin{answer}
First we show that $A\subseteq B$. Let $n\in A$, then there is $k\in Z$ such that $n=2k+1$. Since $2k+1=(k+1)^2-k^2$, we conclude that $n\in B$. To show that $B\subseteq A$, let $m\in B$, then $m$ can be written as $m=(\ell+1)^2-\ell^2$ for some $\ell\in \mathbb{Z}$. Since $(\ell+1)^2-\ell^2=2\ell+1$, we see that $m\in A$.
\end{answer}

We can measure the size of a given set by counting the number of elements in it.

\begin{defi} By \underline{cardinality}\index{cardinality} of a set $S$, we mean the number of elements in $S$. The cardinality of $S$ is denoted by $|S|$.
\end{defi}

\begin{example} If $A=\{n: n\text{ is a prime number less than or equal to }10\}=\{2,3,5,7\}$, then $|A|=4$. The cardinality of the set of natural numbers, $\mathbb{N}$, is infinite. We write $|\mathbb{N}|=\infty$.
\end{example}

Recall that in Problem \ref{num of subsets}, $A$ has $2^2=4$ distinct subsets and $B$ has $2^3=8$ distinct subsets. In fact, we have the following theorem (see Exercise \ref{counting exercises}.\ref{counting set} for a proof).

\begin{thm}\label{powerset} If $S$ is a set with $|S|=n$, then $S$ has $2^n$ distinct subsets.
\end{thm}

Now we consider the concept of the product of two sets. We start with ordered $n$-tuples.

\begin{defi} The \underline{ordered $n$-tuple}\index{ordered $n$-tuple} $(a_1,a_2,\ldots,a_n)$ is the ordered collection that has $a_1$ as its first element, $a_2$ as its second element,\ldots, and $a_n$ as its $n^\text{th}$ element. In particular, an ordered $2$-tuple is called an \underline{ordered pair},\index{ordered pair} and an ordered $3$-tuple is called an \underline{ordered triple}.\index{ordered triple}
\end{defi}

\begin{remark} When $a\neq b$, we \textit{distinguish} $(a,b)$ from $(b,a)$. In general, $(a_1,a_2,\ldots, a_n) = (b_1,b_2,\ldots, b_n)$ if and only if $a_1=b_1$, $a_2=b_2$, \ldots, and $a_n=b_n$.
\end{remark}

\begin{defi} Let $A$ and $B$ be sets. The \underline{Cartesian product}\index{Cartesian product} of $A$ and $B$, denoted by $A\times B$, is the set of all ordered pairs $(a,b)$, where $a\in A$ and $b\in B$. In other words, $A\times B=\{(a,b): a\in A \text{ and } b\in B\}$.
\end{defi}

\begin{example}\label{finite cartesian} Let $A=\{a,b\}$ and $B=\{1,2,3\}$. Then \\
$$A\times B=\{(a,1), (a,2), (a,3), (b,1), (b,2), (b,3)\}$$ and
$$B\times A=\{(1,a), (1,b), (2,a), (2,b), (3,a), (3,b)\}.$$
\end{example}

\begin{remark} The Cartesian product can be defined for more than two sets. That is, if $A_1,A_2,\ldots,A_n$ are sets, then the Cartesian product of $A_1,A_2,\ldots,A_n$, denoted by $A_1\times A_2\times \cdots \times A_n$, is defined to be
$$A_1\times A_2\times \cdots \times A_n=\{(a_1,a_2,\ldots,a_n): a_1\in A_1,a_2\in A_2,\ldots,a_n\in A_n\}.$$
\end{remark}

\begin{example}
The Euclidean $n$-space\index{Euclidean $n$-space} $\mathbb{R}^n$ is defined to be
$$\mathbb{R}^n=\underbrace{\mathbb{R}\times \mathbb{R}\times \cdots \times \mathbb{R}}_{n \text{ times}}.$$ In other words,
$$\mathbb{R}^n=\{(x_1,x_2,\ldots,x_n): x_i \in \mathbb{R} \text{ for all }i\}.$$
\end{example}

\begin{remark}
Note that $6=|A\times B|=|A||B|=2\times 3$ in Example \ref{finite cartesian}. In general, if all $A_i$'s have finitely many elements, then
$$|A_1\times A_2 \times \cdots \times A_n|=|A_1||A_2|\cdots|A_n|.$$ See Example \ref{prod rule cartesian}.
\end{remark}

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item Describe the following sets by listing their elements:
\begin{enumerate}
\item $A=\{n: n^2-n-12\leq 0 \text{ and } n\in \mathbb{Z}\}$.
\item $B=\{x: \log_2(x+1)+\log_2(x-1)=3\}$.
\end{enumerate}

\item Let $$A=\{n: n=k^2 \text{ for some }k\in \mathbb{Z}\}$$ and $$B=\{m: m=3\ell \text{ or }m=3\ell+1 \text{ for some }\ell\in \mathbb{Z}\}.$$ Show that $A\subseteq B$ and $B\nsubseteq A$.\\
    \textit{Hint}: To show that $A\subseteq B$, note that if $k\in \mathbb{Z}$, then $k$ must be of the form $k=3p$, $k=3p+1$, or $k=3p-1$, where $p\in \mathbb{Z}$. To show that $B\nsubseteq A$, it is enough to find an element $b\in B$ such that $b\notin A$.

\item Choose all that are true:
\begin{enumerate}
\item $0\in \{0\}$
\item $0\in \emptyset$
\item $0\subseteq \{0\}$
\item $\emptyset\subseteq \emptyset$
\item $\emptyset\in \{\emptyset\}$
\item $\emptyset\subseteq \{\emptyset\}$
\item $\{1,2,3\}\subseteq \{1,2,3,4,5\}$
\item $\{1\}\in \{1,2,3,4,5\}$
\item $1\subseteq \{1,2,3,4,5\}$
\item $\{1\}\in \{\{1\},2,3,\{2,3\}\}$
\item $\{2,3\}\in \{\{1\},2,3,\{2,3\}\}$
\item $\{2,3\}\subseteq \{\{1\},2,3,\{2,3\}\}$
\end{enumerate}

\item Find the Cartesian product of $A\times B\times C$, where $A=\{0,1\}$, $B=\{1,2\}$, and $C=\{0,1,2\}$.
\end{enumerate}
\end{exercise}

\section{Set Operations}

In this section, we consider various set operations as a way of constructing new sets from old ones, and review some properties of these operations.

\begin{defi} Let $A$ and $B$ be sets.
\begin{enumerate}
\item The \underline{union}\index{union} of $A$ and $B$, denoted by $A\cup B$, is the set that contains those elements that are either in $A$ or $B$, or in both. In other words, $A\cup B=\{x: x\in A \text{ or } x\in B\}$.
\item The \underline{intersection}\index{intersection} of $A$ and $B$, denoted by $A\cap B$, is the set that contains those elements in both $A$ and $B$. In other words, $A\cap B=\{x: x\in A \text{ and } x\in B\}$.
\item Two sets are said to be \underline{disjoint}\index{disjoint} if their intersection is the empty set.
\end{enumerate}
\end{defi}

\begin{example} Let $A=\{1,3,4,5\}$ and $B=\{1,2,3\}$, then $A\cup B=\{1,2,3,4,5\}$ and $A\cap B=\{1,3\}$.
\end{example}

\begin{example}
If $S=(-\infty, 2)$ and $T=[-1,\infty)$, then $S\cap T=[-1,2)$ and $S\cup T=(-\infty,\infty)=\mathbb{R}$.
\end{example}

\begin{example}\label{oddeven} Let $O=\{2k+1: k\in \mathbb{Z}\}$ and $E=\{2k: k\in \mathbb{Z}\}$, then $O$ and $E$ are disjoint.
\end{example}

Obviously, $A\cup B$ contains both $A$ and $B$, so $|A\cup B|\geq |A|$ and $|A\cup B|\geq |B|$. But exactly how many elements are there in $A\cup B$?

\begin{thm}\label{union number} Let $A,B$ be sets with finitely many elements, then
$$|A\cup B|=|A|+|B|-|A\cap B|.$$ In particular, if $A$ and $B$ are disjoint, then $|A\cup B|=|A|+|B|$.
\end{thm}

Note that in Theorem \ref{union number}, $|A\cap B|$ is subtracted because elements in $A\cap B$ are double counted in $|A|+|B|$.

\begin{problem} Let $A$ and $B$ be subsets of $C$. Suppose that $|C|=50$, $|A|=30$, and $|B|=40$. Prove that
$20\leq |A\cap B|\leq 30$.
\end{problem}

\begin{answer}
Since $A\cup B\subseteq C$, $|A\cup B|\leq |C|=50$. Since $|A\cap B|=|A|+|B|-|A\cup B|=70-|A\cup B|$, it follows that $|A\cap B|\geq 70-50=20$. On the other hand, since $A\cap B \subseteq A$, it follows that $|A\cap B|\leq |A|=30$.
\end{answer}

\begin{defi} Let $A$ and $B$ be sets.
\begin{enumerate}
\item The \underline{difference}\index{difference of sets} of $A$ and $B$, denoted by $A-B$ or $A\setminus B$, is the set containing those elements that are in $A$ but not in $B$. In other words, $A-B=\{x: x\in A \text{ and } x\notin B\}$.
\item The \underline{universal set}\index{universal set}, denoted by $U$, is the set of all objects under consideration in a given context.
\item The \underline{complement}\index{complement} of $A$, denoted by $A^c$, is $U-A$. In other words, $A^c=\{x: x\notin A\}$.
\end{enumerate}
\end{defi}

\begin{example} Let $U=\{1,2,3,4,5,6,7,8,9,10\}$, $A=\{1,2,3,4,5\}$, and $B=\{3,5,7,9\}$. Then $A-B=\{1,2,4\}$, $A^c=\{6,7,8,9,10\}$, and $B^c=\{1,2,4,6,8,10\}$.
\end{example}

\begin{example} Let $O$ and $E$ be as in Example \ref{oddeven}. If $U=\mathbb{Z}$, then we see that $O^c=E$ and $E^c=O$.
\end{example}

\begin{problem} Prove that $A-B=A\cap B^c$.
\end{problem}

\begin{answer}
$x\in A-B$ if and only if $x\in A$ and $x\notin B$. In other words, $x\in A-B$ if and only if $x\in A$ and $x\in B^c$. This shows that $A-B=A\cap B^c$.
\end{answer}

\begin{remark} Some properties of set operations are summarized in the following table:
\begin{center}
\begin{tabular}{|m{7cm}||m{5cm}|} \hline
\begin{center} Identity \end{center} & \begin{center}Name\end{center}\\
\hline\hline
\begin{center}  $A\cap U=A$ \\ $A\cup \emptyset=A$   \end{center} & \begin{center}   Identity laws  \end{center}  \\
\hline
\begin{center}   $A\cup U=U$ \\ $A\cap \emptyset=\emptyset$  \end{center} & \begin{center} Domination laws  \end{center}  \\
\hline
\begin{center}  $A\cup A=A$ \\ $A\cap A=A$   \end{center} & \begin{center} Idempotent laws  \end{center}  \\
\hline
\begin{center}  $(A^c)^c = A$ \end{center} & \begin{center} Complementation law  \end{center}  \\
\hline
\begin{center}  $A \cup B =B\cup A$ \\ $A\cap B=B\cap A$   \end{center} & \begin{center} Commutative laws\index{Commutative laws}  \end{center}  \\
\hline
\begin{center}  $A\cup (B\cup C)=(A\cup B)\cup C$ \\ $A\cap (B\cap C)=(A\cap B)\cap C$ \end{center} & \begin{center} Associative laws\index{Associative laws}  \end{center}  \\
\hline
\begin{center}  $A\cup (B\cap C)=(A\cup B)\cap (A\cup C)$ \\ $A\cap (B\cup C)=(A\cap B)\cup (A\cap C)$ \end{center} & \begin{center} Distributive laws\index{Distributive laws}  \end{center}  \\
\hline
\begin{center} $(A\cup B)^c=A^c\cap B^c$ \\ $(A\cap B)^c=A^c\cup B^c$ \end{center} & \begin{center} De Morgan's laws\index{De Morgan's laws}  \end{center}  \\
\hline
\begin{center}  $A \cup (A \cap B) = A $ \\ $A \cap (A \cup B) = A$ \end{center} & \begin{center} Absorption laws  \end{center}  \\
\hline
\begin{center}  $A \cup A^c = U$ \\ $A \cap A^c =\emptyset$ \end{center} & \begin{center} Complement laws  \end{center}  \\
\hline
\end{tabular}
\end{center}
\end{remark}
\newpage

\begin{problem}
Show that $(A-B)\cup B=A\cup B$.
\end{problem}

\begin{answer}
$(A-B)\cup B=(A\cap B^c)\cup B=(A\cup B)\cap \blue{(B^c\cup B)}=(A\cup B)\cap \blue{U}=A\cup B$.
\end{answer}

One can define union and intersection for more than two sets.
\begin{defi} Let $A_1,A_2,\ldots,A_n$ be sets. Then
\begin{enumerate}
\item the union $\displaystyle{\bigcup_{i=1}^n A_i}=A_1\cup\cdots\cup A_n$ is the set containing those elements that are members of at least one of $A_i$, $1\leq i\leq n$.
\item the intersection $\displaystyle{\bigcap_{i=1}^n A_i}=A_1\cap\cdots\cap A_n$ is the set containing those elements that are members of $A_i$ for all $i$, $1\leq i \leq n$.
\item the union $\displaystyle{\bigcup_{i=1}^\infty A_i}=A_1\cup A_2\cup \cdots $ is the set containing those elements that are members of at least one of $A_i$, $i\geq 1$.
\item the intersection $\displaystyle{\bigcap_{i=1}^\infty A_i}=A_1\cap A_2\cap \cdots$ is the set containing those elements that are members of $A_i$ for all $i$, $i\geq 1$.
\end{enumerate}
\end{defi}

\begin{problem} For $i=1,2,\ldots$, define $A_i=\{x: 0\leq x <\frac{1}{i}\}$. What is $\displaystyle{\bigcap_{i=1}^\infty A_i}$?
\end{problem}

\begin{answer}
We claim that $\displaystyle{\bigcap_{i=1}^\infty A_i}=\{0\}$. Clearly $0\in A_i$ for every $i$, so it suffices to show that $0$ is the only element in $\displaystyle{\bigcap_{i=1}^\infty A_i}$. Suppose $x\in \displaystyle{\bigcap_{i=1}^\infty A_i}$, then clearly $x\geq 0$. If $x>0$, then there is sufficiently large $i_0\in \mathbb{N}$ such that $x>\frac{1}{i_0}$ and hence $x\notin A_{i_0}$. This shows that if $x>0$, then $x\notin \displaystyle{\bigcap_{i=1}^\infty A_i}$.
\end{answer}

We close this section by introducing the concept of the indicator function, which is useful in expressing piecewise defined functions.

\begin{defi}
Let $A$ be a subset of $\mathbb{R}$. The \underline{indicator function}\index{indicator function} of $A$, denoted by $I_A$, is a function defined by
$$I_A(x)=\begin{cases} 1,&x\in A,\\ 0, &x\notin A \end{cases}.$$
\end{defi}

\begin{example}
Consider a piecewise defined function $f$ given by $$f(x)=\begin{cases} x^2,&x\geq 1,\\ 0,&x<1. \end{cases}$$ Then $f$ can be compactly expressed as $f(x)=x^2I_A(x)$, where $A=\{x:x\geq 1\}$.
\end{example}

\begin{example}
Let $X$ be a discrete random variable such that
$$P(X=n)=\frac{6}{\pi^2n^2},\quad n=1,2,3,\ldots.$$ Then the probability mass function $f$ of $X$ can be expressed by $f(x)=\frac{6}{\pi^2x^2}I_{\mathbb{N}}(x)$.
\end{example}


\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]

\item Let $A,B,C$ be sets with finitely many elements. Prove that
$$|A\cup B\cup C|=|A|+|B|+|C|-|A\cap B|-|A\cap C|-|B\cap C|+|A\cap B\cap C|.$$
\textit{Hint}: Treat $B\cup C$ as a single set, then
$$|A\cup \blue{B\cup C}|=|A|+|\blue{B\cup C}|-|A\cap \blue{(B\cup C)}|=|A|+|B\cup C|-|(A\cap B)\cup (A\cap C)|.$$
Use Theorem \ref{union number} to compute $|(A\cap B)\cup (A\cap C)|$.

\item Suppose $S\subseteq U$ and $U=\displaystyle{\bigcup_{i=1}^n A_i}$. Prove that $S=\displaystyle{\bigcup_{i=1}^n (S\cap A_i)}$.

\item Let $A$ and $B$ be sets.
\begin{enumerate}
\item Prove that $(A-B)\cup (B-A)=(A\cup B)-(A\cap B)$.
\item Prove that $|(A-B)\cup (B-A)|=|A|+|B|-2|A\cap B|$.
\end{enumerate}

\item For $i=1,2,\ldots$, define $A_i=\{x: -\frac{1}{i}<x<\frac{1}{i}\}$. Compute $\displaystyle{\bigcap_{i=1}^\infty A_i}$ and $\displaystyle{\bigcup_{i=1}^\infty A_i}$.

\item Let $P$ be the set of all nonnegative numbers, that is, $P=\{x:x\geq 0\}$. Prove that $x(2I_P(x)-1)=|x|$.

\item Let $A$ and $B$ be subsets of $\mathbb{R}$. Describe $I_{A\cap B}$ and $I_{A\cup B}$ in terms of $I_A$ and $I_B$.
\end{enumerate}
\end{exercise}

\section{Basics of Counting}

Many counting problems that appear in discrete probability can be solved by finding the number of ways to arrange a specified number of distinct elements of a set of a given size. We begin with what is called the \textit{Product Rule}.

\begin{thm}[The Product Rule]\index{Product Rule} Suppose that a procedure can be broken into a sequence of two tasks. If there are $n_1$ ways to do the first task and for each of these ways of doing the first task, there are $n_2$ ways to do the second task, then there are $n_1n_2$ ways to do the procedure.
\end{thm}

\begin{example} If chairs of an auditorium are to be labeled with a letter followed by a positive integer not exceeding $100$, then $26\times 100=2600$ chairs can be labeled differently.
\end{example}

The product rule extends to the case a procedure can be broken down into a sequence of more than just two tasks. Suppose that the procedure is carried out by performing the tasks $T_1,T_2,\ldots, T_k$ in sequence. If each task $T_i$ can be done in $n_i$ ways, regardless of how the previous tasks were done, then there are $n_1n_2\cdots n_k$ ways to carry out the procedure.

\begin{problem}\label{5p3 permutation} In how many ways can we select three students from a group of five students to stand in line for a picture?
\end{problem}

\begin{answer}
The first place can be filled with any of five students and the second place can take any of four remaining students. Similarly, the third place can be filled with any of remaining three students, so there are $5\times 4\times 3 =60$ ways.
\end{answer}

\begin{example}\label{prod rule cartesian}
Let $A_i$, $1\leq i \leq n$, be sets with finitely many elements. All elements of $A_1\times A_2\times \cdots \times A_n$ are of the form $(a_1,a_2,\ldots,a_n)$, where $a_i\in A_i$. Note that $a_1$ can be any of $|A_1|$ elements of $A_1$ and for each choice of $a_1$, there are $|A_2|$ ways of selecting $a_2$. Proceeding in this fashion, we can verify that
$$|A_1\times A_2 \times \cdots \times A_n|=|A_1||A_2|\cdots|A_n|.$$
\end{example}

\begin{defi} A \underline{permutation}\index{permutation} of a set of distinct objects is an ordered arrangement of these objects. An ordered arrangement of $r$ elements of a set is called an \underline{$r$-permutation}. The number of $r$-permutations of a set with $n$ elements is denoted by $P(n,r)$ or $_nP_r$.
\end{defi}

\begin{example} Let $S=\{1,2,3,4,5\}$. The ordered arrangement $(4,2,1,5,3)$ is a permutation of $S$. $(3,1,4)$ is a $3$-permutation of $S$.
\end{example}

\begin{example} Problem \ref{5p3 permutation} shows that $_5P_3=60$.
\end{example}

\begin{problem} How many ways are there to choose a chair and a vice chair from a group of four students?
\end{problem}

\begin{answer}
Let $\{A,B,C,D\}$ denote the group of students. We note that an ordered pair can be identified with a possible selection of a chair and a vice chair. For example, $(A,D)$ corresponds to the selection of $A$ as chair and $D$ as vice chair. It follows that there are $_4P_2=12$ ways to choose a chair and a vice chair. Note that $(A,B)$ should be distinguished from $(B,A)$.
\end{answer}

Recall that for $n\in \mathbb{N}$, the \underline{factorial}\index{factorial} $n!$ is defined to be
$$n!=1\cdot 2\cdots (n-1)\cdot n $$ with the convention that $0!=1$. In general, we get the following formula.

\begin{thm} For $n\geq r\geq 0$,
\begin{align*}_nP_r&=\underbrace{n\cdot(n-1)\cdot(n-2)\cdots(n-r+1)}_{r\textrm{ factors}}\\
&=\frac{n\cdot(n-1)\cdot(n-2)\cdots(n-r+1)\blue{(n-r)(n-r-1)\cdots 2\cdot 1}}{\blue{(n-r)(n-r-1)\cdots 2\cdot 1}}\\
&=\frac{n!}{(n-r)!}.
\end{align*}
In particular,
$$_nP_n=n\cdot(n-1)\cdot(n-2)\cdots 2 \cdot 1 = n!.$$
\end{thm}

\begin{problem}
How many permutations of the letters $\{R,A,I,N,B,O,W\}$ start with $R$ and end with $W$?
\end{problem}

\begin{answer}
The first and last spots are fixed ($R$ and $W$, respectively), so the number of desired permutations is determined by remaining five spots in the middle, which can be filled in $_5P_5=5!=120$ ways.
\end{answer}

\begin{example}
Let $n$ be a positive integer. The \underline{symmetric group on $n$ letters}\index{symmetric group on $n$ letters}, denoted by $S_n$\index{$S_n$}, is the set of all bijections from $\{1,2,\ldots,n\}$ onto itself. Note that each $\sigma\in S_n$ is in one-to-one correspondence with the $n$-tuple $(\sigma(1),\sigma(2),\ldots,\sigma(n))$. Since $\sigma(1)$ can be any of $\{1,2,\ldots,n\}$, $\sigma(2)$  can be any of $\{1,2,\ldots,n\}$ except for $\sigma(1)$, and $\sigma(3)$  can be any of $\{1,2,\ldots,n\}$ except for $\sigma(1)$ and $\sigma(2)$, etc, it follows that $|S_n|=n!$.
\end{example}

\begin{remark} \label{Sndef}
For $\sigma\in S_n$, an ordered pair $(i,j)$ is called an \underline{inversion}\index{inversion} of $\sigma$ if $i<j$ and $\sigma(i)>\sigma(j)$. See Definition \ref{detdefi} for a use of this concept in the definition of the determinant of a square matrix.
\end{remark}

We now turn our attention to counting \textit{unordered} selection of objects.

\begin{defi} An \underline{$r$-combination}\index{combination} of elements of a set is an unordered selection of $r$ elements from the set. The number of $r$-combinations of a set with $n$ elements is denoted by $C(n,r)$, $_nC_r$, or $n\choose r$.
\end{defi}

\begin{example} Let $S=\{1,2,3,4,5\}$. Then $\{1,3,4\}$ is a $3$-combination of $S$.
\end{example}

\begin{problem}\label{fourc2list}
Find all $2$-combinations of $\{1,2,3,4,5\}$.
\end{problem}

\begin{answer}
They are $\{1,2\},\{1,3\},\{1,4\},\{1,5\},\{2,3\},\{2,4\},\{2,5\},\{3,4\},\{3,5\},\{4,5\}$. This shows that ${5\choose 2}=10$.
\end{answer}

In general, we get the following formula.

\begin{thm}\label{formula for combination} The number of $r$-combinations of a set with $n$ elements, where $n$ is a nonnegative integer and $r$ is an integer with $0\leq r\leq n$, equals $${n\choose r}=\frac{n!}{r!(n-r)!}.$$
\end{thm}

\begin{remark} We explain Theorem \ref{formula for combination} with $5 \choose 3$ as an example. Let $S=\{1,2,3,4,5\}$ and consider all 3-permutations of $S$ consisting of $\{1,2,4\}$. They are: $$(1,2,4),(1,4,2),(2,1,4),(2,4,1),(4,1,2),\text{ and } (4,2,1).$$
On the other hand, there is only one $3$-combination of $S$ consisting of $\{1,2,4\}$; just $\{1,2,4\}$. Hence there are 6 times as many permutations as combinations consisting of $\{1,2,4\}$. Here $6$ comes from the total number of different permutations of $\{1,2,4\}$, that is, $6=3!$. Since this is true for all $3$-combinations of $S$, we conclude that
$$_5P_3=3!\times {5 \choose 3}.$$
In other words, $${5\choose 3}=\frac{1}{3!}\times _5P_3=\frac{1}{3!}\times\frac{5!}{(5-2)!}=\frac{5!}{3!(5-2)!}.$$
\end{remark}

\begin{problem} Compute ${n \choose 0}$, ${n \choose 1}$, and ${n\choose n}$.
\end{problem}

\begin{answer}
${n \choose 0}=\frac{n!}{0!n!}=1$, ${n \choose 1}=\frac{n!}{1!(n-1)!}=n$, and ${n\choose n}=\frac{n!}{n!(n-n)!}=1$.
\end{answer}

\begin{problem}[Pascal's Identity]\index{Pascal's Identity} Let $n$ and $k$ be positive integers with $n\geq k$. Prove that
$${n+1 \choose k}={n \choose k-1} + {n \choose k}.$$
\end{problem}

\begin{answer}
\begin{align*}
{n \choose k-1} + {n \choose k}&=\frac{n!}{(k-1)!(n-k+1)!}+\frac{n!}{k!(n-k)!}\\
&=\frac{n!k}{k!(n-k+1)!}+\frac{n!(n-k+1)}{k!(n-k+1)!}\\
&=\frac{(n+1)!}{k!(n-k+1)!}\\
&={n+1 \choose k}.
\end{align*}
\end{answer}

\begin{exercise}\label{counting part exer}\quad
\begin{enumerate}[\bfseries 1.]
\item Consider permutations of the letters $ABCDEFG$.
\begin{enumerate}
\item How many permutations contain the string $ABC$?
\item How many permutations contain exactly two letters between $A$ and $B$?
\item In how many permutations does $A$ come before $B$?
\end{enumerate}
\item\label{binomial interchange} Prove that ${n\choose r}={n \choose n-r}$.
\item What is the largest possible number of inversions of $\sigma$ in $S_n$? When is it attained?\\
\textit{Hint}: It is clear that the number of inversions of $\sigma\in S_n$ cannot be larger than ${n \choose 2}=\frac{n(n-1)}{2}$.
\item Suppose that there are seven faculty members in the mathematics department and six in the statistics department in a college. How many ways are there to select a committee to develop a mathematical statistics course at the school if the committee is to consist of three faculty members from the mathematics department and two from the statistics department?
\item Let $m,n$ be integers greater than $1$. Prove that
$${m+n \choose n}={m+n-2 \choose n-2}+2{m+n-2 \choose n-1}+{m+n-2 \choose n}.$$
\end{enumerate}
\end{exercise}

\section{Binomial Theorem and Its Consequences}

When $(x+y)^5=(x+y)(x+y)(x+y)(x+y)(x+y)$ is expanded, we obtain $2^5=32$ terms. Each term is determined by the choice of either $x$ or $y$ in each factor. For example, the choice of \blue{blue} letters as in
$$(\blue{x}+y)(\blue{x}+y)(x+\blue{y})(\blue{x}+y)(x+\blue{y})$$
gives a term $\blue{xxyxy}=\blue{x^3y^2}$. Note that each term is of the form $x^5$, $x^4y$, $x^3y^2$, $x^2y^3$, $xy^4$, or $y^5$. To obtain a term of the form $x^5$, $x$ must be chosen from each of five factors and this can be done in only one way which implies that the coefficient of $x^5$ when $(x+y)^5$ is multiplied out is 1. To obtain a term of the form $x^3y^2$, $x$ must be chosen from three of the five factors (and consequently $y$ must be chosen from the remaining two factors). Since this can be done in $5 \choose 3$ ways, the coefficient of $x^3y^2$ when $(x+y)^5$ is expanded will be $5 \choose 3$. In general, we have the following theorem.

\begin{thm}[The Binomial Theorem]\index{Binomial Theorem} Let $x$ and $y$ be variables, and let $n$ be a nonnegative integer. Then
\begin{align*}
(x+y)^n&=\sum_{k=0}^n {n\choose k} x^{n-k}y^k\\
&={n\choose 0} x^n + {n\choose 1} x^{n-1}y + {n \choose 2} x^{n-2}y^2
+\cdots +{n\choose n-1} xy^{n-1} + {n\choose n} y^n.\\
&={n\choose n} x^n + {n\choose n-1} x^{n-1}y + {n \choose n-2} x^{n-2}y^2
+\cdots +{n\choose 1} xy^{n-1} + {n\choose 0} y^n.\\
&=\sum_{k=0}^n {n\choose n-k} x^{n-k}y^k.
\end{align*}
\end{thm}

\begin{remark} \quad
\begin{enumerate}
\item The third equality in the Binomial Theorem above comes from Exercise \ref{counting part exer}.\ref{binomial interchange}.
\item Since $x+y=y+x$, it follows that
$$(x+y)^n=(y+x)^n=\sum_{k=0}^n {n\choose k} x^ky^{n-k}=\sum_{k=0}^n {n\choose n-k} x^ky^{n-k}.$$
\end{enumerate}
\end{remark}

\begin{example} When $n=5$, we get
\begin{align*}(x+y)^5&={5\choose 0}x^5+{5\choose 1}x^4y+{5\choose 2}x^3y^2+{5\choose 3}x^2y^3+{5\choose 4}xy^4+{5\choose 5}y^5\\
&=x^5+5x^4y+10x^3y^2+10x^2y^3+5xy^4+y^5.
\end{align*}
\end{example}

\begin{problem} Find the coefficient of $x^{12}y^{13}$ in the expansion of $(x+y)^{25}$ .
\end{problem}

\begin{answer}
The coefficient of $x^{12}y^{13}$ is given by ${25\choose 12}=\frac{25!}{12!13!}$.
\end{answer}

\begin{problem} Determine whether we have a term of the form $x^{14}$ in the expansion of $\left(x^2+\frac{1}{x}\right)^{10}.$ If we do, what is the coefficient?
\end{problem}

\begin{answer}
A general term of the expansion must be of the form ${10\choose k} (x^2)^{10-k}\left(\frac{1}{x}\right)^k={10\choose k} x^{20-3k}$. To get the coefficient of $x^{14}$, we see that $k$ must equal $2$ and the corresponding coefficient is ${10\choose 2}=45$.
\end{answer}

\begin{problem} Show that
${n \choose 0}+2{n \choose 1}+2^2{n \choose 2}+\cdots+2^n{n \choose n}=3^n$.
\end{problem}

\begin{answer}
$3^n=(2+1)^n={n \choose 0}2^0\cdot1^n+{n \choose 1}2^1\cdot1^{n-1}+{n \choose 2}2^2\cdot1^{n-2}+\cdots+{n \choose n}2^n\cdot 1^0$.
\end{answer}

\begin{example}[Binomial Distribution]\index{Binomial Distribution} Let $X\sim Binom(n,p)$, that is, let $X$ be a discrete random variable such that $P(X=k)={n\choose k}p^k(1-p)^{n-k}$ for $k=0,1,2,\ldots,n$. It follows from the Binomial Theorem that
$$\sum_{k=0}^n P(X=k)=\sum_{k=0}^n {n\choose k}p^k(1-p)^{n-k}=(p+(1-p))^n=1.$$
\end{example}

\begin{problem} Let $X\sim Binom(n,p)$. Compute the expectation $E(X)$ of $X$.
\end{problem}

\begin{answer}
\begin{align*}E(X)&=\sum_{k=0}^n kP(X=k)\\
&=\sum_{k=0}^n k{n\choose k}p^k(1-p)^{n-k}\\
(\text{first term is zero})&=\sum_{k=1}^n k{n\choose k}p^k(1-p)^{n-k}\\
&=\sum_{k=1}^n \frac{\red{n!}}{(n-k)!(k-1)!}\blue{p^k}(1-p)^{n-k}\\
&=\red{n}\blue{p}\sum_{k=1}^n \frac{\red{(n-1)!}}{(n-k)!(k-1)!}\blue{p^{k-1}}(1-p)^{n-k}\\
(\text{use }j=k-1)&=np\sum_{j=0}^{n-1} \frac{(n-1)!}{(n-1-j)!j!}p^{j}(1-p)^{n-1-j}\\
(\text{Binomial Theorem applied reversely})&=np(p+(1-p))^{n-1}\\
&=np.
\end{align*}
\end{answer}

\begin{problem}\label{allelefreq}
How big should a sample size be to detect an allele with a $0.5$\% frequency with $99$\% probability?
\end{problem}

\begin{answer}
Let $N$ denote the sample size, then the observed number $X$ of the allele with a $0.5$\% frequency will follow $Binom(2N,0.005)$. Solving the inequality 
$$0.01>P(X=0)={2N \choose 0}(0.005)^0(1-0.005)^{2N-0}=0.995^{2N}$$
gives $N\geq 460$.
\end{answer}

\begin{problem}[Vandermonde's Identity]\index{Vandermonde's Identity}\label{VandId} For $m,n,r\in \mathbb{N}$, show that
$${m+n \choose r}=\sum_{k=0}^r {m \choose k}{n \choose r-k}.$$
\end{problem}

\begin{answer}
We compute the coefficient of $x^r$ in the expansion of $(1+x)^{m+n}$ in two ways. First, using the Binomial Theorem, we get
$$(1+x)^{m+n}=\sum_{r=0}^{m+n}{m+n\choose r}x^r.$$ On the other hand, since
$$(1+x)^m=\sum_{i=0}^{m}{m\choose i}x^i\quad\text{and}\quad (1+x)^n=\sum_{j=0}^{n}{n\choose j}x^j,$$
the $x^r$ terms of $(1+x)^{m+n}=(1+x)^{m}(1+x)^{n}$ will be collected from the sum
$${m\choose 0}x^{0}{n\choose r}x^r+{m\choose 1}x^{1}{n\choose r-1}x^{r-1}+\cdots+{m\choose r}x^{r}{n\choose 0}x^0.$$This leads to the desired identity.
\end{answer}

\begin{example}\label{hypergeo}
Consider an experiment of drawing $n$ balls, without replacement, out of a box that contains $N$ white or red balls, exactly $K(K\geq n)$ of which being red.  Let $X$ denote the number of red balls in this drawing, then
$$P(X=k)=\frac{{K \choose k} {N-K \choose n-k}}{{N \choose n}}, \quad k=0,1,2,\ldots,n.$$
The random variable $X$ is said to follow \underline{Hypergeometric Distribution}\index{Hypergeometric Distribution} and is denoted as $X\sim hypergeo(N,K,n)$. It follows from Vandermonde's Identity (see Problem \ref{VandId}) that
$$\sum_{k=0}^nP(X=k)=\frac{1}{{N \choose n}}\sum_{k=0}^n {K \choose k} {N-K \choose n-k}=\frac{1}{{N \choose n}}{K+(N-K) \choose n}=1.$$
\end{example}

\begin{exercise}\quad\label{counting exercises}
\begin{enumerate}[\bfseries 1.]
\item Find the coefficient of $x^{3}y^{7}$ in the expansion of $(2x-3y)^{10}$.
\item\label{counting set} Let $n$ be a positive integer.
\begin{enumerate}
\item \label{application of binomial theorem}Show that ${n \choose 0} + {n\choose 1} + \cdots + {n\choose n} = 2^n$.
\item Prove Theorem \ref{powerset}.\\
\textit{Hint}: If $|S|=n$, the number of subsets of $S$ having exactly $k$ elements equals ${n \choose k}$.
\item Consider a multiple linear regression with $p$ covariates:
$$Y=\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p+\epsilon.$$
Show that there are $2^p-1$ nested models within the given full model. 
\end{enumerate}

\item Let $X\sim Binom(n,p)$. Show that $E(X^2)=np(np+1-p)$ and $Var(X)=np(1-p)$, where $Var(X)$ denotes the variance of $X$. \\
\textit{Hint}: Note that $$E(X^2)=\sum_{k=0}^n k^2P(X=k)=\sum_{k=0}^n k(k-1){n\choose k}p^k(1-p)^{n-k}+E(X).$$
\item Simplify $\displaystyle{n{n \choose 0}+(n-1){n \choose 1}+(n-2){n \choose 2}+\cdots+2{n\choose n-2}+{n \choose n-1}}$.\\
\textit{Hint}: First, expand $(x+1)^n$ using the Binomial Theorem, then differentiate with respect to $x$.
\item Simplify each of the following summations:
\begin{enumerate}
\item $\displaystyle{\sum_{n=0}^{10} {10 \choose n} 2^n}$.
\item $\displaystyle{\sum_{n=0}^{10} n{10 \choose n} 2^n}$.
\end{enumerate}
\item Let $X\sim hypergeo(N,K,n)$. Show that $E(X)=\frac{nK}{N}$.\\\textit{Hint}: You may want to use Vandermonde's Identity (see Problem \ref{VandId})
$${N-1 \choose n-1}=\sum_{s=0}^{n-1} {K-1 \choose s}{N-K \choose n-1-s}.$$
\end{enumerate}
\end{exercise}

\section{Other Topics in Discrete Mathematics}

In this section, we cover miscellaneous topics in discrete mathematics. We begin with the formula for some special summations.

\begin{thm}\label{special summation} \quad
\begin{enumerate}
\item\label{finite geometric} $\displaystyle{\sum_{k=1}^n ar^{k-1} = a+ar+ar^2+\cdots+ar^{n-1}=\frac{ar^n-a}{r-1}}$, provided $r\neq 1$. When $r=1$, $\displaystyle{\sum_{k=1}^n ar^{k-1} = a+a+\cdots+a=na}$.
\item $\displaystyle{\sum_{k=1}^n k=1+2+3+\cdots+n=\frac{n(n+1)}{2}}$.
\item $\displaystyle{\sum_{k=1}^n k^2=1^2+2^2+3^2+\cdots+n^2=\frac{n(n+1)(2n+1)}{6}}$.
\item $\displaystyle{\sum_{k=1}^n k^3=1^3+2^3+3^3+\cdots+n^3=\frac{n^2(n+1)^2}{4}}$.
\end{enumerate}
\end{thm}

\begin{remark}
Replacing $n$ by $n-1$ in Theorem \ref{special summation}, we get
\begin{enumerate}
\item $\displaystyle{\sum_{k=1}^{n-1} k=\frac{n(n-1)}{2}}$.
\item $\displaystyle{\sum_{k=1}^{n-1} k^2=\frac{n(n-1)(2n-1)}{6}}$.
\item $\displaystyle{\sum_{k=1}^{n-1} k^3=\frac{n^2(n-1)^2}{4}}$.
\end{enumerate}
\end{remark}

\begin{example}
$1+2+3+\cdots+99+\blue{100}=\frac{\blue{100}(\blue{100}+1)}{2}=5050$.
\end{example}

\begin{problem}
The \underline{Spearman correlation coefficient}\index{Spearman correlation coefficient} $\rho$ is defined as the Pearson correlation coefficient between the ranked variables. Let $x_i$, $y_i$ ($1\leq i \leq n$) be ranked variables. Assuming that there are no ties, show that
$$\rho=1-\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)},$$
where $d_i=x_i-y_i$.
\end{problem}

\begin{answer}
Note that by definition 
\begin{equation}\label{spearman_rho}\rho=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}},\end{equation}
where $\bar{x}=\bar{y}=\frac{\sum_{i=1}^n i}{n}=\frac{n+1}{2}$.
Since $\sum_{i=1}^n x_i=\sum_{i=1}^n y_i=\sum_{i=1}^n i=\frac{n(n+1)}{2}$, the numerator of (\ref{spearman_rho}) becomes
$$\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^n x_iy_i -\bar{y}\sum_{i=1}^n x_i -\bar{x}\sum_{i=1}^n y_i +n \bar{x}\bar{y}=\sum_{i=1}^n x_iy_i-\frac{n(n+1)^2}{4}.$$
In the denominator of (\ref{spearman_rho}), 
\begin{align*}
\sum_{i=1}^n(x_i-\bar{x})^2&=\sum_{i=1}^n(x_i^2-2\bar{x}x_i+\bar{x}^2)\\
&=\sum_{i=1}^n x_i^2-n\bar{x}^2\\
&=\frac{n(n+1)(2n+1)}{6}-n\left(\frac{n+1}{2}\right)^2\\
&=\frac{n(n^2-1)}{12}.
\end{align*}
Similarly, $\sum_{i=1}^n(y_i-\bar{y})^2=\frac{n(n^2-1)}{12}$ and hence the denominator of (\ref{spearman_rho}) reduces to $\frac{n(n^2-1)}{12}$ and it follows that
\begin{equation}\label{spearman_rho2}\rho=\frac{\sum_{i=1}^n x_iy_i-\frac{n(n+1)^2}{4}}{\frac{n(n^2-1)}{12}}=\frac{12\sum_{i=1}^n x_iy_i-3n(n+1)^2}{n(n^2-1)}.\end{equation}
On the other hand,
$$\sum_{i=1}^n d_i^2=\sum_{i=1}^n x_i^2 + \sum_{i=1}^n y_i^2 -2\sum_{i=1}^n x_iy_i=\frac{n(n+1)(2n+1)}{3}-2\sum_{i=1}^n x_iy_i,$$
so it follows that
\begin{align*}
1-\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)}&=\frac{n(n^2-1)-2n(n+1)(2n+1)+12\sum_{i=1}^n x_iy_i}{n(n^2-1)}\\
&=\frac{12\sum_{i=1}^n x_iy_i-3n(n+1)^2}{n(n^2-1)}.
\end{align*}
From this and (\ref{spearman_rho2}) the result follows.
\end{answer}

To motivate the Principle of Mathematical Induction, we consider an infinite row of dominoes labeled by $1,2,3,\ldots$. One way to knock over all dominoes is to knock off the first domino after making sure that neighboring dominoes are close enough so that the fall of the $k^{\text{th}}$ domino guarantees the fall of the $k+1^{\text{st}}$ domino, for all $k$. This is summarized in the following theorem.

\begin{thm}[Principle of Mathematical Induction]\index{Mathematical Induction} Let $P(n)$ be a statement involving a positive integer $n$. To prove that $P(n)$ is true for all positive integers $n$, it suffices to show the following two steps:\\
$\bullet$ Basis Step: verify that $P(1)$ is true.\\
$\bullet$ Inductive Step: show that the conditional statement $P(k)\to P(k+1)$ is true for all positive integers $k$.
\end{thm}

\begin{problem}\label{induction sum of n} Use Mathematical Induction to prove that $1+2+3+\cdots+n=\frac{n(n+1)}{2}$ for all $n\in \mathbb{N}$.
\end{problem}

\begin{answer} Let $P(n)$ denote the statement ``$1+2+3+\cdots+n=\frac{n(n+1)}{2}$". We want to show that $P(n)$ is true for all positive integers $n$. We will do so by completing the Basis Step and the Inductive Step.\\
Basis Step: Is $P(1)$ true? Yes, because $P(1)$ means ``$1=\frac{1\cdot 2}{2}$", which is obviously true.\\
Inductive Step: Suppose $P(k)$ is true. To complete this step, we need to show that $P(k+1)$ is true. Since we are assuming that $P(k)$ is true, it means that
\begin{equation}\label{given}1+2+3+\cdots+k=\frac{k(k+1)}{2}. \end{equation}
Using this assumption, we want to show that
\begin{equation}\label{to be shown}1+2+3+\cdots+k+(k+1)=\frac{(k+1)(k+2)}{2}. \end{equation}
Indeed, $\blue{1+2+3+\cdots+k}+(k+1)=\blue{\frac{k(k+1)}{2}}+(k+1)$ (since we assumed that $(\ref{given})$ is true), and this in turn equals $(k+1)\left(\frac{k}{2}+1\right)=\frac{(k+1)(k+2)}{2}$, proving $(\ref{to be shown})$. This means that $P(k+1)$ is true.
\end{answer}

\begin{problem} Use Mathematical Induction to prove that $2^n<n!$ for every integer $n$ with $n\geq 4$.
\end{problem}

\begin{answer}
Let $P(n)$ denote the statement ``$2^n<n!$".\\
Basis Step: First note that this time the basis step is to show that $P(4)$ is true. Indeed $P(4)$ is true, because $2^4=16$ is less than $4!=24$.\\
Inductive Step: Suppose that $P(k)$ is true, that is, suppose that $2^k<k!$ We need to show that $P(k+1)$ is true, that is, $2^{k+1}<(k+1)!$. Indeed,
\begin{align*}(k+1)!-2^{k+1}&=k!(k+1)-2^{k+1}\\
(\text{since }2^k<k!)&>2^k(k+1)-2^{k+1}\\
&=2^k(k-1)\\
&>0,
\end{align*}
as desired.
\end{answer}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]
\item The goal of this exercise is to shows that $\displaystyle{\sum_{k=1}^n k^2=\frac{n(n+1)(2n+1)}{6}}$ (see Theorem \ref{special summation}).
\begin{enumerate}
\item Verify that
\begin{equation}\label{cube expansion} (k+1)^3-k^3=6k^2+6k+1 \end{equation}
 for all $k$.
\item Taking summation over $k=1,2,\ldots,n$ on both sides of (\ref{cube expansion}), show that
\begin{equation}\label{cube expansion 2}
(n+1)^3-1^3=6\sum_{k=1}^n k^2+6\sum_{k=1}^n k+n.
\end{equation}
\item Using Problem \ref{induction sum of n} in (\ref{cube expansion 2}), derive that $\displaystyle{\sum_{k=1}^n k^2=\frac{n(n+1)(2n+1)}{6}}$.
\end{enumerate}
\item Show that $1+3+5+\cdots+(2n-1)=n^2$ for all positive integers $n$ using
\begin{enumerate}
\item Theorem \ref{special summation}.
\item Mathematical Induction.
\end{enumerate}
\item Show that $1+2+2^2+\cdots+2^n=2^{n+1}-1$ for all \textit{nonnegative} integers $n$ using
\begin{enumerate}
\item Theorem \ref{special summation}.
\item Mathematical Induction.
\end{enumerate}
\item Use Mathematical Induction to prove that $n<2^n$ for all positive integers $n$.
\end{enumerate}
\end{exercise}

\chapter{Single Variable Calculus}

\section{Limits and Continuity}

Roughly speaking, A function is continuous if one can graph it without lifting a pen. In other words, a function is said to be continuous if it has neither jumps nor drops. The notion of continuity arises frequently in probability. For example, the cumulative distribution function (cdf)\index{cumulative distribution function}\index{cdf} of a random variable is always right-continuous. In this section, we review basic properties and consequences of continuity such as the Intermediate Value Theorem and the Extreme Value Theorem. We begin with a rigorous definition of limits.

\begin{defi}\label{ft cts defi} Let $f:\mathbb{R}\to \mathbb{R}$ and $a\in \mathbb{R}$.
\begin{enumerate}
\item We say that the \underline{limit}\index{limit of a function} of $f$ as $x$ approaches $a$ is $\ell$ if for every $\epsilon>0$, there is $\delta>0$ such that
$0<|x-a|<\delta$ implies $|f(x)-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{x\to a} f(x)=\ell}$.
\item $f$ is said to be \underline{continuous}\index{continuous} at $a$ if $\displaystyle{\lim_{x\to a} f(x)=f(a)}$.
\item $f$ is said to be continuous over an interval $I$ if $f$ is continuous at $a$ for all $a\in I$.
\end{enumerate}
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item The choice of $\delta$ depends on $\epsilon$. In general, smaller $\epsilon$ requires smaller $\delta$.
\item $f$ is continuous at $a$ if and only if for every $\epsilon>0$, there is $\delta>0$ such that $|x-a|<\delta$ implies $|f(x)-f(a)|<\epsilon$. Note that the inequality $|x-a|>0$ is not a required condition since $|x-a|=0$ implies that $x=a$ and that $|f(x)-f(a)|=|f(a)-f(a)|=0<\epsilon$ for all $\epsilon>0$.
\end{enumerate}
\end{remark}

\begin{problem}\label{limit of 2x}
Let $a\in \mathbb{R}$. Show that $\displaystyle{\lim_{x\to a}2x=2a}$.
\end{problem}

\begin{answer}
Let $\epsilon >0$ be given. We need to find $\delta>0$ such that $0<|x-a|<\delta$ implies $|2x-2a|<\epsilon$. Since $|2x-2a|=2|x-a|$, to make $|2x-2a|<\epsilon$, it suffices to have $|x-a|<\frac{\epsilon}{2}$ and we can take $\delta=\frac{\epsilon}{2}$. In fact, any positive number less than $\frac{\epsilon}{2}$ would work as a $\delta$.
\end{answer}

Definition \ref{ft cts defi} generalizes to \textit{one-sided} versions.

\begin{defi}
Let $f:\mathbb{R}\to \mathbb{R}$ and $a\in \mathbb{R}$.
\begin{enumerate}
\item We say that the \underline{limit from the right}\index{limit from the right} of $f$ as $x$ approaches $a$ is $\ell$ if for every $\epsilon>0$, there is $\delta>0$ such that
$a<x<a+\delta$ implies $|f(x)-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{x\to a+} f(x)=\ell}$ or $f(a+)=\ell$.
\item We say that the \underline{limit from the left}\index{limit from the left} of $f$ as $x$ approaches $a$ is $\ell$ if for every $\epsilon>0$, there is $\delta>0$ such that
$a-\delta<x<a$ implies $|f(x)-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{x\to a-} f(x)=\ell}$  or $f(a-)=\ell$.
\item $f$ is said to be \underline{right-continuous}\index{right-continuous} at $a$ if $\displaystyle{\lim_{x\to a+} f(x)=f(a)}$.
\item $f$ is said to be \underline{left-continuous}\index{left-continuous} at $a$ if $\displaystyle{\lim_{x\to a-} f(x)=f(a)}$.
\end{enumerate}
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item $\displaystyle{\lim_{x\to a} f(x)=\ell}$ if and only if $\displaystyle{\lim_{x\to a+} f(x)=\lim_{x\to a-} f(x)=\ell}$. In particular, $f$ is continuous at $a$ if and only if $f$ is both right- and left-continuous at $a$.
\item For $f$ to be continuous at $a$, the following three conditions must be met:
\begin{itemize}
\item $f(a)$ exists,
\item $\displaystyle{\lim_{x\to a} f(x)}$ also exists, and
\item $\displaystyle{\lim_{x\to a} f(x)=f(a)}$.
\end{itemize}
\end{enumerate}
\end{remark}

\begin{example} In the graph of $y=f(x)$ in Figure \ref{figure of function}, a solid bullet ($\bullet$) indicates the function value. For example, $f(0.5)=1.8$. Note that $\displaystyle{\lim_{x\to 2}f(x)}=-0.7$ and $\displaystyle{\lim_{x\to 3}f(x)}=-0.9$. Since $f(2)$ does not exist, $f$ is \textit{not} continuous at $2$. Although $f(3)=1.5$ exists, since $f(3)=1.5 \neq -0.9=\displaystyle{\lim_{x\to 3}f(x)}$, $f$ is \textit{not} continuous at $3$ either.
\end{example}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=2, yscale=2, >=triangle 45]
\draw [->] (-4,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;
\draw [->] (0,-2) -- (0,2.3) ;
\node [above right] at (0,2.3) {$y$} ;
\draw [thick] plot [smooth, tension=1] coordinates { (-3,-1.5)(-2,-1)(-1,1) };
\draw [thick] plot [smooth, tension=1] coordinates { (-1,-1.3)(0,0.5)(0.5,0.8) };
\draw [thick] plot [smooth, tension=1] coordinates { (0.5,1.5)(1.3,1.3)(2,-0.7)(2.5,-0.6)(3,-0.9)(3.5,1.2) };

\draw [dashed] (-3,-1.5) -- (-3,0) ;
\node [above] at (-3,0) {$a$} ;

\draw [dashed] (-1,1) -- (-1,-1.3) ;
\draw [dashed] (-1,1) -- (0,1) ;
\draw [dashed] (0,-1.3) -- (-1,-1.3) ;
\node [right] at (0,1) {$1$} ;
\node [above right] at (-1,0) {$-1$} ;
\node [right] at (0,-1.3) {$-1.3$} ;

\node[left] at (0,1.8) {$1.8$} ;
\node[left] at (0,1.5) {$1.5$} ;
\node[below] at (0.5,0) {$0.5$} ;
\node[below] at (3.5,0) {$b$} ;
\node[above] at (2,0) {$2$} ;
\node[above left] at (3,0) {$3$} ;
\node[left] at (0,0.8) {$0.8$} ;
\node[left] at (0,-0.7) {$-0.7$} ;
\node[left] at (0,-0.9) {$-0.9$} ;
\draw [dashed] (0,1.8) -- (0.5,1.8) -- (0.5,0) ;
\draw [dashed] (0, 1.5) -- (3,1.5) -- (3,-0.9) -- (0,-0.9) ;
\draw [dashed] (0, -0.7) -- (2,-0.7) -- (2,0) ;
\draw [dashed] (0,0.8) -- (0.5,0.8) ;
\draw [dashed] (3.5,0) -- (3.5,1.2) ;

\draw [fill=black] (-1,1) circle (1.3pt) ;
\draw [fill=black] (-1,-1.3) circle (1.3pt) ;
\draw [fill=white] (-1,-1.3) circle (1.0pt) ;
\draw [fill=black] (0.5,0.8) circle (1.3pt) ;
\draw [fill=white] (0.5,0.8) circle (1.0pt) ;
\draw [fill=black] (0.5,1.5) circle (1.3pt) ;
\draw [fill=white] (0.5,1.5) circle (1.0pt) ;
\draw [fill=black] (2,-0.7) circle (1.3pt) ;
\draw [fill=white] (2,-0.7) circle (1.0pt) ;
\draw [fill=black] (0.5,1.8) circle (1.3pt) ;
\draw [fill=black] (3,1.5) circle (1.3pt) ;
\draw [fill=black] (3,-0.9) circle (1.3pt) ;
\draw [fill=white] (3,-0.9) circle (1.0pt) ;

\end{tikzpicture}
\caption{A function defined on $[a,b]$}
\label{figure of function}
\end{center}
\end{figure}

\begin{problem}
Based on the graph of $y=f(x)$ in Figure \ref{figure of function}, compute $\displaystyle{\lim_{x\to -1+}f(x)}$ and $\displaystyle{\lim_{x\to -1-}f(x)}$. Is $f$  left-continuous at $-1$?
\end{problem}

\begin{answer}
$\displaystyle{\lim_{x\to -1+}f(x)=-1.3}$ and $\displaystyle{\lim_{x\to -1-}f(x)=1}$. Since $f(-1)=1$, $f$ is left-continuous.
\end{answer}

\begin{example}
It is easy to show that polynomial functions are continuous everywhere (see Example \ref{poly conti}). In fact, it is known that all elementary functions such as trigonometric functions, exponential functions, and logarithmic functions are continuous on its domain.
\end{example}

The definition of the limit as $x$ goes to $\pm\infty$ is given below.

\begin{defi}\label{lim at inf}\index{limit at infinity} Let $f:\mathbb{R}\to \mathbb{R}$.
\begin{enumerate}
\item We say that the limit of $f$ as $x$ goes to $\infty$ is $\ell$ if for every $\epsilon>0$, there is $M$ such that
$x>M$ implies $|f(x)-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{x\to \infty} f(x)=\ell}$.
\item We say that the limit of $f$ as $x$ goes to $-\infty$ is $\ell$ if for every $\epsilon>0$, there is $N$ such that
$x<N$ implies $|f(x)-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{x\to -\infty} f(x)=\ell}$.
\end{enumerate}
\end{defi}

\begin{remark}\label{inf as limit}
Often we will use an expression such as $\displaystyle{\lim_{x\to a} f(x)=\infty}$ by which we mean that for any $M$, there is $\delta>0$ such that $0<|x-a|<\delta$ implies $f(x)>M$. Other expressions such as $\displaystyle{\lim_{x\to a+} f(x)=\infty}$ and  $\displaystyle{\lim_{x\to \infty} f(x)=-\infty}$ can be defined analogously.
\end{remark}

\begin{example}
By $\displaystyle{\lim_{x\to a-} f(x)=-\infty}$, we mean the following: for any $N$, there is $\delta>0$ such that $a-\delta<x<a$ implies $f(x)<N$.
\end{example}

Some basic properties of limit are given below. Roughly speaking, limit is preserved by constant multiple, addition/subtraction, multiplication/division, and function composition.

\begin{thm}\label{properties of limits} Suppose that $f,g$, and $h$ are functions. Let $a$ be a real number or $a=\pm\infty$.
\begin{enumerate}
\item If $f(x)=c$ for all $x$, then $\displaystyle{\lim_{x\to a}}f(x)$ exists and equals $c$.
\item If both $\displaystyle{\lim_{x\to a}}f(x)$ and $\displaystyle{\lim_{x\to a}}g(x)$ exist (say $\displaystyle{\lim_{x\to a}}f(x)=\ell$ and $\displaystyle{\lim_{x\to a}}g(x)=k$), then
    \begin{enumerate}[i.]
    \item $\displaystyle{\lim_{x\to a}}(cf(x))$ ($c$ is a constant) also exists and equals $c\ell$.
    \item $\displaystyle{\lim_{x\to a}}|f(x)|$ also exists and equals $|\ell|$.
    \item $\displaystyle{\lim_{x\to a}}(f(x)\pm g(x))$ also exists and equals $\ell\pm k$.
    \item $\displaystyle{\lim_{x\to a}}(f(x)\cdot g(x))$ also exists and equals $\ell\cdot k$.
    \item If in addition $k\neq 0$, then $\displaystyle{\lim_{x\to a}}\frac{f(x)}{g(x)}$ also exists and equals $\frac{\ell}{k}$.
    \end{enumerate}
\item (Squeeze Theorem)\index{Squeeze Theorem} If $f(x)\leq h(x)\leq g(x)$ for all $x\neq a$ and if $\displaystyle{\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=\ell}$, then $\displaystyle{\lim_{x\to a}}h(x)$ also exists and equals $\ell$.
\item If $\displaystyle{\lim_{x\to a}g(x)=b}$ and $f$ is continuous at $b$, then $\displaystyle{\lim_{x\to a}f(g(x))=f(b)}$.
\end{enumerate}
\end{thm}

\begin{remark}\quad
\begin{enumerate}
\item Theorem \ref{properties of limits} remains valid if the limit is replaced by one-sided limits.
\item Theorem \ref{properties of limits} implies that if $f,g$ are continuous at $a$, then so are $cf$, $f \pm g$, $fg$, and $f/g$, provided that $g(a)\neq 0$.
\end{enumerate}
\end{remark}

\begin{example}\label{poly conti}
From Problem \ref{limit of 2x} and Theorem \ref{properties of limits}, it follows that
$$\lim_{x\to a} x = \lim_{x\to a} \frac{1}{2}(2x)=\frac{1}{2}\cdot 2a=a.$$ In general, if $p$ is a polynomial and $a\in \mathbb{R}$, then $\displaystyle{\lim_{x\to a}p(x)=p(a)}$. This shows that every polynomial is continuous on $\mathbb{R}$.
\end{example}

\begin{example}
Since $-1\leq \sin \frac{1}{x} \leq 1$, it follows that $-x\leq x\sin \frac{1}{x} \leq x$ for all $x\neq 0$. Since $\displaystyle{\lim_{x\to 0}(-x)=\lim_{x\to 0}x=0}$, we conclude that $\displaystyle{\lim_{x\to 0}x\sin \frac{1}{x}=0}$.
\end{example}

\begin{example} \label{euler}
It is known that $\displaystyle{\lim_{x\to 0} (1+x)^{\frac{1}{x}}=e}$, where $e=2.7182\cdots$, a number called \underline{Euler's number}\index{Euler's number} or the \underline{base of natural logarithm}.\index{base of natural logarithm}
\end{example}

\begin{problem}
Compute $\displaystyle{\lim_{x\to 0}\frac{\ln(1+x)}{x}}$.
\end{problem}

\begin{answer}
Theorem \ref{properties of limits} and Example \ref{euler}, $\displaystyle{\lim_{x\to 0}\frac{\ln(1+x)}{x}=\lim_{x\to 0} \ln (1+x)^{\frac{1}{x}}=\ln e=1}$.
\end{answer}

Now we study two main theorems regarding a continuous function defined over a closed interval. We start with the Intermediate Value Theorem.

\begin{thm}[Intermediate Value Theorem]\index{Intermediate Value Theorem} Suppose $f$ is continuous on a closed interval $[a,b]$. If $k$ is any number between $f(a)$ and $f(b)$, then there is at least one number $c$ in $[a,b]$ such that $f(c)=k$ (see Figure \ref{ivt example1}).
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [->] (-2,-1) -- (-2,5) ;
\node [above right] at (-2,5) {$y$} ;

\draw [thick, domain=-1.5:2.5] plot (\x, {1/2*\x*\x+1}) ;

\draw [dashed] (-1.5,0) -- (-1.5,1/2*1.5*1.5+1);
\draw [dashed] (2.5,0) -- (2.5,1/2*2.5*2.5+1);

\draw [dashed] (2,0) -- (2,1/2*2*2+1);
\draw [dashed] (-2,1/2*2*2+1) -- (2,1/2*2*2+1) ;
\draw [dashed] (-2,1/2*1.5*1.5+1) -- (-1.5,1/2*1.5*1.5+1) ;
\draw [dashed] (-2,1/2*2.5*2.5+1) -- (2.5,1/2*2.5*2.5+1) ;

\node [left] at (-2,1/2*2*2+1) {$k$} ;
\node [left] at (-2,1/2*1.5*1.5+1) {$f(a)$} ;
\node [left] at (-2,1/2*2.5*2.5+1) {$f(b)$} ;

\node [below] at (-1.5,0) {$a$} ;
\node [below] at (2.5,0) {$b$} ;
\node [below] at (2,0) {$c$} ;

\draw [fill=black] (-1.5,1/2*1.5*1.5+1) circle (2pt) ;
\draw [fill=black] (2.5,1/2*2.5*2.5+1) circle (2pt) ;
\draw [fill=red, red] (2,1/2*2*2+1) circle (2pt) ;  %% boundary is also red

\end{tikzpicture}
\end{center}
\caption{Intermediate Value Theorem}
\label{ivt example1}
\end{figure}

\begin{example} Consider $f(x)=x^2-2x$. Note that $f(2)=0$ and $f(6)=24$. By the Intermediate Value Theorem, for any $k$ between $0$ and $24$, there must be a $c$ between $2$ and $6$ such that $f(c)=k$. In particular, if $k=8$, then $c=4$ works.
\end{example}

\begin{problem}
Show that there is $c\in [0,1]$ such that $ce^c=1$.
\end{problem}

\begin{answer}
Let $f(x)=xe^x$, then clearly $f$ is continuous on $[0,1]$. Since $f(0)=0<1<e=f(1)$, by Intermediate Value Theorem, there is $c\in [0,1]$ such that $f(c)=1$, that is, $ce^c=1$.
\end{answer}

\begin{remark}[Bisection Method]\index{Bisection Method} The Intermediate Value Theorem can be used to find a numeric root of a continuous function. Suppose that $f$ is a continuous function defined on a closed interval $[a,b]$ such that $f(a)\cdot f(b)<0$. By the Intermediate Value Theorem, there exists $x_0$ such that $f(x_0)=0$. Let $c=\frac{a+b}{2}$. If $f(c)=0$, then $c$ is a solution of the equation $f(x)=0$ and we are done. If $f(c)\neq 0$, then either $f(a)f(c)<0$ or $f(c)f(b)<0$. Without loss of generality, assume that $f(c)f(b)<0$, then the equation must have a solution in $(c,b)$. Note that the length the interval in which a solution to $f(x)=0$ decreased by half, and we can keep halving the intervals until a solution is found within desired accuracy. See Figure \ref{bisection figure}.
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [red, ultra thick] (0,0) -- (1.5,0) ;

\draw [->] (-4,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [thick] plot [domain=-3:3] (\x, {(\x+2)^2/4-2}) ;

\node [right] at (3, 25/4-2) {$y=f(x)$};

\draw [dashed] (-3,0) -- (-3, 1/4-2) ;
\node [above] at (-3,0) {$a$};
\draw [dashed] (3,0) -- (3, 25/4-2) ;
\node [below] at (3,0) {$b$};
\draw [dashed] (0,0) -- (0,-1);
\node [above] at (0,0) {$c$};
\draw [dashed] (1.5,0) -- (1.5, 3.5*3.5/4-2);
\node [below] at (1.5,0) {$d$};
\node [above left] at (0.95,0) {$x_0$};
\draw [fill=black] (0.83,0) circle (2pt) ;

\end{tikzpicture}
\caption{Bisection Method: a solution to $f(x)=0$ is captured in the red line segment}
\label{bisection figure}
\end{center}
\end{figure}

Before we move on to the next result, some important notions regarding bounded subsets are in order.

\begin{defi}
Let $S$ be a nonempty subset of $\mathbb{R}$. If there is $m$ such that $m\leq s$ (respectively, $m\geq s$) for all $s\in S$, then we say that $S$ is \underline{bounded from below}\index{bounded from below} (respectively, \underline{bounded from above}\index{bounded from above}) and $m$ is called a \underline{lower bound}\index{lower bound} (respectively, \underline{upper bound})\index{upper bound} of $S$. The largest lower bound of $S$ is called the \underline{infimum}\index{infimum} and the smallest upper bound of $S$ is called the \underline{supremum}\index{supremum}. The infimum (respectively, supremum) of $S$ is denoted by $\inf S$ (respectively, $\sup S$).
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item Infimum and supremum generalize the notion of minimum and maximum, respectively. The main difference between infimum and minimum is in that $\inf S$ always exists whenever $S$ is bounded from below. This is not the case when $\inf S$ is replaced by $\min S$: $\min S$ may not exist when $S$ is bounded from below (see Example \ref{example of sup inf}). Similar argument holds for $\sup S$ and $\max S$.
\item $\inf S$ does not need be an element of $S$. In fact, $\inf S\in S$ if and only if $S$ has the minimum element and in this case, we have $\inf S=\min S$. Likewise, $\sup S$ need not bean element of $S$ and $\sup S\in S$ if and only if $S$ has the maximum element and in this case, $\sup S=\max S$.
\end{enumerate}
\end{remark}

\begin{example}\label{example of sup inf}
Let $S=(1,4]\cup \{5\}$, as described in Figure \ref{supinf}. Any number less than or equal to $1$ is a lower bound of $S$ and $\inf S=1$. Any number greater than or equal to $5$ is an upper bound of $S$ and $\sup S=5$. Note that $\max S=5$, but $\min S$ does not exist.
\end{example}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw (-2,0) -- (8,0) ;
\draw [red, ultra thick] (1,0) -- (4,0) ;
\fill [fill=red] (1,0) circle (3pt) ;
\fill [fill=white] (1,0) circle (2.2pt) ;
\fill [fill=red] (4,0) circle (3pt) ;
\fill [fill=red] (5,0) circle (3pt) ;

\node [below] at (1,0) {$1$};
\node [below] at (4,0) {$4$};
\node [below] at (5,0) {$5$};

\end{tikzpicture}
\caption{A bounded subset of $\mathbb{R}$}
\label{supinf}
\end{center}
\end{figure}

The next theorem states that a continuous function defined on a finite closed interval achieves both maximum and minimum. See Figure \ref{extreme value theorem}.

\begin{thm}[Extreme Value Theorem]\index{Extreme Value Theorem}\label{minmax}
Suppose that $f$ is continuous on a closed interval $[a,b]$.Then there exist $x_0$ and $x_1$ in $[a,b]$ such that $f(x_0)\leq f(x) \leq f(x_1)$ for all $x\in [a,b]$.
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [thick, domain=-1.5:2.5] plot (\x, {1/2*\x*\x+1}) ;

\draw [dashed] (0,0) -- (0,1);
\draw [dashed] (2.5,0) -- (2.5,1/2*2.5*2.5+1);
\node [below] at (0,0) {$x_0$} ;
\node [below] at (2.5,0) {$x_1$} ;
\node [above] at (-1.5,1/2*1.5*1.5+1) {$y=f(x)$} ;
\draw [fill=black] (0,1) circle (2pt) ;
\draw [fill=black] (2.5,1/2*2.5*2.5+1) circle (2pt) ;

\end{tikzpicture}
\end{center}
\caption{Extreme Value Theorem}
\label{extreme value theorem}
\end{figure}

\begin{remark}
The Extreme Value Theorem fails if the closed interval $[a,b]$ is replaced by a (half) open interval. Consider $f:(0,5]\to \mathbb{R}$ defined by $f(x)=\frac{1}{x}$. It is easy to check that $f$ fails to have the maximum. See Figure \ref{evt fails}.
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [->] (-1,0) -- (5.7,0) ;
\node [above right] at (5.7,0) {$x$} ;
\draw [->] (0,-1) -- (0,5.5) ;
\node [above right] at (0,5.5) {$y$} ;

\draw [<-,thick, dashed] plot [domain=0.2:0.3, samples=50] (\x, {1/\x}) ;
\draw [thick] plot [domain=0.3:5, samples=50] (\x, {1/\x}) ;

\node [below left] at (0,0) {$O$};
\node [below] at (5,0) {$5$};

\end{tikzpicture}
\caption{$f(x)=\frac{1}{x}$ on $(0,5]$}
\label{evt fails}
\end{center}
\end{figure}

\begin{exercise}\label{conti exer} \quad
\begin{enumerate}[\bfseries 1.]
\item Based on the graph of $y=f(x)$ in Figure \ref{figure of function}, compute $\displaystyle{\lim_{x\to 0.5+}f(x)}$ and $\displaystyle{\lim_{x\to 0.5-}f(x)}$. Is $f$ left or right-continuous at $0.5$?

\item\label{Cauchy ftl} Suppose that $f:\mathbb{R}\to \mathbb{R}$ satisfies $f(a+b)=f(a)+f(b)$ for all real numbers $a$ and $b$. Suppose that $f$ is continuous at $x=0$. Show that $f$ is continuous \textit{everywhere}.\\
\textit{Hint}: First, show that $f(0)=0$. Let $x_0\in \mathbb{R}$. For given $\epsilon>0$, one needs to find $\delta>0$ such that $|x-x_0|<\delta$ implies that $|f(x)-f(x_0)|<\epsilon$. By assumption, there exists $\delta_0$ such that $|y|<\delta_0$ implies $|f(y)|<\epsilon$. Show that taking $\delta=\delta_0$ works. You may want to use the fact that
$$f(x)-f(x_0)=f(x-x_0+x_0)-f(x_0)=f(x-x_0)+f(x_0)-f(x_0)=f(x-x_0)$$ combined with the substitution $y=x-x_0$.

\item Prove that the equation
$$\frac{x^4+x^2+5}{x-1}+\frac{x^8+4x^5+1}{x-7}=0$$
has a solution between $1$ and $7$.\\
\textit{Hint}: Consider a new equation $(x-7)(x^4+x^2+5)+(x-1)(x^8+4x^5+1)=0$.

\item Let $f:[0,2]\to \mathbb{R}$ be a continuous function such that $f(0)=0$ and $f(2)=2$. Show that there is $x_0\in [0,1]$ such that $f(x_0+1)-f(x_0)=1$.\\
    \textit{Hint}: Consider a function $g:[0,1]\to \mathbb{R}$ given by $g(x)=f(x+1)-f(x)$.

\item Let $f:(0,5]\to \mathbb{R}$ defined by $f(x)=\frac{1}{x}$ (see Figure \ref{evt fails}) and define $S=\{f(x):x\in (0,5]\}$. Is $S$ bounded from below? from above? Does $\inf S$ exist? How about $\sup S$, $\min S$, and $\max S$?
\end{enumerate}
\end{exercise}

\section{Differentiation}

Another important class of functions is that of differentiable functions. In probability, if the cdf of a random variable $X$ is differentiable, then the probability density function (pdf)\index{probability density function}\index{pdf} of $X$ exists and equals the derivative of the cumulative distribution function. A formal definition of differentiability is in order.

\begin{defi} For $f:\mathbb{R}\to \mathbb{R}$, consider the limit
$$\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}.$$
If the limit exists, then we say that the function is \underline{differentiable at $a$}\index{differentiable} and write $$f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}.$$ If the limit does not exist, then we say that the function is \underline{not differentiable at $a$}.
\end{defi}

Roughly speaking, a function is differentiable at $x=a$ if its graph is \textit{smooth} there: a tangent line exists at the point $(a,f(a))$ (see Figure \ref{not diff at a}).

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [thick, domain=-2:0, samples=50] plot (\x, {1-2*\x-\x*\x}) ;
\draw [thick, domain=0:3.5, samples=50] plot (\x, {1+2*sqrt(\x)}) ;


\node [below] at (0,0) {$c$} ;
\node [below] at (-1,0) {$a$} ;
\node [below] at (2,0) {$b$} ;

\node [right] at  (3.5,{1+2*sqrt(3.5)}) {$y=f(x)$} ;
\node [right] at  (0.7,{1+0.7/2}) {?} ;

\draw [color=red] (-2,2) -- (0,2) ;
\draw [color=red, domain=1:3] plot (\x, {\x/sqrt(2) +1+sqrt(2)}) ;
\draw [dashed, color=blue, domain=-0.7:0.7] plot (\x, {\x/2+1}) ;

\draw [fill=black] (0,1) circle (2pt) ;
\draw [fill=black] (-1,2) circle (2pt) ;
\draw [fill=black] (2,{1+2*sqrt(2)}) circle (2pt) ;
\draw [dashed] (0,0) -- (0,1);
\draw [dashed] (-1,0) -- (-1,2);
\draw [dashed] (2,0) -- (2,{1+2*sqrt(2)});

\end{tikzpicture}
\end{center}
\caption{$f$ is differentiable at $x=a$ and $x=b$, but not differentiable at $x=c$}
\label{not diff at a}
\end{figure}

\begin{remark}\quad
\begin{enumerate}
\item $f'(a)$ is called the \underline{derivative of $f$ at $a$}.\index{derivative of $f$ at $a$}
\item If $f$ is differentiable at $a$, then $f$ is continuous at $a$. The converse is not true.
\item $f'(a)$ represents the \textit{slope} of the tangent line to the graph of $y=f(x)$ at $x=a$. See Figure \ref{geometric meaning of derivative}.
\item If $f$ is differentiable at every point in an interval, the function is said to be differentiable on that interval.
\end{enumerate}
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [->] (-2,-1) -- (-2,5) ;
\node [above right] at (-2,5) {$y$} ;

\draw [color=red] (-1.5,-3/4+7/8) -- (2.5,5/4+7/8) ;

\draw [thick, domain=-1.5:2.5] plot (\x, {1/2*\x*\x+1}) ;
\node [below left] at (-2,0) {$O$} ;

\draw [dashed] (1/2,0) -- (1/2,9/8);
\node [below] at (1/2,0) {$a$} ;
\node [right] at (2.5,5/4+7/8) {$\red{\text{slope}=f'(a)}$} ;
\node [right] at (2.5,1/2*2.5*2.5+1) {$y=f(x)$} ;
\draw [fill=black] (1/2,9/8) circle (2pt) ;

\end{tikzpicture}
\end{center}
\caption{Geometric meaning of $f'(a)$}
\label{geometric meaning of derivative}
\end{figure}

\begin{example}
Let $f(x)=|x|$. If $a>0$, then for $h$ close to $0$, one gets
$$\frac{f(a+h)-f(a)}{h}=\frac{|a+h|-|a|}{h}=\frac{a+h-a}{h}=1$$ and it follows that $\displaystyle{f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=1}$. If $a<0$, then for $h$ close to $0$,
$$\frac{f(a+h)-f(a)}{h}=\frac{|a+h|-|a|}{h}=\frac{-(a+h)-(-a)}{h}=-1,$$
so $f'(a)=-1$. We now claim that $f$ is not differentiable at $x=0$. Indeed,
$$\lim_{h\to 0+}\frac{f(0+h)-f(0)}{h}=\lim_{h\to 0+}\frac{|h|}{h}=\lim_{h\to 0+}\frac{h}{h}=1,$$
while
$$\lim_{h\to 0-}\frac{f(0+h)-f(0)}{h}=\lim_{h\to 0-}\frac{|h|}{h}=\lim_{h\to 0+}\frac{-h}{h}=-1.$$
This shows that $\displaystyle{\lim_{h\to 0}\frac{f(0+h)-f(0)}{h}}$ does not exist.
\end{example}

Instead of computing $f'(a)$ for each point $a$, we now consider a generic derivative $f'(x)$. This \textit{function} gives the slope of the tangent line to $y=f(x)$ at point $x$.

\begin{defi} Let $f$ be a function. The \underline{derivative function}\index{derivative function} or simply \underline{derivative}\index{derivative} of $f$, denoted by $f'$ or $\displaystyle{\frac{df}{dx}}$, is a function defined by
$$f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.$$
\end{defi}

\begin{example}\label{geometric meaning}
Let $f(x)=\frac{1}{4}x^2-4$, then
$$ f'(x)=\lim_{h\to 0}\frac{\frac{1}{4}(x+h)^2-4-(\frac{1}{4}x^2-4)}{h}=\lim_{h\to 0}\frac{\frac{1}{2}xh+\frac{h^2}{4}}{h}=\frac{1}{2}x.$$
\end{example}

\begin{problem}
Let $f(x)=\frac{1}{x}$. Find $f'(x)$.
\end{problem}

\begin{answer}
$$ f'(x)=\lim_{h\to 0}\frac{\frac{1}{x+h}-\frac{1}{x}}{h}=\lim_{h\to 0}\frac{\frac{-h}{x(x+h)}}{h}=-\frac{1}{x^2}.$$
\end{answer}

To understand the geometric meaning of the derivative function, we now compare the graphs of $f$ and $f'$ using $f(x)=\frac{1}{4}x^2-4$. Note that the \red{\textit{slope}} of the tangent line to $y=f(x)$ at $x=2$ (part of it is given in Figure \ref{original graph}) is $\red{1}$.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7, >=triangle 45]
\draw [->] (-8,0) -- (8,0) ;
\node [above right] at (8,0) {$x$} ;
\draw [->] (0,-5) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\foreach \y in {-4,-3,-2,-1}
	{
	 \draw [dashed] (-8,\y) -- (8,\y) ;
	 \node [below left] at (0.1,\y) {$^{\y}$} ;
	}
\foreach \y in {1, 2, 3, 4}
	{
	 \draw [dashed] (-8,\y) -- (8,\y) ;
	 \node [above left] at (0.1,\y) {$_{\y}$} ;
	}
\foreach \x in {-7,-6,-5,-4, -3, -2, -1}
	{
	 \draw [dashed] (\x,-5) -- (\x,5) ;
	 \node [below left] at (\x+0.1,0) {$^{\x}$} ;
	}
\foreach \x in {1, 2, 3, 4, 5, 6, 7}
	{
	 \draw [dashed] (\x,-5) -- (\x,5) ;
	 \node [below right] at (\x,0) {$_{\x}$} ;
	}
\draw [thick] plot [domain=-6:6] (\x, {(\x)^2/4-4}) ;

\draw [blue, thick] plot [domain=-5.5:-2.5] (\x, {-2*\x-8}) ;

\node [below left] at (-2.5, -3) {\blue{slope $=-2$}};

\draw [red, thick] (0.5,-4.4) -- (3.5,-1.5) ;

\node [right] at (3.5, -1.5) {\red{slope $=1$}};

\draw [fill=black] (2,-3) circle (3pt) ;
\draw [fill=black] (-4,0) circle (3pt) ;
\node [below left] at (0.1, 0) {$O$} ;
\node [above right] at (6,4.7) {$y=f(x)$} ;
\end{tikzpicture}
\caption{Graph of $y=f(x)=\frac{1}{4}x^2-4$}
\label{original graph}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7, >=triangle 45]
\draw [->] (-8,0) -- (8,0) ;
\node [above right] at (8,0) {$x$} ;
\draw [->] (0,-5) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\foreach \y in {-4,-3,-1}
	{
	 \draw [dashed] (-8,\y) -- (8,\y) ;
	 \node [below left] at (0.1,\y) {$^{\y}$} ;
	}
\foreach \y in {2, 3, 4}
	{
	 \draw [dashed] (-8,\y) -- (8,\y) ;
	}

\foreach \y in {2, 3, 4}
	{
	 \node [above left] at (0.1,\y) {$_{\y}$} ;
	}

\node [above left] at (0.1,1) {$_{\red{1}}$} ;

\node [below left] at (0.1,-2) {$_{\blue{-2}}$} ;

\draw [red, dashed, very thick] (0,1) -- (2,1);

\draw [blue, dashed, very thick] (0,-2) -- (-4,-2);

\draw [dashed] (-8,1) -- (0,1) ;

\draw [dashed] (8,1) -- (2,1) ;

\draw [dashed] (-8,-2) -- (-4,-2) ;

\draw [dashed] (8,-2) -- (0,-2) ;

\foreach \x in {-7,-6,-5,-4, -3, -2, -1}
	{
	 \draw [dashed] (\x,-5) -- (\x,5) ;
	 \node [below left] at (\x+0.1,0) {$^{\x}$} ;
	}
\foreach \x in {1, 2, 3, 4, 5, 6, 7}
	{
	 \draw [dashed] (\x,-5) -- (\x,5) ;
	 \node [below right] at (\x,0) {$_{\x}$} ;
	}
\draw [thick] (-7,-3.5) -- (6,3) ;
\draw [fill=black] (2,1) circle (3pt) ;
\draw [fill=black] (-4,-2) circle (3pt) ;
\node [below left] at (0.1, 0) {$O$} ;
\node [above right] at (6,3) {$y=f'(x)$} ;
\end{tikzpicture}
\caption{Graph of $y=f'(x)=\frac{1}{2}x$}
\label{derivative graph}
\end{center}
\end{figure}

In Example \ref{geometric meaning}, we observe that the derivative of $f$ is given by $f'(x)=\frac{1}{2}x$, whose graph is given in Figure \ref{derivative graph}. Note that \red{$f'(2)=1$}. Similarly, the \blue{\textit{slope}} of the tangent line to $y=f(x)$ at $x=-4$ is $-2$, so \blue{$f'(-4)=-2$}.\\

It is known that all elementary functions are differentiable on its domain. Below is the collection of derivative functions of elementary functions.

\begin{thm}\label{elementary functions} Let $a$ and $c$ be constants, where $a>0$, $a\neq 1$.
\begin{multicols}{2}
\begin{enumerate}
\item If $f(x)=c$, then $f'(x)=0$.
\item If $f(x)=x^n$, then $f'(x)=nx^{n-1}$.
\item If $f(x)=e^x$, then $f'(x)=e^x$.
\item If $f(x)=a^x$, then $f'(x)=a^x\ln a$.
\item If $f(x)=\ln x$, then $f'(x)=\frac{1}{x}$.
\item If $f(x)=\log_a x$, then $f'(x)=\frac{1}{x\ln a}$.
\item If $f(x)=\sin x$, then $f'(x)=\cos x$.
\item If $f(x)=\cos x$, then $f'(x)=-\sin x$.
\item If $f(x)=\tan x$, then $f'(x)=\sec^2 x$.
\item If $f(x)=\arctan x$, then $f'(x)=\frac{1}{1+x^2}$.
\end{enumerate}
\end{multicols}
\end{thm}

Some basic properties of derivatives, which are immediate consequences of Theorem \ref{properties of limits}, are recorded below.

\begin{thm}\label{basic properties} Let $f$ and $g$ be differentiable functions and $c$ be a constant.
\begin{enumerate}
\item (Constant Multiple Rule)\index{Constant Multiple Rule}  $cf(x)$ is also differentiable and $(cf(x))'=cf'(x)$.
\item (Sum Rule)\index{Sum Rule}  $f(x)\pm g(x)$ is also differentiable and $(f(x)\pm g(x))'=f'(x)\pm g'(x)$.
\item (Product Rule)\index{Product Rule} $f(x)g(x)$ is also differentiable and $(f(x)g(x))'=f'(x)g(x)+f(x)g'(x)$.
\item (Quotient Rule)\index{Quotient Rule} $\frac{f(x)}{g(x)}$ is also differentiable and $\displaystyle{\left(\frac{f(x)}{g(x)}\right)'=\frac{f'(x)g(x)-f(x)g'(x)}{(g(x))^2}}$, provided that $g(x)\neq 0$.
\item (Chain Rule)\index{Chain Rule} $(f(g(x)))'=f'(g(x))g'(x)$.
\end{enumerate}
\end{thm}

The second order derivative $f''$ of $f$ is defined by the derivative function of $f'$, provided that $f'$ is also differentiable (In this case, we say that $f$ is \underline{twice differentiable}\index{differentiable!twice}). Higher order derivatives are defined likewise.

\begin{remark}
The \underline{curvature}\index{curvature} $\kappa$ of a plane curve $y=f(x)$ is given by
$$\kappa=\frac{|f''(x)|}{(1+f'(x))^{3/2}}.$$
Therefore, the second derivative, together with the first derivative in the denominator, determines how rapidly a given curve bends.
\end{remark}

The sign of the derivative function determines whether $f$ is increasing, and the sign of the second derivative determines the concavity of $f$, as summarized in the following.

\begin{thm}\label{derivatives and original} Let $f$ be differentiable on an interval $I$. Then
\begin{enumerate}
\item $f'>0$ on $I$ if and only if $f$ is increasing over $I$.
\item $f'<0$ on $I$ if and only if $f$ is decreasing over $I$.
\end{enumerate}

If $f$ is twice differentiable, then
\begin{enumerate}
\item $f''>0$ on $I$ if and only if $f'$ is increasing over $I$ if and only if $f$ is concave up $I$.
\item $f''<0$ on $I$ if and only if $f'$ is increasing over $I$ if and only if $f$ is concave down $I$.
\end{enumerate}
\end{thm}

We close this section with L'Hopital's Theorem, a powerful tool to compute limits.

\begin{thm}[L'Hopital's Theorem]\index{L'Hopital's Theorem} Let $f$ and $g$ be differentiable functions. Let $c$ be a real number or, $c=\pm \infty$. If $\frac{f(c)}{g(c)}$ is of the form $\frac{0}{0}$ or $\frac{\infty}{\infty}$, then
$$\lim_{x\to c}\frac{f(x)}{g(x)}=\lim_{x\to c}\frac{f'(x)}{g'(x)},$$
provided that $\displaystyle{\lim_{x\to c}\frac{f'(x)}{g'(x)}}$ exists.
\end{thm}

\begin{problem}
Compute $\displaystyle{\lim_{x\to 2} \frac{2^x-4}{x-2}}$.
\end{problem}

\begin{answer}
$\displaystyle{\lim_{x\to 2} \frac{2^x-4}{x-2}=\lim_{x\to 2} \frac{2^x\ln 2}{1}=4\ln 2}$.
\end{answer}

\begin{problem}
Compute $\displaystyle{\lim_{x\to \infty} xe^{-2x}}$.
\end{problem}

\begin{answer}
$\displaystyle{\lim_{x\to \infty} xe^{-2x}=\lim_{x\to \infty} \frac{x}{e^{2x}}=\lim_{x\to \infty} \frac{1}{2e^{2x}}=0}$.
\end{answer}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]

\item Show that $f(x)=x|x|$ is differentiable everywhere. Is $f'$ continuous? differentiable?

\item Consider a piecewise defined function
$$f(x)=\begin{cases} ax+b, &x\leq 1, \\ 2-(x-2)^2, &x>1. \end{cases}$$
Determine constants $a$ and $b$ so that $f$ is continuous and differentiable everywhere.

\item The goal of this exercise is to prove the Quotient Rule using the Product Rule. Let $f$ and $g$ be differentiable function and $g(x)\neq 0$.
\begin{enumerate}
\item Define $h(x)=\displaystyle{\frac{f(x)}{g(x)}}$. Compute $f\rq(x)$ in terms of $h(x),h'(x),g(x)$, and $g'(x)$.
\item Express $h'(x)$ in terms of $f(x),f'(x),g(x)$, and $g'(x)$, and deduce the Quotient Rule.
\end{enumerate}

\item Let $f:[0,\infty)\to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $0\leq f'(x)\leq f(x)$ for all $x\geq 0$. Show that $f(x)=0$ for all $x\geq 0$.\\
\textit{Hint}: Define $g:[0,\infty)\to \mathbb{R}$ by $g(x)=e^{-x}f(x)$, then $g(0)=0$ and $g$ is nonnegative for $x\geq 0$. Show that $g$ is decreasing.

\item Compute the following limits:
\begin{multicols}{2}
\begin{enumerate}
\item $\displaystyle{\lim_{x\to 3} \frac{x^2-9}{2x^2-5x-3}}$
\item $\displaystyle{\lim_{x\to 0} \frac{\sin (3x)}{2x}}$
\item $\displaystyle{\lim_{t\to 0} \frac{t^2}{e^t-t-1}}$
\item $\displaystyle{\lim_{x\to \infty} x^{2}e^{-x}}$
\item $\displaystyle{\lim_{x\to \infty} \frac{\ln x}{\ln(x^2+1)}}$
\item $\displaystyle{\lim_{x\to 0} \left(\frac{1}{x}-\frac{1}{\sin x}\right)}$
\end{enumerate}
\end{multicols}

\item Let $a>0$. For $x\neq 0$, define let $\psi(x)=\frac{a^x-1}{x}$. Determine $\psi(0)$ (in terms of $a$) so that $\psi$ is continuous at $x=0$.

\end{enumerate}
\end{exercise}

\section{Mean Value Theorem and Its Consequences}

One of the important properties of differentiable functions is depicted in the Mean Value Theorem. We begin with Rolle's Theorem, which can be thought as a special case of the Mean Value Theorem.

\begin{thm}[Rolle's Theorem]\index{Rolle's Theorem}
Let $f$ be a continuous function defined on a closed interval $[a,b]$. Suppose that $f$ is differentiable on $(a,b)$ and $f(a)=f(b)$, then there is $c\in (a,b)$ such that $f'(c)=0$.
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-4,0) -- (3,0) ;
\node [above right] at (3,0) {$x$} ;

\draw [->] (-3,-1) -- (-3,4.5) ;
\node [above right] at (-3,4.5) {$y$} ;

\draw [thick, domain=-2.5:2.5] plot (\x, {1/2*\x*\x+1}) ;
\draw [dashed] (-3,3) -- (2.5,3) ;
\draw [dashed] (-2,0) -- (-2,3) ;
\draw [dashed] (2,0) -- (2,3) ;
\draw [dashed] (0,0) -- (0,1) ;
\draw [color=red] (-2.5,1) -- (2.5,1) ;

\node [right] at (2.5, 1) {\red{$f'(c)=0$}};
\node [below] at (-2,0) {$a$} ;
\node [below] at (2,0) {$b$} ;
\node [below] at (0,0) {$c$} ;
\node [left] at (-3,3) {$f(a)=f(b)$} ;

\draw [fill=black] (2,3) circle (2pt) ;
\draw [fill=black] (-2,3) circle (2pt) ;
\draw [fill=black] (0,1) circle (2pt) ;

\end{tikzpicture}
\end{center}
\caption{Geometric interpretation of Rolle's Theorem}
\label{Rolle}
\end{figure}

The geometric meaning of Rolle's Theorem is described in Figure \ref{Rolle}. Roughly speaking, if $f(a)=f(b)$, then there must be a point $c$ between $a$ and $b$ at which the tangent line is horizontal. Note that there could be multiple such $c$'s. What if $f(a)\neq f(b)$? Rolle's Theorem can be generalized to the following.

\begin{thm}[Mean Value Theorem]\index{Mean Value Theorem}
Let $f$ be a continuous function defined on the closed interval $[a,b]$. Suppose that $f$ is differentiable on $(a,b)$, then there exists $c\in (a,b)$ such that
\begin{equation}\label{mean value eq} \frac{f(b)-f(a)}{b-a}=f'(c). \end{equation}
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;

\draw [->] (-2,-1) -- (-2,5) ;
\node [above right] at (-2,5) {$y$} ;

\draw [color=blue] (-1.5,-0.5*1.5+2) -- (2.5,0.5*2.5+2) ;
\draw [color=red] (-1.5,-3/4+7/8) -- (2.5,5/4+7/8) ;
\draw [dashed] (-1,0) -- (-1,3/2);
\draw [dashed] (2,0) -- (2,3);

\draw [thick, domain=-1.5:2.5] plot (\x, {1/2*\x*\x+1}) ;
\node [below left] at (-2,0) {$O$} ;
\node [below] at (-1,0) {$a$} ;
\node [below] at (2,0) {$b$} ;
\node [left] at (-2,3/2) {$f(a)$} ;
\node [left] at (-2,3) {$f(b)$} ;
\draw [dashed] (-2,3/2) -- (-1,3/2);
\draw [dashed] (-2,3) -- (2,3);
\draw [dashed] (1/2,0) -- (1/2,9/8);
\node [below] at (1/2,0) {$c$} ;
\node [right] at (2.5,5/4+7/8) {\red{$\text{slope}=f'(c)$}} ;
\node [right] at (2.5,0.5*2.5+2) {\blue{$\text{slope}=\frac{f(b)-f(a)}{b-a}$}} ;

\draw [fill=black] (-1,3/2) circle (2pt) ;
\draw [fill=black] (2,3) circle (2pt) ;
\draw [fill=black] (1/2,9/8) circle (2pt) ;

\end{tikzpicture}
\end{center}
\caption{Geometric interpretation of Mean Value Theorem}
\label{mean value fig}
\end{figure}

The geometric interpretation of the Mean Value Theorem is given in Figure \ref{mean value fig}. Note that (\ref{mean value eq}) can be written as $f(b)=f(a)+f'(c)(b-a)$ and this can be viewed as an approximation of $f(b)$ by $f(a)$ with an error $f'(c)(b-a)$.

\begin{problem}\label{must be constant}
Suppose that $f$ is differentiable on $(a,b)$ and $f'(x)=0$ for all $x\in (a,b)$. Show that $f$ is a constant function.
\end{problem}

\begin{answer}
Pick any $s,t\in (a,b)$ with $s\neq t$. By the Mean Value Theorem, there is $r$ between $s$ and $t$ such that $$\frac{f(s)-f(t)}{s-t}=f'(r).$$ Since $f'(r)=0$, it follows that $f(s)=f(t)$.
\end{answer}

\begin{problem}\label{ftl eqn}
Let $f:\mathbb{R}\to \mathbb{R}$ with $f(0)\neq 0$. Suppose that $f(x+y)=f(x)f(y)$ for all $x,y\in \mathbb{R}$ and $f$ is differentiable at $0$ with $f'(0)=\lambda$. Show that $f(x)=e^{\lambda x}$.
\end{problem}

\begin{answer}
First, we note that $f(0)=f(0+0)=f(0)f(0)=f(0)^2$. Since $f(0)\neq 0$, it follows that $f(0)=1$. Since $f(x+h)=f(x)f(h)$, we get
$$\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=f(x)\lim_{h\to 0}\frac{f(h)-1}{h}=f(x)\underbrace{\lim_{h\to 0}\frac{f(h+0)-f(0)}{h}}_{f'(0)}=\lambda f(x),$$
which implies that $f'(x)=\lambda f(x)$ for all $x$. Define $g:\mathbb{R}\to \mathbb{R}$ by $g(x)=f(x)e^{-\lambda x}$, then $g$ is differentiable everywhere and moreover $g'(x)=e^{-\lambda x}(f'(x)-\lambda f(x))=0$ for all $x$. By Problem \ref{must be constant}, we conclude that there is a constant $c$ such that $f(x)=ce^{\lambda x}$. Since $f(0)=1$, it follows that $f(x)=e^{\lambda x}$.
\end{answer}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]
\item Let $f$ and $g$ be continuous on the closed interval $[a,b]$. Suppose that $f$ and $g$ are differentiable on $(a,b)$ and $g(a)\neq g(b)$. Show that there exists $c\in (a,b)$ such that $$ \frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(c)}{g'(c)}.$$
\textit{Hint}: Define a function $h$ on $[a,b]$ by $h(x)=g(x)(f(b)-f(a))-f(x)(g(b)-g(a))$, then clearly $h$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Show that $h(a)=h(b)$ and apply Rolle's Theorem. This result is often called \underline{Cauchy Mean Value Theorem}.\index{Cauchy Mean Value Theorem}
\item Suppose that $f:\mathbb{R}\to \mathbb{R}$ satisfies $$|f(x)|\leq 1000|x|^2$$ for all $x\in \mathbb{R}$. Show that $f$ is differentiable at $0$. What is $f'(0)$?

\item A function $f:\mathbb{R}\to \mathbb{R}$ is said to be \textit{convex} if
\begin{equation}\label{convex def} f(\lambda x+(1-\lambda) y)\leq \lambda f(x)+(1-\lambda) f(y) \end{equation}
for all $x,y\in \mathbb{R}$ and for all $\lambda$ with $0< \lambda < 1$.
The purpose of this problem is to show that if $f:\mathbb{R}\to \mathbb{R}$ is twice differentiable with $f''(x)\geq 0$ for all $x$, then $f$ is convex.
\begin{enumerate}
\item Let $x<y$ be real numbers and $0<\lambda<1$. Let $z=\lambda x + (1-\lambda) y$. Show that $x<z<y$.
\item Show that there exist $u,v \in \mathbb{R}$ with $$x<u<z<v<y$$ such that
$$f'(u)=\frac{f(z)-f(x)}{z-x}\quad\text{and}\quad f'(v)=\frac{f(y)-f(z)}{y-z}.$$
\item Suppose that $f$ is twice differentiable with $f''(x)\geq 0$ for all $x$. Show that $f'(v)\geq f'(u)$ and from this conclude that (\ref{convex def}) holds.
\end{enumerate}
\item Let $f:\mathbb{R}\to \mathbb{R}$ be an infinitely differentiable function. Suppose that
$$f(1)=f(0)=f'(0)=f''(0)=f'''(0)=\cdots=f^{(99)}(0)=0.$$
Prove that $f^{(100)}(x_0)=0$ for some $x_0$ in $(0,1)$.

\item Suppose that $f:\mathbb{R}\to \mathbb{R}$ be twice differentiable with $f'(x)>0$ and $f''(x)>0$ for all $x\in \mathbb{R}$. Show that $\displaystyle{\lim_{x\to\infty}f(x)=\infty}$. 
\\\textit{Hint}: Suppose not, then $f$ must be bounded from above and $\displaystyle{\lim_{x\to\infty}f(x)=\sup_{x\in \mathbb{R}} f(x)=A}$ exists (why?). Pick any $x_0\in \mathbb{R}$ and let $h=f(x_0+1)-f(x_0)$, then $h>0$. Choose $M$ so that $M>\frac{A-f(x_0+1)}{h}$. Apply the Mean Value Theorem to $\frac{f(x_0+1+M)-f(x_0+1)}{M}$ to show that $f(x_0+1+M)>A$, a contradiction.
\end{enumerate}
\end{exercise}

\section{Finding Extreme Values}

Maximum likelihood estimation is a method of estimating the parameter of a statistical model based. As the name suggests, the method involves finding the maximum of the likelihood, a function of the parameter determined by a given data set.

\begin{defi} Let $f$ be defined on an interval $I$ and let $p\in I$.
\begin{enumerate}
\item $f$ is said to have a \underline{local minimum}\index{local minimum} at $p$ if $f(p)$ is less than or equal to the values of $f$ for points near $p$.
\item $f$ is said to have a \underline{local maximum}\index{local maximum} at $p$ if $f(p)$ is greater than or equal to the values of $f$ for points near $p$.
\item $f$ is said to have a \underline{local extremum}\index{local extremum} at $p$ if $f$ has either a local minimum or a local maximum at $p$.
\end{enumerate}
\end{defi}

\begin{example} In Figure \ref{extreme1}, $f$ has two local minima at $b$ and $d$ and two local maxima at $a$ and $c$.
\end{example}

\begin{figure}[!h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (0,0) -- (16,0) ;
\node [above right] at (16,0) {$x$} ;
\draw [thick] plot [smooth, tension=1] coordinates {(1,-1) (3,1) (6,-2) (9,2.5)};
\draw [thick] plot [smooth, tension=1] coordinates {(9,2.5) (10,-1.2) (11,-1)};
\draw [thick] plot[domain=11:15] (\x, {(\x-13)^3/4+1}) ;
\draw [dashed] (2.7,1) -- (2.7,0) ;
\node [below] at (2.7,0) {$a$} ;
\draw [dashed] (5.7,-2) -- (5.7,0) ;
\node [above] at (5.7,0) {$b$} ;
\draw [dashed] (9,2.5) -- (9,0) ;
\node [below] at (9,0) {$c$} ;
\draw [dashed] (10.5,-1.5) -- (10.5,0) ;
\node [above] at (10.5,0) {$d$} ;
\draw [dashed] (13,1) -- (13,0) ;
\node [below] at (13,0) {$e$} ;
\node [left] at (14.5,2) {$y=f(x)$} ;
\end{tikzpicture}
\caption{A function with local extrema}
\label{extreme1}
\end{center}
\end{figure}

\begin{defi} A point $p$ in the domain of a function $f$ where $f'(p)=0$ or $f'(p)$ is undefined is called a \underline{critical point}\index{critical point} of the function.
\end{defi}

\begin{example} In Figure \ref{extreme1}, $f$ has five critical points: $a,b,c,d$, and $e$.
\end{example}

When the graph of a function is present, identifying local extrema is not that difficult. However, with only the formula of a function, finding local extrema might be challenging. The next theorems explain a systematic way to find local extrema without graphing the function.

\begin{thm}[Critical Point Theorem]\index{Critical Point Theorem} Suppose $f$ is defined on an interval and has a local extremum at $p$, which is not an endpoint of the interval. Then $p$ must be a critical point of $f$.
\end{thm}

\begin{remark} \quad
\begin{enumerate}
\item The converse of the Critical Point Theorem is not true, as the point $e$ shows in Figure \ref{extreme1}.
\item If $f$ is differentiable everywhere, the Critical Point Theorem means that local extrema occur only at points where $f'$ equals zero.
\end{enumerate}
\end{remark}

Roughly speaking, critical points are the only candidates of local extrema. The next theorem, which is based on Theorem \ref{derivatives and original},  explains how to classify critical points.

\begin{thm}[The Second Derivative Test]\index{Second Derivative Test} Let $f$ be a function. Suppose $f'(p)=0$ (so $p$ is a critical point of $f$).
\begin{enumerate}
\item If $f''(p)>0$, then $f$ has a local minimum at $p$.
\item If $f''(p)<0$, then $f$ has a local maximum at $p$.
\item If $f''(p)=0$, then the test fails. That is, $f$ can have a local minimum, local maximum, or neither at $p$.
\end{enumerate}
\end{thm}

\begin{problem}\label{polyextreme} Classify the critical points of $f(x)=x^3-9x^2-48x+52$ as local maxima or local minima.
\end{problem}

\begin{answer} Note that $f'(x)=3x^2-18x-48=3(x-8)(x+2)$ and $f''(x)=6x-18$. It follows that $f'(x)=0$ if $x=8$ or $x=-2$, giving two critical points. Since $f''(8)>0$ and $f''(-2)<0$, we conclude that $f$ has a local minimum $-396$ at $8$ and a local maximum $104$ at $-2$.
\end{answer}

 The single greatest (or least) value of a function $f$ over an interval is called the \underline{global maximum}\index{global maximum} (or \underline{global minimum}\index{global minimum}) of $f$ over the interval. It is apparent that the global extremum itself must be a local extremum.

\begin{problem} Find the global maximum and global minimum of $g(x)=x+\frac{1}{x}$ defined on $(0,\infty)$.
\end{problem}

\begin{answer}
Since $g'(x)=1-\frac{1}{x^2}$ and $g''(x)=\frac{2}{x^3}$, we see that $g'(1)=0$ and $g''(1)>0$. It follows that $g$ has a unique local minimum $2$ at $x=1$, which should be the global minimum. $g$ does not have the global maximum since $\displaystyle{\lim_{x\to \infty}g(x)=\infty}$.
\end{answer}

\begin{problem} For $\sigma>0$, define $f$ by $f(\sigma)=\frac{1}{\sigma}e^{-\frac{1}{\sigma^2}}$. Find the global maximum of $f$ over $(0,\infty)$.
\end{problem}

\begin{answer}
Since $f(\sigma)>0$ for all $\sigma>0$ and $\ln x$ is an increasing function, it suffices to maximize $\ell(\sigma)=\ln f(\sigma)=-\ln \sigma-\frac{1}{\sigma^2}$, which is easier to handle. From $\ell'(\sigma)=-\frac{1}{\sigma}+\frac{2}{\sigma^3}=0$, we get $\ell'(\sqrt{2})=0$. Since $\ell''(\sigma)=\frac{1}{\sigma^2}-\frac{6}{\sigma^4}$, we have that $\ell''(\sqrt{2})<0$. It follows that $\ell$, and hence $f$ has the global maximum at $\sqrt{2}$.
\end{answer}

For a continuous function defined on a bounded closed interval, there is an easy way to determine the global extremum.

\begin{thm}[The Global Extremum Theorem]\label{Global Extremum Theorem} Let $f$ be a continuous function on a closed interval. Then the global maximum and global minimum are obtained either at critical points of the function or at the endpoints of the interval.\end{thm}

\begin{example} Consider $f(x)=x^3-9x^2-48x+52$ on the interval $[-5,15]$. From Problem \ref{polyextreme}, we see that $f$ has a local minimum $-396$ at $8$ and a local maximum $104$ at $-2$. Since $f(-5)=-58$ and $f(15)=682$, we conclude that the global maximum is $682$ and the global minimum is $-396$.
\end{example}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]
\item Let $f:[0,1]\to \mathbb{R}$ be continuous on $[0,1]$ and differentiable on $(0,1)$. Suppose that $f(0)=0$ and $|f'(x)|\leq |f(x)|$ for all $x\in (0,1)$. Prove that $f(x)=0$ for all $x\in [0,1]$.\\
\textit{Hint}: Since $f$ is continuous on $[0,1]$, so is $|f|$. By the Extreme Value Theorem, there exists $y_0\in [0,1]$ such that $|f(y_0)|=\max\{|f(x)|:x\in [0,1]\}$. If $y_0=0$, then the result follows obviously (why?). If $y_0>0$, then show that there is $z_0\in (0,y_0)$ such that $f(y_0)=f'(z_0)\cdot y_0$ and hence $|f(y_0)|\leq |f'(z_0)||y_0|\leq |f(z_0)|\leq |f(y_0)|$. Conclude that $f$ has either maximum or minimum at $z_0$. Use the Critical Point Theorem to show that $f'(z_0)=0$.
\item Find the local extrema of $f(x)=\frac{2x}{x^2+1}$. Are they global extrema, too?
\item  Find the global extrema of $g(x)=xe^{-x}$, if any.
\item For $p\in [0,1]$, define $f$ by $f(p)=p^2(1-p)^6$. Find the global maximum of $f$.
\item Let $H_1,H_2,\ldots,H_m$ be null hypotheses of $m$ independent tests with a common significance level $\alpha_0$. The \underline{familywise error rate}\index{familywise error rate} (FWER)\index{FWER} is the probability of making even one type I error in the family: $1-(1-\alpha_0)^m$. Bonferroni correction states that if $\alpha_0=\frac{\alpha}{m}$, then FWER is less than $\alpha$. Prove this. \\\textit{Hint}: Let $f:[0,1]\to \mathbb{R}$ be defined to be 
$$f(\alpha)=1-\left(1-\frac{\alpha}{m}\right)^m.$$ Show that $f(\alpha)\leq \alpha$.

\end{enumerate}
\end{exercise}

\section{Integration}

When $X$ is a continuous random variable with the pdf $f$, the probability that $X$ belongs to an interval $[a,b]$ is given by the definite integral of $f$ over $[a,b]$. Moreover, moments of $X$, in particular the expectation of $X$, are all expressed by definite integrals. To define the definite integral of $f$ over $[a,b]$, let $n\in \mathbb{N}$, and partition the interval $[a,b]$ into $n$ subintervals of equal length, $\{a=t_0<t_1<t_2<\cdots<t_n=b\}$, where $t_k=a+k\frac{b-a}{n}$. Consider the \underline{upper sum}\index{upper sum}
$$U(f,n)=\sum_{k=1}^n \sup\{f(x):t\in [t_{k-1},t_k]\}\cdot\frac{b-a}{n}$$
and the \underline{lower sum}\index{lower sum}
$$L(f,n)=\sum_{k=1}^n \inf\{f(x):t\in [t_{k-1},t_k]\}\cdot\frac{b-a}{n}.$$
Note that both upper and lower sums represent the sum of rectangular areas. See the first three plots in Figure \ref{riemann sum} for an illustration when $n=6,12,30$.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.4, yscale=0.4, >=triangle 45, declare function={ ff(\x) = -(\x-1)*(\x-6)*(\x-11)/40+5;}]

\def\numofnodes{6} %% number of nodes
\draw [->] (-2,0) -- (14,0) ;
\node [above right] at (14,0) {$x$} ;

\node [below] at (0,0) {$a$};
\node [below] at (12,0) {$b$};
\node [above] at (6,7) {$n=\numofnodes$};

\foreach \k in {1,...,\numofnodes}{
	\def\height{{max(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=cyan]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\foreach \k in {1,...,\numofnodes}{
	\def\height{{min(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=yellow]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\draw [thick, domain=-1:13, samples=50] plot (\x, {ff(\x)});

\begin{scope}[xshift=19cm]
\def\numofnodes{12} %% number of nodes
\draw [->] (-2,0) -- (14,0) ;
\node [above right] at (14,0) {$x$} ;

\node [below] at (0,0) {$a$};
\node [below] at (12,0) {$b$};
\node [above] at (6,7) {$n=\numofnodes$};

\foreach \k in {1,...,\numofnodes}{
	\def\height{{max(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=cyan]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\foreach \k in {1,...,\numofnodes}{
	\def\height{{min(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=yellow]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\draw [thick, domain=-1:13, samples=50] plot (\x, {ff(\x)});
\end{scope}

\begin{scope}[yshift=-13cm]
\def\numofnodes{30} %% number of nodes
\draw [->] (-2,0) -- (14,0) ;
\node [above right] at (14,0) {$x$} ;

\node [below] at (0,0) {$a$};
\node [below] at (12,0) {$b$};
\node [above] at (6,7) {$n=\numofnodes$};

\foreach \k in {1,...,\numofnodes}{
	\def\height{{max(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=cyan]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\foreach \k in {1,...,\numofnodes}{
	\def\height{{min(ff((\k-1+0/5)*12/\numofnodes), ff((\k-1+1/5)*12/\numofnodes), ff((\k-1+2/5)*12/\numofnodes), ff((\k-1+3/5)*12/\numofnodes), ff((\k-1+4/5)*12/\numofnodes), ff((\k-1+5/5)*12/\numofnodes))}}

 \draw [fill=yellow]({(\k-1)*12/\numofnodes},0) -- ({(\k-1)*12/\numofnodes},\height) -- ({\k*12/\numofnodes},\height) --  ({\k*12/\numofnodes},0) -- cycle ;
	}

\draw [thick, domain=-1:13, samples=50] plot (\x, {ff(\x)});
\end{scope}

\begin{scope}[xshift=19cm, yshift=-13cm]
\def\numofnodes{30} %% number of nodes
\draw [->] (-2,0) -- (14,0) ;
\node [above right] at (14,0) {$x$} ;

\node [below] at (0,0) {$a$};
\node [below] at (12,0) {$b$};
\node [above] at (6,7) {$n\to\infty,\quad\displaystyle{\int_a^b f(x)\,dx}$};

\draw [fill=yellow] (0,0) -- plot [domain=0:12, samples=50] (\x, {ff(\x)}) -- (12,0) -- cycle;
\draw [thick, domain=-1:13, samples=50] plot (\x, {ff(\x)});

\end{scope}

\end{tikzpicture}
\caption{Upper and lower sums}
\label{riemann sum}
\end{center}
\end{figure}

As can be seen in Figure \ref{riemann sum}, as $n$ gets bigger and bigger, the difference between $U(f,n)$ and $L(f,n)$ gets smaller and smaller. In fact, it is known that if $f$ has at most finitely many discontinuities in $[a,b]$, then both limits $\displaystyle{\lim_{n\to \infty}U(f,n)}$ and $\displaystyle{\lim_{n\to \infty}L(f,n)}$ exist and equal each other. When this happens, we say that $f$ is \underline{Riemann integrable} or simply \underline{integrable}\index{integrable function}, and define the \underline{definite integral}\index{definite integral} $\displaystyle{\int_a^b f(x)\,dx}$ of $f$ between $a$ and $b$ by
$$\int_a^b f(x)\,dx=\lim_{n\to \infty}U(f,n),$$
which also equals $\displaystyle{\lim_{n\to \infty}L(f,n)}$.

As the last plot in Figure \ref{riemann sum} shows, when $f$ is nonnegative over $[a,b]$, $\displaystyle{\int_a^b f(x)\,dx}$ can be interpreted as the area under the graph of $f$ and above the $x$-axis between vertical lines $x=a$ and $x=b$.

%% \begin{figure}[h]
%% \begin{center}
%% \begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
%% \fill[fill=yellow] (-1.5,0) -- plot [domain=-1.5:2.5] (\x, {((\x)^3-(\x)^2-12*\x)/30+2}) -- (2.5,0) -- cycle;
%% \draw [->] (-3,0) -- (4,0) ;
%% \node [above right] at (4,0) {$x$} ;
%% \draw [thick, domain=-2.5:3.5, samples=70] plot (\x, {((\x)^3-(\x)^2-12*\x)/30+2}) ;
%% \draw [dashed] (-1.5,0) -- (-1.5,  {((-1.5)^3-(-1.5)^2-12*(-1.5))/30+2}) ;
%% \draw [dashed] (2.5,0) -- (2.5,  {((2.5)^3-(2.5)^2-12*(2.5))/30+2}) ;
%% \node [below] at (-1.5,0) {$a$} ;
%% \node [below] at (2.5,0) {$b$} ;
%% \end{tikzpicture}
%% \caption{A nonnegative function over $[a,b]$}
%% \label{nonnegative}
%% \end{center}
%% \end{figure}

\begin{example} The graph of $y=\sin x$ is given in Figure \ref{sine}. The areas $A$ and $B$ can be described by
$A=\displaystyle{\int_{-\frac{3\pi}{2}}^{-\frac{5\pi}{4}}\sin x\,dx}$ and $B=\displaystyle{\int_{\frac{\pi}{2}}^{\pi}\sin x\,dx}$.
\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (-3*pi/2,0) -- plot [domain=-3*pi/2:-5*pi/4] (\x, {sin(\x r)}) -- (-5*pi/4,0) -- cycle;
\fill[fill=yellow] (pi/2,0) -- plot [domain=pi/2:pi] (\x, {sin(\x r)}) -- cycle;
\draw [->] (-2*pi-0.5,0) -- (2*pi+0.5,0) ;
\node [above right] at (2*pi+0.5,0) {$x$} ;
\draw [->] (0,-1.5) -- (0,1.5) ;
\node [above right] at (0,1.5) {$y$} ;
\draw [thick, domain=-2*pi:2*pi, samples=70] plot (\x, {sin(\x r)}) ;  % \x r for radian
\draw [dashed] (pi/2,0) -- (pi/2,1) ;
\draw [dashed] (-3*pi/2,0) -- (-3*pi/2, 1) ;
\draw [dashed] (-5*pi/4,0) -- (-5*pi/4, {sin(-5*pi/4 r)}) ;
\node [below] at (-2*pi,0) {$-2\pi$} ;
\node [above right] at (-pi,0) {$-\pi$} ;
\node [below] at (-3*pi/2,0) {$-\frac{3\pi}{2}$} ;
\node [below] at (-5*pi/4,0) {$-\frac{5\pi}{4}$} ;
\node [above] at (2*pi,0) {$2\pi$} ;
\node [below right] at (0,0) {$O$} ;
\node [below] at (pi/2,0) {$\frac{\pi}{2}$} ;
\node [above right] at (pi,0) {$\pi$} ;
\node at (-4.3, 0.35) {$A$} ;
\node at (2.2, 0.35) {$B$} ;
\end{tikzpicture}
\caption{Graph of $f(x)=\sin x$}
\label{sine}
\end{center}
\end{figure}
\end{example}

\begin{problem}
Compute $\displaystyle{\int_{-3}^3 \sqrt{9-x^2}\,dx}$.
\end{problem}

\begin{answer}
Note that the graph of $y=\sqrt{9-x^2}$ is the upper semicircle with radius $3$ centered at the origin (see Figure \ref{semicircle}), so $\displaystyle{\int_{-3}^3 \sqrt{9-x^2}\,dx=\frac{1}{2}\cdot\pi\cdot 3^2=\frac{9\pi}{2}}$.
\begin{figure}[!h]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7, >=triangle 45]
\fill[fill=yellow] (3,0) --  (3,0) arc (0:180:3) -- cycle;
\draw [->] (-4,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;
\draw [->] (0,-1) -- (0,4) ;
\node [above right] at (0,4) {$y$} ;
\draw [thick] (3,0) arc (0:180:3) ;
\node [below] at (3,0) {$3$} ;
\node [below] at (-3,0) {$-3$} ;
\node [above left] at (0,3) {$3$} ;
\node [below left] at (0,0) {$O$} ;
\end{tikzpicture}
\caption{Graph of $y=\sqrt{9-x^2}$}
\label{semicircle}
\end{center}
\end{figure}
\end{answer}

How do we interpret the definite integral of $f$ over $[a,b]$ when $f$ is not always nonnegative? If $f$ is positive for some $x$ values and negative for others, then $\displaystyle{\int_a^b f(x)\,dx}$ is interpreted as the \textit{signed} area, that is, it is the sum of areas above the $x$-axis, counted positively, and areas below the $x$-axis, counted negatively. For example, in Figure \ref{positive and negative}, suppose the shaded area above the $x$-axis is $2$ and the shaded area below the $x$-axis is $3$. Then
$$\int_{a}^bf(x)\,dx=2+(-3)=-1.$$

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (-2,0) -- plot [domain=-2:3] (\x, {((\x)^3-(\x)^2-12*\x)/10}) -- (3,0) -- cycle;
\draw [->] (-3,0) -- (4,0) ;
\node [above right] at (4,0) {$x$} ;
\draw [thick, domain=-2.5:3.5, samples=70] plot (\x, {((\x)^3-(\x)^2-12*\x)/10}) ;
\draw [dashed] (-2,0) -- (-2,  {((-2)^3-(-2)^2-12*(-2))/10}) ;
\draw [dashed] (3,0) -- (3,  {((3)^3-(3)^2-12*(3))/10}) ;
\node [below] at (-2,0) {$a$} ;
\node [above] at (3,0) {$b$} ;
\node at (-1.3,0.7) {$+$} ;
\node at (1.8, -1) {$-$} ;
\end{tikzpicture}
\caption{A function that has both positive and negative values over $[a,b]$}
\label{positive and negative}
\end{center}
\end{figure}

\begin{problem}\label{sineknown} It is known that $\displaystyle{\int_{\frac{\pi}{2}}^\pi \sin x\,dx=1}$  (see Problem \ref{sineverify}). Using this, compute
\begin{enumerate}
\item $\displaystyle{\int_{0}^\frac{3\pi}{2} \sin x\,dx}$
\item $\displaystyle{\int_{0}^1 \arcsin x\,dx}$
\end{enumerate}
\end{problem}

\begin{answer} \quad
\begin{enumerate}
\item By symmetry of sine, we see that the area under the sine curve between $x=0$ and $x=\frac{\pi}{2}$ equals $1$. Similarly, the area enclosed by the sine curve, the $x$-axis, and vertical lines $x=\pi$ and $x=\frac{3\pi}{2}$ also equals $1$. It follows that $\displaystyle{\int_{0}^\frac{3\pi}{2} \sin x\,dx=1+1+(-1)=1.}$  See Figure \ref{sine2}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (pi/2,0) -- plot [domain=pi/2:pi] (\x, {sin(\x r)}) -- cycle;
\draw [->] (-pi-0.5,0) -- (2*pi+0.5,0) ;
\node [above right] at (2*pi+0.5,0) {$x$} ;
\draw [->] (0,-1.5) -- (0,1.5) ;
\node [above right] at (0,1.5) {$y$} ;
\draw [thick, domain=-pi:2*pi, samples=50] plot (\x, {sin(\x r)}) ;  % \x r for radian
\draw [dashed] (pi/2,0) -- (pi/2,1) ;
\draw [dashed] (3*pi/2,0) -- (3*pi/2,-1) ;
\node [above] at (-pi,0) {$-\pi$} ;
\node [above] at (3*pi/2,0) {$\frac{3\pi}{2}$} ;
\node [above] at (2*pi,0) {$2\pi$} ;
\node [below right] at (0,0) {$O$} ;
\node [below] at (pi/2,0) {$\frac{\pi}{2}$} ;
\node [above right] at (pi,0) {$\pi$} ;
\node at (2.2, 0.35) {$1$} ;
\end{tikzpicture}
\caption{Graph of sine}
\label{sine2}
\end{center}
\end{figure}

\item It follows from Figure \ref{arcsine} that
\begin{align*}\int_{\frac{1}{2}}^1 \arcsin x\,dx&=\frac{\pi}{2}\cdot 1-\int_{0}^\frac{\pi}{2} \sin x\,dx \\
&=\frac{\pi}{2}-\int_{\frac{\pi}{2}}^{\pi} \sin x\,dx \\
&=\frac{\pi}{2}-1.
\end{align*}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=3.6, yscale=3.6, >=triangle 45]
\fill[fill=cyan]  plot [domain=0:1] (\x, {pi*asin(\x)/180}) --  (1,pi/2) -- (0,pi/2) -- cycle;
\fill[fill=yellow] (0,0) -- plot [domain=0:1] (\x, {pi*asin(\x)/180}) --(1,0) -- cycle;
\draw [->] (-0.2,0) -- (1.8,0) ;
\node [above right] at (1.8,0) {$x$} ;
\draw [->] (0,-0.2) -- (0,1.8) ;
\node [above right] at (0,1.8) {$y$} ;
\draw [thick, domain=0:pi/2, samples=50] plot (\x, {sin(\x r)}) ;
\draw [thick, domain=0:1, samples=50] plot (\x, {pi*asin(\x)/180}) ;   % asin returns degree values
\draw [dashed] (pi/2,0) -- (pi/2,1) -- (0,1) ;
\draw [dashed] (0,pi/2) -- (1,pi/2) --(1,0) ;
\draw [dashed] (0,0) -- (pi/2,pi/2) ;
\node [below] at (pi/2,0) {$\frac{\pi}{2}$} ;
\node [below left] at (0,0) {$O$} ;
\node [below] at (1,0) {$1$} ;
\node [left] at (0,1) {$1$} ;
\node [left] at (0,pi/2) {$\frac{\pi}{2}$} ;
\node [above] at (pi/2,1) {$y=\sin x$} ;
\node [above] at (pi/2,pi/2) {$y=x$} ;
\node [above left] at (0.9,1.2) {$y=\arcsin x$} ;

\end{tikzpicture}
\caption{Graph of arcsine}
\label{arcsine}
\end{center}
\end{figure}

\end{enumerate}
\end{answer}

\begin{defi} Let $f$ be a function. If $F'(x)=f(x)$ for all $x$, then we say that $F$ is an \underline{antiderivative}\index{antiderivative} of $f$.
\end{defi}

\begin{example} $F(x)=\frac{1}{3}x^3$ is an antiderivative of $f(x)=x^2$. Note that $G(x)=\frac{1}{3}x^3+2$ is also an antiderivative of $f$.
\end{example}

\begin{remark} In general, if $F(x)$ is an antiderivative of $f(x)$, then so is $F(x)+C$ for any constant $C$. Moreover, if $F$ and $G$ are both antiderivatives of $f$, then $F$ and $G$ differ by only a constant by the Mean Value Theorem (apply Problem \ref{must be constant} to $F(x)-G(x)$). The collection of all antiderivatives of $f$ is called the \underline{indefinite integral}\index{indefinite integral} of $f$ and denoted by $$\int f(x)\,dx.$$ Therefore, if $F$ is an antiderivative of $f$, then we have
$$\int f(x)\,dx = F(x) +C,$$
where $C$ is an arbitrary constant.
\end{remark}

\begin{thm}[The Fundamental Theorem of Calculus]\index{Fundamental Theorem of Calculus} Let $f$ be a continuous function on the interval $[a,b]$ and $f(x)=F'(x)$ (so $F$ is an antiderivative of $f$). Then
$$\int_{a}^b f(x)\,dx=\left[F(x)\right]_a^b=F(b)-F(a).$$
\end{thm}

\begin{problem} Compute $\displaystyle{\int_{-1}^3 (2x+3)\,dx}$ using the Fundamental Theorem of Calculus.
\end{problem}

\begin{answer}
Let $F(x)=x^2+3x$, then it is easy to check that $F'(x)=2x+3.$ It follows that $\displaystyle{\int_{-1}^3 (2x+3)\,dx=F(3)-F(-1)=18-(-2)=20}$.
\end{answer}

\begin{problem} \label{sineverify} (see Problem \ref{sineknown}) Find $\displaystyle{\int_{\frac{\pi}{2}}^\pi \sin x\,dx}$.
\end{problem}

\begin{answer} Since $(-\cos x)'=\sin x $, by the Fundamental Theorem of Calculus,
$$\displaystyle{\int_{\frac{\pi}{2}}^\pi \sin x\,dx=\left[-\cos x\right]_\frac{\pi}{2}^\pi=-\cos \pi +\cos \frac{\pi}{2}=1}.$$
\end{answer}

By the Fundamental Theorem of Calculus, finding definite integral of a function $f$ boils down to finding its indefinite integral. From Theorem \ref{elementary functions} and \ref{basic properties}, we have the following theorems.

\begin{thm} For $n\neq -1$ and $a\neq -1$, $a>0$,
\begin{multicols}{2}
\begin{enumerate}
\item $\displaystyle{\int x^n\,dx=\frac{x^{n+1}}{n+1}+C}$.
\item $\displaystyle{\int \frac{1}{x}\,dx=\ln |x|+C}$.
\item $\displaystyle{\int e^x\,dx=e^x+C}$.
\item $\displaystyle{\int a^x\,dx=\frac{a^x}{\ln a}+C}$.
\item $\displaystyle{\int \sin x\,dx=-\cos x+C}$.
\item $\displaystyle{\int \cos x\,dx=\sin x+C}$.
\item $\displaystyle{\int \sec^2 x\,dx=\tan x+C}$.
\item $\displaystyle{\int \frac{1}{1+x^2} \,dx=\arctan x+C}$.
\end{enumerate}
\end{multicols}
\end{thm}

\begin{thm}\label{integral properties} Let $f$ and $g$ be integrable functions and let $a$, $b$, and $c$ be constants. Then
\begin{enumerate}
\item $\displaystyle{\int_{a}^cf(x)\,dx=\int_{a}^bf(x)\,dx+\int_{b}^cf(x)\,dx}$.
\item $\displaystyle{\int_{a}^b(f(x)\pm g(x))\,dx=\int_{a}^bf(x)\,dx\pm\int_{a}^bg(x)\,dx}$.
\item $\displaystyle{\int_{a}^bcf(x)\,dx=c\int_{a}^bf(x)\,dx}$.
\end{enumerate}
\end{thm}

Recall that $f:\mathbb{R}\to \mathbb{R}$ is called \underline{odd}\index{odd function} (respectively, \underline{even}\index{even function}) if $f(-x)=-f(x)$ (respectively, $f(-x)=f(x)$) for all $x\in \mathbb{R}$. In other words, $f$ is an odd (respectively, even) function if $f$ is symmetric with respect to the origin (respectively, the $y$-axis). Integral of a symmetric function over a symmetric region has the following convenient properties.

\begin{thm} Let $f$ be a continuous function (see Figure \ref{symm integral}).
\begin{enumerate}
\item If $f$ is odd, then $\displaystyle{\int_{-a}^a f(x)\,dx=0}$.
\item If $f$ is even, then $\displaystyle{\int_{-a}^a f(x)\,dx=2\int_{0}^af(x)\,dx}$.
\end{enumerate}
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (-2,0) -- plot [domain=-2:2] (\x, {2*sin(\x r)}) -- (2,0) -- cycle;
\draw [->] (-3.5,0) -- (3.5,0) ;
\node [above right] at (3.5,0) {$x$} ;
\draw [->] (0,-2.5) -- (0,2.5) ;
\node [above right] at (0,2.5) {$y$} ;
\draw [thick, domain=-3:3, samples=50] plot (\x, {2*sin(\x r)}) ;   % \x r for radian

\draw [dashed] (-2,0) -- (-2, {2*sin(-2 r)}) ;
\draw [dashed] (2,0) -- (2, {2*sin(2 r)}) ;

\node [below right] at (0,0) {$O$} ;
\node [below] at (2,0) {$a$} ;
\node [above] at (-2,0) {$-a$} ;

\begin{scope}[xshift=8.5cm]
\fill[fill=yellow] (-2,0) -- plot [domain=-2:2] (\x, {2*exp(-\x*\x/10)+0.2}) -- (2,0) -- cycle;
\draw [->] (-3.5,0) -- (3.5,0) ;
\node [above right] at (3.5,0) {$x$} ;
\draw [->] (0,-1) -- (0,3) ;
\node [above right] at (0,3) {$y$} ;
\draw [thick, domain=-3:3, samples=50] plot (\x, {2*exp(-\x*\x/10)+0.2}) ;

\draw [dashed] (-2,0) -- (-2, {2*exp(-4/10)+0.2}) ;
\draw [dashed] (2,0) -- (2, {2*exp(-4/10)+0.2}) ;

\node [below left] at (0,0) {$O$} ;
\node [below] at (2,0) {$a$} ;
\node [below] at (-2,0) {$-a$} ;
\end{scope}

\end{tikzpicture}
\caption{Integral of symmetric functions}
\label{symm integral}
\end{center}
\end{figure}

\begin{problem} Compute $\displaystyle{\int_{-2}^2 \left(\frac{1}{\sqrt{2}}x^{101}-100x^{97}+23x^5+3x^2+4\right)dx}$.
\end{problem}

\begin{answer}
Note that the function $\frac{1}{\sqrt{2}}x^{101}-100x^{97}+23x^5+3x^2+4$ can be written as the sum of $f$ and $g$, where
$f(x)=\frac{1}{\sqrt{2}}x^{101}-100x^{97}+23x^5$ and $g(x)=3x^2+4$, Since $f$ is odd and $g$ is even, it follows that
$$\int_{-2}^2 \left(\frac{1}{\sqrt{2}}x^{101}-100x^{97}+23x^5+3x^2+4\right)dx=2\int_{0}^2 g(x)\,dx=2[x^3+4x]_0^2=32.$$
\end{answer}

Theorem \ref{integral properties} gives a way to integrate piecewise defined functions.

\begin{example}
Let $f:[-2,5]\to \mathbb{R}$ be defined by $$f(x)=\begin{cases} 1, & -2\leq x < 1, \\ \frac{x^2}{2}, & 1\leq x <3, \\ 4-x, & 3\leq x \leq 5. \end{cases}$$
Consider a function $g:[-2,5]\to \mathbb{R}$ defined by $\displaystyle{g(x)=\int_{-2}^x f(t)\,dt}$ (see Figure \ref{piecewise}). It is clear that we need to treat cases $x<1$, $1\leq x <3$, and $x\geq 3$ separately. For $x<1$, $\displaystyle{g(x)=\int_{-2}^x \,dt=x+2}$. If $1\leq x < 3$, then $\displaystyle{g(x)=\int_{-2}^1\,dt+\int_1^x\frac{t^2}{2}\,dt=\frac{x^3+17}{6}}$. If $x\geq 3$, then  $\displaystyle{g(x)=\int_{-2}^1\,dt+\int_1^3\frac{t^2}{2}\,dt+\int_3^x (4-t)\,dt=4x-\frac{x^2}{2}-\frac{1}{6}}$.
\end{example}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (-2,0) -- (-2,1) -- (1,1) -- (1,0) -- cycle;
\fill[fill=yellow] (1,0) -- plot [domain=1:2.5] (\x,{\x^2/2}) -- (2.5,0) -- cycle;
\draw [->] (-2.5,0) -- (5.5,0) ;
\node [above right] at (5.5,0) {$x$} ;
\draw [->] (0,-1.5) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\draw [thick] (-2,1) -- (1,1) ;
\draw [thick, domain=1:3] plot (\x, {\x^2/2}) ;
\draw [thick, domain=3:5] plot (\x, {4-\x}) ;
\draw [dashed] (-2,1) -- (-2,0) ;
\draw [dashed] (5,-1) -- (5,0) ;
\draw [dashed] (1,1) -- (1,0) ;
\draw [dashed] (3,4.5) -- (3,0) ;
\draw [dashed] (0,4.5) -- (3,4.5) ;
\draw [dashed] (0,-1) -- (5,-1) ;
\draw [dashed] (2.5,2.5^2/2) -- (2.5,0) ;
\draw [fill=black] (1,1) circle (3pt) ;
\draw [fill=white] (1,1) circle (2.5pt) ;
\draw [fill=black] (3,4.5) circle (3pt) ;
\draw [fill=white] (3,4.5) circle (2.5pt) ;
\draw [fill=black] (1,0.5) circle (3pt) ;
\draw [fill=black] (3,1) circle (3pt) ;
\node [below] at (-2,0) {$-2$} ;
\node [below] at (1,0) {$1$} ;
\node [below] at (3,0) {$3$} ;
\node [below] at (4,0) {$4$} ;
\node [below] at (2.5,0) {$x$} ;
\node [above] at (5,0) {$5$} ;
\node [below left] at (0,0) {$O$} ;
\node [above left] at (0,1) {$1$} ;
\node [left] at (0,4.5) {$\frac{9}{2}$} ;
\node [left] at (0,-1) {$-1$} ;
\node [left] at (2.6,2.6^2/2) {$y=f(x)$} ;
\end{tikzpicture}
\end{center}
\caption{A piecewise defined function}
\label{piecewise}
\end{figure}

\begin{problem} Determine a constant $c$ so that the function $\displaystyle{\int_0^2 (c-2|x-1|)\,dx=1}$.
\end{problem}

\begin{answer} Since
$$ \int_0^2 (c-2|x-1|)\,dx =\int_0^1 (c-2(1-x))\,dx + \int_1^2 (c-2(x-1))\,dx=2c-2, $$
it follows that $c=\frac{3}{2}$.
\end{answer}

\begin{exercise} \label{integral exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item Determine a constant $a$ that minimizes the value of $\displaystyle{\int_a^{a+1}|x|\,dx}$.

\item Let $p,q>1$ be such that $\frac{1}{p}+\frac{1}{q}=1$. Prove Young's Inequality\index{Young's Inequality}:
\begin{equation}\label{young ineq formula}
ab\leq \frac{a^p}{p}+\frac{b^q}{q}
\end{equation}
for all nonnegative numbers $a$ and $b$, with equality when $a^p=b^q$. \\
\textit{Hint}: Note that $\frac{1}{p-1}=q-1$ and $\frac{1}{q-1}=p-1$. Consider the geometric interpretation of both sides of (\ref{young ineq formula}) using Figure \ref{young inequality}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45, declare function={gg(\x) = \x^2/3 ;}]

\fill[fill=yellow] (0,0) -- plot [domain=0:3] (\x, {gg(\x)}) --(3,0) -- cycle;
\def\bbb{2.5}
\fill[fill=cyan] (0,{gg(\bbb)}) -- (\bbb,{gg(\bbb)}) -- plot [domain=0:\bbb] (\x, {gg(\x)}) --(0,0) -- cycle;
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,6) ;
\node [above right] at (0,6) {$y$} ;

\draw [thick, domain=0:4, samples=50] plot (\x, {gg(\x)}) ;
\draw [dashed] (3,0) -- (3, {gg(3)}) -- (0, {gg(3)}) ;
\draw [dashed] (0,{gg(\bbb)}) -- (3,{gg(\bbb)}) ;

\node [below] at (3,0) {$a$};
\node [left] at (0, {gg(3)}) {$a^{p-1}$};
\node [left] at (0,{gg(\bbb)}) {$b$};
\node [right] at (4, {gg(4)}) {\parbox{2cm}{$y=x^{p-1}$ \\ $x=y^{q-1}$}} ;
\node [below left] at (0,0) {$O$} ;

\begin{scope}[xshift=8cm]
\fill[fill=yellow] (0,0) -- plot [domain=0:3] (\x, {gg(\x)}) --(3,0) -- cycle;
\def\bbb{3.5}
\fill[fill=cyan] (0,{gg(\bbb)}) -- (\bbb,{gg(\bbb)}) -- plot [domain=0:\bbb] (\x, {gg(\x)}) --(0,0) -- cycle;
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,6) ;
\node [above right] at (0,6) {$y$} ;

\draw [thick, domain=0:4, samples=50] plot (\x, {gg(\x)}) ;
\draw [dashed] (3, {gg(3)}) -- (0, {gg(3)}) ;
\draw [dashed] (0,{gg(\bbb)}) -- (\bbb,{gg(\bbb)}) ;
\draw [dashed] (3,0) -- (3, {gg(\bbb)}) ;

\node [below] at (3,0) {$a$};
\node [left] at (0, {gg(3)}) {$a^{p-1}$};
\node [left] at (0,{gg(\bbb)}) {$b$};
\node [right] at (4, {gg(4)}) {\parbox{2cm}{$y=x^{p-1}$ \\ $x=y^{q-1}$}} ;
\node [below left] at (0,0) {$O$} ;
\end{scope}

\end{tikzpicture}
\caption{Young's inequality}
\label{young inequality}
\end{center}
\end{figure}

\item Determine a constant $c$ so that $\displaystyle{\int_{0}^{1} \frac{c}{1+x^2}\,dx=1}$.
\item Compute $\displaystyle{\lim_{x\to 0}\frac{1}{e^x-1}\int_0^x e^{t^2}\,dt}$. \\\textit{Hint}: Combine L'Hopital's Theorem and the Fundamental Theorem of Calculus.
\item \label{integral order preserving} Suppose $f(x)\geq g(x)$ for all $x$ in $[a,b]$. Show that $\displaystyle{\int_a^b f(x)\,dx\geq \int_a^b g(x)\,dx}$. \\\textit{Hint}: Consider $\displaystyle{\int_a^b h(x)\,dx}$, where $h(x)=f(x)-g(x)$.

\item Consider a piecewise defined function
$$f(x)=\begin{cases} x^2+2, &x\leq 0, \\ e^{-x}, &x>0. \end{cases}$$
For $y\in \mathbb{R}$, define $\displaystyle{g(y)=\int_{-2}^y f(x)\,dx}$.
\begin{enumerate}
\item Compute $g(1)$.
\item Is $g$ continuous at $0$?
\item Is $g$ differentiable at $0$?
\end{enumerate}

\end{enumerate}
\end{exercise}

\section{Techniques of Integration}
In this section we discuss several techniques of finding indefinite integrals. We begin with Integration by Substitution. Consider the composition $f(g(x))$ of two differentiable functions $f$ and $g$. By the Chain Rule, we know that
$$(f(g(x)))'=f'(g(x))g'(x),$$
and this gives
$$\int f'(g(x))g'(x)\,dx=f(g(x))+C.$$

\begin{example} \label{subs example 1}
Consider $\displaystyle{\int 2x\cos (x^2)\,dx}$. Note that $2x\cos (x^2)=f'(g(x))g'(x)$, where $f(x)=\sin x$ and $g(x)=x^2$. Therefore,  $\displaystyle{\int 2x\cos (x^2)\,dx=\sin(x^2)+C}$.
\end{example}

In practice, it is hard to pick out $f$ and $g$. \underline{Integration by Substitution}\index{Integration by Substitution} is a technique with which we can resolve this problem. \\
\begin{center}
\fbox{
\begin{minipage}{6 in}\vspace{0.3cm}\textbf{Integration by Substitution}
\begin{itemize}
\item Step 1: Pick a function inside a function and call it $u$.
\item Step 2: Find $\frac{du}{dx}$, and express $dx$ in terms of $du$, treating $\frac{du}{dx}$ as a fraction.
\item Step 3: Replace $dx$ and all $x$'s using $du$ and $u$'s. Now you have an integral of a function of $u$.
\item Step 4: Integrate and replace all $u$'s with functions of $x$.
\item Step 5: Check your answer by differentiating it.
\end{itemize}
\vspace{0.1cm}
\end{minipage}
 }
\end{center}
\medskip
\begin{example} Following the steps described above, we redo Example \ref{subs example 1}. One can easily note that $x^2$ is a function inside another function, say $\cos x$, so we let $u=x^2$. Now $\frac{du}{dx}=2x$ and $dx=\frac{du}{2x}$, so we get $$\int 2x\cos (\red{x^2})\,\blue{dx}=\int 2x\cos \red{u}\,\blue{\frac{du}{2x}}=\int \cos u\,du.$$ The last integral contains only $u$'s and can be easily integrated: $\displaystyle{\int \cos u\,du=\sin u + C}$. Replacing $u$ in terms of a function of $x$, we get that $\displaystyle{\int 2x\cos (x^2)\,dx=\sin(x^2)+C}$.
\end{example}

\begin{example}\label{defintbysub} Consider $\displaystyle{\int_{2}^3 (3x-7)^4\,dx}$. Note that $3x-7$ is a function inside another function, so we let $u=3x-7$ and get $\frac{du}{dx}=3$, which gives $dx=\frac{du}{3}$. It follows that $\displaystyle{\int (3x-7)^4\,dx=\int u^4 \,\frac{du}{3}=\frac{u^5}{15}+C=\frac{(3x-7)^5}{15}+C}$. In particular, it follows that $\displaystyle{\int_{2}^3 (3x-7)^4\,dx=\left[\frac{(3x-7)^5}{15}\right]_{2}^3=\frac{33}{15}}$.
\end{example}

\begin{example}\label{xexsquare}
Consider $\displaystyle{\int xe^{-x^2}\,dx}$. Let $u=-x^2$, then $\frac{du}{dx}=-2x$ and it follows that $\displaystyle{\int xe^{-x^2}\,dx=\int xe^u\left(-\frac{du}{2x}\right)=-\frac{1}{2}\int e^u\,du=-\frac{1}{2}e^u+C=-\frac{1}{2}e^{-x^2}+C}$.
\end{example}

There is also a definite integral version of Integration by Substitution. It is often called the \underline{Change of Variables Formula}.\index{Change of Variables Formula!one-dimensional}

\begin{thm} \label{1dcov}
Let $\varphi:[a,b]\to \mathbb{R}$ be a differentiable function. Then
$$\int_{\varphi(a)}^{\varphi(b)}f(u)\,du=\int_a^b f(\varphi(x))\varphi'(x)\,dx.$$
\end{thm}

\begin{example}
Consider $\displaystyle{\int_{2}^3 (3x-7)^4\,dx}$ (see Example \ref{defintbysub}). Let $\varphi(x)=3x-7$, then $\varphi(2)=-1$, $\varphi(3)=2$, and $\varphi'(x)=3$. By Theorem \ref{1dcov}, it follows that
$$\int_{2}^3 (3x-7)^4\,dx=\frac{1}{3}\int_{2}^3 (3x-7)^4\cdot 3\,dx=\frac{1}{3}\int_{-1}^2 u^4\,du=\left[\frac{u^5}{15}\right]_{-1}^2=\frac{33}{15}.$$
\end{example}

We now introduce another technique of integration. By the Product Rule, we get
$$(uv)'=u'v+uv'\quad\text{or}\quad uv'=(uv)'-u'v.$$
Integrating both sides, we get
$$\int uv'\,dx=uv-\int u'v\,dx.$$

\begin{example} Consider $\displaystyle{\int xe^x\,dx}$. Letting $u=x$ and $v=e^x$, we get that the given integral equals $\displaystyle{\int uv'\,dx}$, which in turn equals $uv-\displaystyle{\int u'v\,dx=xe^x-\int e^x\,dx=xe^x-e^x+C}$.
\end{example}

\begin{problem} Integrate $\displaystyle{\int x\cos x\,dx}$.
\end{problem}

\begin{answer}
Let $u=x$ and $v=\sin x$, then
$$\int x\cos x \,dx=\int uv'\,dx=uv-\int u'v\,dx=x\sin x -\int \sin x\,dx=x\sin x +\cos x +C.$$
\end{answer}

The procedure used in the previous examples is called the \underline{Integration by Parts}\index{Integration by Parts} and can be summarized as in the following:\\

\begin{center}
\fbox{
\begin{minipage}{6 in}\vspace{0.3cm}\textbf{Integration by Parts}
\begin{itemize}
\item Step 1: Break up the function into the product of $u$ and $v'$. The following chart helps in determining functions to be uses as $u$ or $v'$:
\begin{center}
\begin{tabular}{|m{1.1cm}|m{1.1cm}|m{4.4cm}|m{2.5cm}|m{1.1cm}|m{1.1cm}|} \hline
\begin{center}$ u $\end{center} & \begin{center}$\ln x$\end{center} & \begin{center}$\ldots,x^{-2},x^{-1},1,x,x^2,\ldots$ \end{center} & \begin{center}$\sin x,~\cos x$ \end{center} & \begin{center}$e^x$ \end{center} &\begin{center}$v'$ \end{center}\\
\hline
\end{tabular}
\end{center}
In general, the function that is easier to integrate is selected as $v'$.
\item Step 2: Find $u'$ by differentiating $u$. Find $v$ by integrating $v'$.
\item Step 3: Integrate $u'v$.
\item Step 4: Compute $\displaystyle{\int uv'\,dx=uv-\int u'v\,dx}$.
\end{itemize}
\vspace{0.1cm}
\end{minipage}
 }
\end{center}

\begin{problem} Integrate $\displaystyle{\int x \ln x\,dx}$.
\end{problem}

\begin{answer}
The integrand is the product of two functions, say $x$ and $\ln x$. According to the chart, $x$ is closer to $v'$ and $\ln x$ is closer to $u$, so we let $u=\ln x$ and $v'=x$, which gives $u'=\frac{1}{x}$ and $v=\frac{x^2}{2}$. It therefore follows that
$$\int x \ln x\,dx=(\ln x)\left(\frac{x^2}{2}\right)-\int \left(\frac{1}{x}\right)\left(\frac{x^2}{2}\right)\,dx=\frac{x^2}{2}\ln x- \frac{x^2}{4}+C.$$
\end{answer}

Some integrals require both Integration by Substitution and Integration by Parts.

\begin{example} Consider $\displaystyle{\int \frac{x}{e^{3x}}\,dx}$. We begin with Integration by Substitution. Letting $w=3x$, we get that $\frac{dw}{dx}=3$, or $dx=\frac{dw}{3}$, and the given integral becomes
$$\int \frac{x}{e^{3x}}\,dx=\int \frac{\frac{w}{3}}{e^w}\frac{dw}{3}=\frac{1}{9}\int we^{-w}\,dw.$$ Now using Integration by Parts with $u=w$ and $v'=e^{-w}$, we obtain
$$\int we^{-w}\,dw=-we^{-w}+\int e^{-w}\,dw=-we^{-w}-e^{-w}+C.$$
Finally, it follows that
$$\int \frac{x}{e^{3x}}\,dx=-\frac{1}{9}we^{-w}-\frac{1}{9}e^{-w}+C=-\frac{1}{9}(3x+1)e^{-3x}+C.$$
\end{example}

\begin{problem}\label{gammaprob}
Let $\alpha>0$. Show that $\displaystyle{\int x^\alpha e^{-x}\,dx=-x^\alpha e^{-x}+\alpha\int x^{\alpha-1}e^{-x}\,dx}$.
\end{problem}

\begin{answer}
Let $u=x^\alpha$ and $v'=e^{-x}$, then $u'=\alpha x^{\alpha-1}$ and $v=-e^{-x}$, so it follows that
$$\int x^\alpha e^{-x}\,dx=-x^\alpha e^{-x}+\alpha\int x^{\alpha-1}e^{-x}\,dx.$$
\end{answer}

There is also a definite integral version of Integration by Parts. It is straightforward to check
$$\int_a^b uv'\,dx=\left[uv\right]_a^b-\int_a^b u'v\,dx=u(b)v(b)-u(a)v(a)-\int_a^b u'v\,dx.$$

We end this section with some special shortcuts in Integration by Substitution.

\begin{thm} We have the following integrals:
\begin{enumerate}
\item $\displaystyle{\int \frac{f'(x)}{f(x)}}\,dx=\ln|f(x)|+C$.
\item If $F'(x)=f(x)$, then $\displaystyle{\int f(ax+b)\,dx=\frac{1}{a}F(ax+b)+C}$.
\end{enumerate}
\end{thm}

\begin{example}
Consider $\displaystyle{\int \frac{2x}{x^2+1}\,dx}$. Since $(x^2+1)'=2x$, it follows that
$$\int \frac{2x}{x^2+1}\,dx=\ln |x^2+1|+C=\ln(x^2+1)+C.$$
\end{example}

\begin{example}
Recall that the function inside a function in Example \ref{defintbysub} was $3x-7$, a linear function. Therefore, $\displaystyle{\int(3x-7)^4\,dx=\frac{1}{3}(3x-7)^5+C}$.
\end{example}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]

\item Integrate:
\begin{enumerate} \item $\displaystyle{\int x^2e^{x^3+4}\,dx}$.
\item $\displaystyle{\int x\sqrt{3-x^2}\,dx}$.
\item $\displaystyle{\int \frac{1}{x\ln x}\,dx}$.
\end{enumerate}
\item Integrate:
\begin{enumerate}
\item $\displaystyle{\int x^2 \ln x\,dx}$
\item $\displaystyle{\int \ln x\,dx}$. \\\textit{Hint}: $\ln x$ can be viewed as $1\cdot \ln x$.
\end{enumerate}
\item Integrate:
\begin{enumerate}
\item $\displaystyle{\int \frac{1}{x\ln x\ln(\ln x)} \,dx}$
\item $\displaystyle{\int (5x+11)^{\frac{1}{3}} \,dx}$.
\end{enumerate}
\item For $x\in \mathbb{R}$, define $f$ by $\displaystyle{f(x)=\int_0^x\ln(t^2+1)\,dt}$.
\begin{enumerate}
\item Compute $f(1)$.
\item Compute $\displaystyle{\lim_{x\to 0}\frac{f(x)}{x^3}}$.
\end{enumerate}

\end{enumerate}
\end{exercise}

\section{Improper Integrals}
The definition of integrals extends to infinite intervals and they are called \underline{improper integrals}\index{improper integral}.
There are various types of improper integrals, but we focus on one type, say, the improper integral of the form $\displaystyle{\int_a^\infty f(x)\,dx}$.

\begin{defi} Suppose that $f$ is a function defined on $[a,\infty)$. If $\displaystyle{\lim_{b\to \infty}\int_a^b f(x)\,dx}$ exists, then we say that $\displaystyle{\int_a^\infty f(x)\,dx}$ is \underline{convergent}\index{improper integral!convergent} and write
$$\int_a^\infty f(x)\,dx=\lim_{b\to \infty}\int_a^b f(x)\,dx.$$
If the limit does not exist, then we say that the integral is \underline{divergent}\index{improper integral!divergent}.
\end{defi}

\begin{remark}
$\displaystyle{\int_{-\infty}^a f(x)\,dx}$ is defined in a similar way: if $\displaystyle{\lim_{b\to -\infty} \int_{b}^a f(x)\,dx}$ exists, we say that $\displaystyle{\int_{-\infty}^a f(x)\,dx}$ is convergent and write
$$\int_{-\infty}^a f(x)\,dx=\lim_{b\to -\infty} \int_{b}^a f(x)\,dx.$$ Otherwise, $\displaystyle{\int_{-\infty}^a f(x)\,dx}$ is divergent. If both $\displaystyle{\int_{-\infty}^a f(x)\,dx}$ and $\displaystyle{\int_a^\infty f(x)\,dx}$ are convergent for some $a$, then we say that $\displaystyle{\int_{-\infty}^\infty f(x)\,dx}$ is convergent and write
$$\int_{-\infty}^\infty f(x)\,dx=\int_{-\infty}^a f(x)\,dx+\int_a^\infty f(x)\,dx.$$
We also write $\displaystyle{\int_\mathbb{R}f(x)\,dx}$ for $\displaystyle{\int_{-\infty}^\infty f(x)\,dx}$.
\end{remark}

\begin{example}\label{sens_spec}
Consider a binary classfication in which prediction is based on \textit{score}, a continuous random variable $X$. To be more precise, for a fixed quantity $T$, let an instance be classified as \textit{positive} if $X>T$,  and \textit{negative} if $X\leq T$. Suppose that $X$ follows a pdf $f_{+}$ (respectively, $f_{-}$) if the instance actually belongs to \textit{positive} (respectively, \textit{negative}). Then the \underline{sensitivity}\index{sensitivity} $sens(T)$ of the test is given by 
$$sens(T)=P(X>T|+)=\int_T^\infty f_{+}(x)\,dx.$$
On the other hand, the \underline{specificity}\index{specificity} $spec(T)$ of the test is given by
$$spec(T)=P(X\leq T|-)=\int_{-\infty}^T f_{-}(x)\,dx.$$
As $T$ decreases from $\infty$ to $-\infty$, $sens(T)$ increases from $0$ to $1$ and $spec(T)$ decreases from $1$ to $0$. The parametric curve given by coordinates $(1-spec(T),sens(T))$ is called the \underline{receiver operating characteristic} (ROC) curve.\index{receiver operating characteristic curve}\index{ROC} Typical ROC curves are given in Figure \ref{ROC}. Note that the ROC curves reflect a trade off between sensitivity and specificity. The area under the ROC curve has a special meaning. See Example \ref{area_under_roc}.
\end{example}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=5, yscale=5, >=triangle 45]
\draw [->] (-0.1,0) -- (1.1,0) ;
\node [above right] at (1.1,0) {$1-spec(T)$} ;
\draw [->] (0,-0.1) -- (0,1.1) ;
\node [above right] at (0,1.1) {$sens(T)$} ;
\draw [thick, red, domain=0:1, samples=80] plot (\x, {sqrt(2*\x-\x^2)}) ;
\draw [thick, blue, domain=0:1, samples=80] plot (\x, {(1-(1-\x)^3)^(1/3)}) ;
\draw [thick, domain=0:1, samples=80] plot (\x, {(1-(1-\x)^4)^(1/4)}) ;
\draw [dashed] (1,0) -- (1,1) -- (0,1) ;
\node [below] at (1,0) {$1$} ;
\node [left] at (0,1) {$1$} ;
\end{tikzpicture}
\end{center}
\caption{Typical ROC curves. For any \textit{cut off} value $T$, the test represented by black curve performs better than \blue{blue}, and \blue{blue} performs better than \red{red}.}
\label{ROC}
\end{figure}

\begin{example} Consider $\displaystyle{\int_1^\infty \frac{1}{x^2}\,dx}$. Since $\displaystyle{\int_1^b \frac{1}{x^2}\,dx=1-\frac{1}{b}\to 0}$ as $b\to \infty$, the given improper integral exists and $\displaystyle{\int_1^\infty \frac{1}{x^2}\,dx=1}$. However, since $\displaystyle{\int_2^b \frac{1}{x}\,dx=\ln b -\ln 2\to \infty}$ as $b\to\infty$, $\displaystyle{\int_2^\infty \frac{1}{x}\,dx}$ is divergent.
\end{example}

\begin{problem} Compute $\displaystyle{\int_0^\infty xe^{-x}\,dx}$, if it is convergent.
\end{problem}

\begin{answer}
Using Integration by Parts, one can easily see that $\displaystyle{\int xe^{-x}\,dx}=-(x+1)e^{-x}+C$. Therefore, it follows that $\displaystyle{\int_0^b xe^{-x}\,dx=\left[-(x+1)e^{-x}\right]_0^b=-(b+1)e^{-b}+1}$. Finally,
$$\int_0^\infty xe^{-x}\,dx=\lim_{b\to \infty} (-(b+1)e^{-b}+1)=1$$ by L'Hopital's Theorem.
\end{answer}

\begin{problem}
For a fixed positive constant $\lambda$, define
$$f(x)=\begin{cases} \frac{\lambda}{2}e^{-\lambda x}, &x\geq 0, \\ \frac{\lambda}{2}e^{\lambda x}, &x<0.\end{cases}$$
\begin{enumerate}
\item Show that $\displaystyle{\int_{-\infty}^\infty f(x)\,dx=1}$.
\item Compute $\displaystyle{\int_{-\infty}^x f(t)\,dt}$.
\end{enumerate}
\end{problem}

\begin{answer}
\begin{enumerate}
\item
\begin{align*}
\int_{-\infty}^{\infty} f(x)\,dx&=\frac{1}{2}\int_{-\infty}^0 \lambda e^{\lambda x}\,dx+\frac{1}{2}\int_0^{\infty} \lambda e^{-\lambda x}\,dx \\
&=\frac{1}{2} \left[e^{\lambda x}\right]_{-\infty}^{0}-\frac{1}{2} \left[e^{-\lambda x}\right]_0^{\infty} \\
&=1.
\end{align*}
\item Let $F(x)=\displaystyle{\int_{-\infty}^x f(t)\,dt}$. For $x<0$, $$F(x)=\int_{-\infty}^x \frac{1}{2}\lambda e^{\lambda t}\,dt=\frac{1}{2}e^{\lambda x}.$$ For $x\geq 0$,
$$F(x)=\int_{-\infty}^0 \frac{1}{2}\lambda e^{\lambda t}\,dt+\int_0^{x} \frac{1}{2} \lambda e^{-\lambda t}\,dt=\frac{1}{2}+\left(-\frac{1}{2}e^{-\lambda x}+\frac{1}{2}\right)=1-\frac{1}{2}e^{-\lambda x}.$$
\end{enumerate}
\end{answer}

\begin{problem}
Suppose that $f$ is an even function such that $\displaystyle{\int_{-\infty}^\infty f(x)\,dx=1}$.
\begin{enumerate}
\item Compute $\displaystyle{\int_{0}^\infty f(x)\,dx}$.
\item If $\displaystyle{\int_{-\infty}^2 f(x)\,dx=0.8}$, then what is $\displaystyle{\int_{-2}^0 f(x)\,dx}$?
\end{enumerate}
\end{problem}

\begin{answer}
\begin{enumerate}
\item Since $f$ is even, $\displaystyle{\int_{0}^\infty f(x)\,dx=\int_{-\infty}^0 f(x)\,dx}$. From
$$1=\int_{-\infty}^\infty f(x)\,dx=\int_{0}^\infty f(x)\,dx+\int_{-\infty}^0 f(x)\,dx,$$
it follows that $\displaystyle{\int_{0}^\infty f(x)\,dx=\int_{-\infty}^0 f(x)\,dx=0.5}$.
\item Note that $\displaystyle{\int_{0}^2 f(x)\,dx=\int_{-\infty}^2 f(x)\,dx-\int_{-\infty}^0 f(x)\,dx=0.8-0.5=0.3}$.
Since $f$ is even, we get $\displaystyle{\int_{-2}^0 f(x)\,dx=\int_{0}^2 f(x)\,dx=0.3}$. See Figure \ref{even improper}.
\end{enumerate}
\end{answer}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill[fill=yellow] (-5,0) -- plot [domain=-5:2] (\x, {2*exp(-\x*\x/10)+0.2}) -- (2,0) -- cycle;
\draw [->] (-5,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,3) ;
\node [above right] at (0,3) {$y$} ;
\draw [thick, domain=-5:5, samples=50] plot (\x, {2*exp(-\x*\x/10)+0.2}) ;

\draw [dashed] (-2,0) -- (-2, {2*exp(-4/10)+0.2}) ;
\draw [dashed] (2,0) -- (2, {2*exp(-4/10)+0.2}) ;

\node [below left] at (0,0) {$O$} ;
\node [below] at (2,0) {$2$} ;
\node [below] at (-2,0) {$-2$} ;

\end{tikzpicture}
\caption{Improper integral of an even function}
\label{even improper}
\end{center}
\end{figure}

\begin{example}\label{gammafcndef}
For $\alpha>0$, we define the \underline{gamma function}\index{gamma function} $\Gamma$ by $\displaystyle{\Gamma(\alpha)=\int_{0}^\infty x^{\alpha-1}e^{-x}\,dx}$. By Problem \ref{gammaprob},
\begin{align*}\Gamma(\alpha+1)&=\lim_{b\to \infty}\int_{0}^b x^{\alpha}e^{-x}\,dx\\
&=\lim_{b\to \infty}\left(\left[-x^\alpha e^{-x}\right]_0^b+\alpha\int_0^b x^{\alpha-1}e^{-x}\,dx\right)\\
&=-\lim_{b\to \infty}\frac{b^\alpha}{e^b}+\alpha\lim_{b\to \infty}\int_0^b x^{\alpha-1}e^{-x}\,dx.\end{align*}
Here $\displaystyle{\lim_{b\to \infty}\frac{b^\alpha}{e^b}=0}$ by L'Hopital's Theorem, and $\displaystyle{\lim_{b\to \infty}\int_0^b x^{\alpha-1}e^{-x}\,dx=\Gamma(\alpha)}$, so it follows that
\begin{equation}\label{gammaidentity}\Gamma(\alpha+1)=\alpha\Gamma(\alpha).\end{equation}
\end{example}
One can easily show that $\displaystyle{\Gamma(1)=\int_0^\infty e^{-x}\,dx=1}$, so by (\ref{gammaidentity}) and mathematical induction, we obtain $$\Gamma(n)=(n-1)!$$ for a positive integer $n$.

\begin{example}
A continuous random variable $X$ is said to follow the \underline{Gamma Distribution}\index{Gamma Distribution} with the shape parameter $\alpha$ and the scale parameter $\beta$, denoted $X\sim Gamma(\alpha,\beta)$, if its pdf is given by
$$f(x|\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac{x}{\beta}},\quad 0<x<\infty,\quad \alpha>0,\quad \beta>0.$$ We show that $f$ is indeed a density, that is, we show that $\displaystyle{\int_0^\infty \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac{x}{\beta}}\,dx=1}$. Starting with Integration by Substitution $y=\frac{x}{\beta}$, we get $\displaystyle{\int_0^b x^{\alpha-1}e^{-\frac{x}{\beta}}\,dx=\int_0^{\frac{b}{\beta}} \beta^\alpha y^{\alpha-1}e^{-y}\,dy}$, so it follows that
$$\int_0^\infty \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac{x}{\beta}}\,dx= \frac{1}{\Gamma(\alpha)\beta^\alpha}\int_0^\infty \beta^\alpha y^{\alpha-1}e^{-y}\,dy=\frac{1}{\Gamma(\alpha)\beta^\alpha}\cdot\beta^\alpha\Gamma(\alpha)=1.$$
\end{example}

\begin{example}
The \underline{beta function}\index{beta function} $B$ is defined for $\alpha>0$, $\beta>0$ by
$$B(\alpha,\beta)=\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\,dx.$$
It is easy to show that $B$ is symmetric: $B(\alpha,\beta)=B(\beta,\alpha)$. In fact, it can be shown that
$$B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}.$$
A continuous random variable $X$ is said to follow \underline{Beta Distribution}\index{Beta Distribution}, denoted $X\sim Beta(\alpha,\beta)$,  if its pdf is given by
$$f(x|\alpha,\beta)=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1},\quad 0<x<1,\quad \alpha>0,\quad \beta>0.$$
\end{example}

\begin{example} \label{def of exp dist}
A continuous random variable $X$ is said to follow \underline{Exponential Distribution}\index{Exponential Distribution} with parameter $\lambda$, denoted $X\sim Exp(\lambda)$, if its pdf is given by
$$f(x|\lambda)=\frac{1}{\lambda} e^{-\frac{x}{\lambda}},\quad x>0.$$
It can be viewed as a special case of the Gamma Distribution. To be precise, $Exp(\lambda)$ distribution is the same as $Gamma(1,\lambda)$ distribution. It therefore follows that $E(X)=\lambda$ and $Var(X)=\lambda^2$. See Exercise \ref{improperexer}.\ref{gammaexp}.
\end{example}

\begin{example}
A continuous random variable $X$ is said to follow \underline{Chi-squared Distribution}\index{Chi-squared Distribution} with $k$ degrees of freedom, denoted $X\sim \chi^2(k)$, if its pdf is given by
$$f(x|k)=\frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^\frac{k}{2}\Gamma(\frac{k}{2})},\quad x>0.$$
Note that $\chi^2(k)$ distribution is the same as $\Gamma(\frac{k}{2},2)$.
\end{example}

\begin{example} A continuous random variable $X$ is said to follow \underline{Normal Distribution}\index{Normal Distribution} with mean $\mu$ and variance $\sigma^2$, $\sigma>0$, denoted $X\sim N(\mu,\sigma^2)$, if its pdf is given by $$f(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\quad -\infty<x<\infty.$$
One can show that this is indeed a pdf (see Exercise \ref{double integral exercise}.\ref{normaldensity}). In particular, if $\mu=0$ and $\sigma=1$, then $X$ is said to follow the \underline{Standard Normal Distribution}\index{Standard Normal Distribution}.
\end{example}

\begin{problem}
Suppose that $Y=\frac{1}{X}$, where $X\sim Gamma(\alpha,\beta)$ with $\alpha>1$ and $\beta>0$. Compute $E(Y)$.
\end{problem}

\begin{answer}
We need to compute $\displaystyle{\int_0^\infty \frac{1}{x}\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac{x}{\beta}}\,dx=\int_0^\infty \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-2}e^{-\frac{x}{\beta}}\,dx}$. Note that the latter integrand is a constant multiple of a $Gamma(\alpha-1,\beta)$ density, so it is natural to transform
$\displaystyle{\frac{1}{\red{\Gamma(\alpha)}\blue{\beta^\alpha}}x^{\alpha-2}e^{-\frac{x}{\beta}}}$ into $\displaystyle{\frac{1}{\blue{\beta}\red{(\alpha-1)}}\cdot\frac{1}{\red{\Gamma(\alpha-1)}\blue{\beta^{\alpha-1}}}x^{\alpha-2}e^{-\frac{x}{\beta}}}$ and it follows that
$$\int_0^\infty \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-2}e^{-\frac{x}{\beta}}\,dx=\frac{1}{\beta(\alpha-1)}\int_0^\infty \underbrace{\frac{1}{\Gamma(\alpha-1)\beta^{\alpha-1}}x^{\alpha-2}e^{-\frac{x}{\beta}}}_{\text{pdf of }Gamma(\alpha-1,\beta)}\,dx=\frac{1}{\beta(\alpha-1)}.$$
\end{answer}

\begin{exercise}\label{improperexer} \quad
\begin{enumerate}[\bfseries 1.]

\item A continuous random variable $X$ is said to follow the \underline{Cauchy Distribution}\index{Cauchy Distribution} if its pdf is given by $f(x)=\frac{1}{\pi(1+x^2)}$, $-\infty<x<\infty$. Show that $f$ is indeed a pdf, that is, show that $\displaystyle{\int_{-\infty}^\infty \frac{dx}{\pi(1+x^2)}=1}$.

\item Determine the value of a constant $c$ so that
$$\int_0^\infty \frac{c\ln (2x+1)}{4x^2+4x+1}\,dx=1.$$

\item Compute $\displaystyle{\int_0^\infty xe^{-x^2}\,dx}$ and $\displaystyle{\int_{-\infty}^\infty xe^{-x^2}\,dx}$. \\\textit{Hint}: See Example \ref{xexsquare}.

\item Compute $\displaystyle{\int_0^\infty \frac{e^x}{(1+e^x)^2}\,dx}$.

\item\label{gammaexp} Suppose that $X$ follows $Gamma(\alpha,\beta)$ distribution. Show that $E(X)=\alpha\beta$ and $Var(X)=\alpha\beta^2$.

\item Let $M(t)=\displaystyle{\int_0^\infty \frac{1}{4}xe^{(t-\frac{1}{2})x}\,dx}$. Show that $M(t)$ converges if and only if $t<\frac{1}{2}$ and when $t<\frac{1}{2}$, show that $M(t)=\frac{1}{(2t-1)^2}$.

\item Suppose that $X$ follows $B(\alpha,\beta)$ distribution. Show that
$$E(X)=\frac{\alpha}{\alpha+\beta}\quad\text{and}\quad Var(X)=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.$$
\end{enumerate}
\end{exercise}

\section{Convergence of Sequences and Series}

In probability, there are several modes of convergence that describe the limit behavior of a sequence of random variables, and some of them can be defined in terms of the limit of a sequence of real numbers. For example, a sequence $(X_n)$ of random variables is said to \underline{converge} to $X$ \underline{in probability}\index{convergence in probability} if for any $\epsilon>0$,
$$\lim_{n\to \infty} P(|X_n-X|\geq \epsilon)=0.$$
$(X_n)$ is said to \underline{converge} to $X$ \underline{in distribution} or \underline{in law}\index{convergence in distribution}\index{convergence in law} if
$$\lim_{n\to \infty} F_n(x)=F(x)$$
for all $x$ at which $F$ is continuous, where $F_n$ and $F$ are cdf's of $X_n$ and $X$, respectively. The convergence of a sequence is defined in a way similar to Definition \ref{lim at inf} and Remark \ref{inf as limit}.

\begin{defi}
Let $(a_n)$ be a sequence of real numbers. We say that the \underline{limit}\index{limit of a sequence} of $(a_n)$ is $\ell$ if for every $\epsilon>0$, there is a corresponding $N$ such that $n>N$ implies $|a_n-\ell|<\epsilon$. In this case, we write $\displaystyle{\lim_{n\to \infty} a_n=\ell}$ or $a_n \to \ell$. We write $\displaystyle{\lim_{n\to \infty} a_n=\infty}$ (respectively, $\displaystyle{\lim_{n\to \infty} a_n=-\infty}$) or $a_n \to \infty$ (respectively, $a_n\to -\infty$) if for every $M$, there exists $N$ such that $n>N$ implies $a_n>M$ (respectively, $a_n<M$).
\end{defi}

\begin{remark}
The same properties as in Theorem \ref{properties of limits} hold for limit of a sequence.
\end{remark}

\begin{example}
We claim that $\frac{1}{n}\to 0$. Let $\epsilon>0$, then there is $N$ such that $\frac{1}{N}<\epsilon$. It then follows that for any $n>N$,
$$\left|\frac{1}{n}-0\right|=\frac{1}{n}<\frac{1}{N}<\epsilon.$$
\end{example}

\begin{example}\label{Q dense}
Let $x\in \mathbb{R}$, then there exists a sequence $(q_n)$ such that $q_n\in \mathbb{Q}$ and $\displaystyle{\lim_{n\to \infty} q_n =x}$.
\end{example}

\begin{example} Some more examples of limits are in order.
\begin{multicols}{2}
\begin{enumerate}
\item $\displaystyle{\lim_{n\to\infty}}\frac{n+1}{2n}=\frac{1}{2}$.
\item $\displaystyle{\lim_{n\to\infty}}\frac{5n^2-100}{2n^2-3n+5}=\frac{5}{2}$.
\item $\displaystyle{\lim_{n\to\infty}}n^2=\infty$.
\item $\displaystyle{\lim_{n\to\infty}}\frac{(-1)^n}{n}=0$.
\item $\displaystyle{\lim_{n\to\infty}}\frac{\sin n}{n}=0$.
\item $\displaystyle{\lim_{n\to\infty}}(1+(-1)^n)$ does not exist.
\item $\displaystyle{\lim_{n\to\infty}}\left(\frac{1}{4}\right)^n=0$.
\item $\displaystyle{\lim_{n\to\infty}}\left(-\frac{1}{2}\right)^n=0$.
\item $\displaystyle{\lim_{n\to\infty}}\left(\frac{4}{3}\right)^n=\infty$.
\item $\displaystyle{\lim_{n\to\infty}}\left(-2\right)^n$ does not exist.
\end{enumerate}
\end{multicols}
\end{example}

\begin{remark} In summary we get
$$\lim_{n\to \infty} \frac{a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0}{b_nx^n+b_{n-1}x^{n-1}+\cdots+b_1x+b_0}=\frac{a_n}{b_n} \quad(\text{provided }b_n\neq 0)$$ and
$$\lim_{n\to \infty} r^n=\begin{cases} \infty & \text{if } r>1 \\
1 & \text{if } r=1 \\
0 & \text{if } -1<r<1 \\
\text{does not exist} &  \text{if } r\leq-1
\end{cases}$$
\end{remark}

\begin{problem} Examine the limit of each of the following sequences:
\begin{multicols}{2}
\begin{enumerate}
\item $a_n=\frac{1+3^n}{2-5^n}$
\item $b_n=\frac{2+4^{n}}{3^n}$
\item $c_n=\frac{6^n+3^{2n}}{3^n+5\cdot 9^n}$
\item $d_n=\frac{2^n+4^{n}}{3^n+2^n}$
\end{enumerate}
\end{multicols}
\end{problem}

\begin{answer}\quad
\begin{enumerate}
\item $\displaystyle{\lim_{n\to \infty}\frac{1+3^n}{2-5^n} = \lim_{n\to \infty}\frac{\frac{1}{5^n}+\left(\frac{3}{5}\right)^n}{\frac{2}{5^n}-1}=\frac{0+0}{0-1}=0}$.
\item $\displaystyle{\lim_{n\to \infty} \frac{2+4^{n}}{3^n}=\lim_{n\to \infty} \frac{\frac{2}{3^n}+\left(\frac{4}{3}\right)^n}{1} =\infty}$.
\item $\displaystyle{\lim_{n\to \infty}\frac{6^n+3^{2n}}{3^n+5\cdot 9^n}=\lim_{n\to \infty}\frac{\left(\frac{6}{9}\right)^n+1}{\left(\frac{3}{9}\right)^n+5}=\frac{1}{5}}$.
\item $\displaystyle{\lim_{n\to \infty}\frac{2^n+4^{n}}{3^n+2^n}=\lim_{n\to \infty}\frac{\left(\frac{2}{3}\right)^n+\left(\frac{4}{3}\right)^n}{1+\left(\frac{2}{3}\right)^n}=\infty}$.
\end{enumerate}
\end{answer}

Let $(a_n)$ be a sequence of real numbers. If $a_n\leq a_{n+1}$ (respectively, $a_n\geq a_{n+1}$) for all $n\geq 1$, we say that $(a_n)$ is \underline{nondecreasing}\index{nondecreasing} (respectively, \underline{nonincreasing}\index{nonincreasing}). Next theorem gives a condition on a sequence that guarantees the existence of the limit.

\begin{thm}\label{bddmonotone}
Suppose that $(a_n)$ is a nondecreasing sequence that is bounded from above, that is, there is $M$ such that $a_n\leq M$ for all $n\geq 1$. Then $\displaystyle{\lim_{n\to \infty}a_n}$ exists. Similarly, if $(a_n)$ is a nonincreasing sequence that is bounded from below, that is, there is $M$ such that $a_n\geq M$ for all $n\geq 1$, then $\displaystyle{\lim_{n\to \infty}a_n}$ exists.
\end{thm}

Sequences can be used to check continuity of a function.

\begin{thm}\label{seq conti}
Let $f$ be a function and $x_0\in \mathbb{R}$. Then $f$ is continuous at $x_0$ if and only if $\displaystyle{\lim_{n\to \infty} f(x_n)=f(x_0)}$ for all sequences $(x_n)$ such that $\displaystyle{\lim_{n\to \infty} x_n =x_0}$.
\end{thm}

\begin{problem}\label{cauchy functional equation} Suppose that $f:\mathbb{R}\to \mathbb{R}$ satisfies $f(a+b)=f(a)+f(b)$ for all real numbers $a$ and $b$. Suppose that $f$ is continuous at $x=0$. Show that $f(x)=\lambda x$ for some $\lambda\in \mathbb{R}$.
\end{problem}

\begin{answer} By Exercise \ref{conti exer}.\ref{Cauchy ftl}, $f$ is continuous \textit{everywhere}. Let $\lambda=f(1)$. Using Mathematical Induction, one can easily show that $f(nx)=nf(x)$ for all positive integer $n$ and for all $x\in \mathbb{R}$. Using this and the fact that $f(0)=0$, one can also get that $f(nx)=nf(x)$ for \textit{all} $n\in \mathbb{Z}$. Since $f(1)=f(n\cdot\frac{1}{n})$, we obtain that $f(\frac{1}{n})=\frac{\lambda}{n}$, $n\neq 0$, and that $f(\frac{m}{n})=\frac{m}{n}\lambda$, $m,n\in \mathbb{Z}$, $n\neq 0$. This shows that $f(q)=\lambda q$ for all $q\in \mathbb{Q}$. Finally, let $x\in \mathbb{R}$. By Example \ref{Q dense}, there is a sequence $(q_n)\subseteq \mathbb{Q}$ such that $\displaystyle{\lim_{n\to \infty} q_n =x}$ and by Theorem \ref{seq conti}, it follows that
$$f(x)=\lim_{n\to \infty}f(q_n)=\lim_{n\to \infty} \lambda q_n =\lambda x.$$
\end{answer}

Let $X$ be a continuous random variable. If
\begin{equation}\label{memless} P(X>s+t|X>s)=P(X>t)\end{equation} for all $s,t\geq 0$, then we say that $X$ has the \underline{memoryless property}\index{memoryless property}. The argument in the answer to Problem \ref{cauchy functional equation} can be used to show that the only class of random variables that has the memoryless property is the exponential distribution (see Example \ref{def of exp dist}). To see this, let  $F$ denote the cdf of $X$, then (\ref{memless}) implies that $$\frac{1-F(s+t)}{1-F(s)}=1-F(t)$$ for all $s,t\geq 0$. In particular, $1-F(0)=1$. Define $G:[0,\infty)\to \mathbb{R}$ by $G(x)=1-F(x)$, then $G(0)=1$ and
\begin{equation}\label{2nd cauchy} G(s+t)=G(s)G(t)\end{equation} for all $s,t\geq 0$. For any $x\geq 0$, substitution $s=t=\frac{x}{2}$ in (\ref{2nd cauchy}) gives $G(x)=(G(\frac{x}{2}))^2\geq 0$. If there exists $x_0\geq 0$ such that $G(x_0)=0$ (necessarily $x_0\neq 0$), then by (\ref{2nd cauchy}), we would have $G(\frac{x_0}{2^n})=0$ for $n=1,2,\ldots$ Since $G$ is right-continuous at $0$, it would imply that (a slight modification of Theorem \ref{seq conti})
$$G(0)=\lim_{n\to \infty}G\left(\frac{x_0}{2^n}\right)=0,$$  and this contradiction shows that $G(x)>0$ for all $x\geq 0$. Consequently $H:[0,\infty)\to \mathbb{R}$, $H(x)=\ln G(x)$ is well-defined. Note that $H(s+t)=H(s)+H(t)$ for all $s,t\geq 0$. Since $H$ is right-continuous at $0$, a slight modification of the answer to Problem \ref{cauchy functional equation} gives $H(x)=\mu x$ for some $\mu$. In other words, $F(x)=1-G(x)=1-e^{H(x)}=1-e^{-\frac{1}{\lambda} x}$, where $\lambda=-\frac{1}{\mu}$.\\

Next, we consider the sum of each term of a given sequence.

\begin{defi} Let $(a_n)$ be a sequence.  An \underline{infinite series}\index{infinite series} $\displaystyle{\sum_{n=1}^\infty a_n}$ is defined to be
$$\sum_{n=1}^\infty a_n=\lim_{k\to \infty}S_k,$$
where $\displaystyle{S_k=\sum_{n=1}^k a_n = a_1+a_2+\cdots+a_{k-1}+a_k}$, the \underline{$k^{\text{th}}$ partial sum}\index{partial sum}.  In other words,
$$\sum_{n=1}^\infty a_n=\lim_{k\to \infty}S_k=\lim_{k\to \infty} \sum_{n=1}^k a_n.$$
\end{defi}

\begin{example}
It is well known that the infinite series $\displaystyle{\sum_{n=1}^\infty \frac{1}{n^p}}$ converges if and only if $p>1$. In particular, the \underline{harmonic series}\index{harmonic series} $\displaystyle{\sum_{n=1}^\infty \frac{1}{n}}$ diverges.
\end{example}

The next theorem is an infinite series version of Theorem \ref{special summation}.

\begin{thm}\label{geometric series}
A geometric series\index{geometric series} with the initial term $a$ and the common ratio\index{common ratio} $r$ $$\sum_{n=1}^\infty ar^{n-1}=a+ar+ar^2+ar^3+\cdots$$
converges if and only if $-1<r<1$. When $-1<r<1$,
$$\sum_{n=1}^\infty ar^{n-1}=\frac{a}{1-r}.$$
\end{thm}

\begin{example} The geometric series $1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\cdots$ converges and equals $\frac{1}{1-\frac{1}{2}}=2.$
\end{example}

\begin{problem}\label{geo dist}
Let $0<p<1$. Prove that the infinite sum
$$\sum_{n=1}^\infty p(1-p)^{n-1}=p+p(1-p)+p(1-p)^2+p(1-p)^3+\cdots$$
converges. What is the sum?
\end{problem}

\begin{answer}
The given sum is a geometric series with the initial term $p$ and the common ratio $1-p$. Since $0<1-p<1$, the infinite sum converges. In fact, by Theorem \ref{geometric series}, we get that
$$p+p(1-p)+p(1-p)^2+p(1-p)^3+\cdots=\frac{p}{1-(1-p)}=1.$$
\end{answer}

\begin{remark}
Problem \ref{geo dist} shows that $P(X=n)=p(1-p)^{n-1}$, $n=1,2,\ldots$ defines a discrete random variable $X$. Such a random variable is said to have a \underline{Geometric Distribution}\index{Geometric Distribution} with parameter $p$.
\end{remark}

\begin{exercise} \quad
\begin{enumerate}[\bfseries 1.]

\item Examine the limit of each of the following sequences:
\begin{multicols}{2}
\begin{enumerate}
\item $a_n=\frac{8n^2}{9n^2-5n+1}$
\item $b_n=\frac{\ln n}{\sqrt{n}}$
\item $c_n=\frac{\sin n + (-1)^n}{n^3}$
\item $d_n=\frac{(-2)^n+5^{n}}{6^n+(-1)^n}$
\end{enumerate}
\end{multicols}

\item The purpose of this exercise is to prove that $\displaystyle{\lim_{n\to \infty}\left(1+\frac{1}{n}\right)^n}$ exists. Let $\displaystyle{s_n=\left(1+\frac{1}{n}\right)^n}$.

\begin{enumerate}
\item For a fixed integer $n>0$, let $f(x)=(n+1)x^n-nx^{n+1}$.
\begin{enumerate}
\item Compute $f(1)$.
\item Show that $f'(x)< 0$ for $x>1$.
\item Using i and ii, explain why the inequality
\begin{equation}\label{eqnonestar} x^n(n+1-nx)<1  \end{equation} holds for all $x>1$.
\end{enumerate}

\item Let $\displaystyle{x=\frac{1+\frac{1}{n}}{1+\frac{1}{n+1}}}$.
\begin{enumerate}
\item Show that $x>1$.
\item Substitute $x$ into (\ref{eqnonestar}) above and show that
\begin{equation}\label{eqntwostar}\left(\frac{1+\frac{1}{n}}{1+\frac{1}{n+1}}\right)^n\left(\frac{n+1}{n+2}\right)<1. \end{equation}
\item Using (\ref{eqntwostar}), show that $s_n<s_{n+1}$ for all $n\in \mathbb{N}$. This proves that $(s_n)$ is a nondecreasing sequence.
\end{enumerate}

\item Let $x=\displaystyle{1+\frac{1}{2n}}$.
\begin{enumerate}
\item Substitute $x$ into (\ref{eqnonestar}) to show that
\begin{equation}\label{eqnthreestar}\left(1+\frac{1}{2n}\right)^n<2.\end{equation}
\item Use (\ref{eqnthreestar}) to show that $s_{2n}<4$ for all $n\in \mathbb{N}$ $n>0$.
\item Show that $(s_n)$ is bounded from above.
\end{enumerate}

\item Use Theorem \ref{bddmonotone} show that $s_n$ converges. The limit, denoted by $e$, is called \underline{Euler's Number}\index{Euler's Number} and is the base of natural logarithm.
\end{enumerate}

\item When does the infinite series $1+2x+4x^2+8x^3+16x^4+\cdots$ converge? Simplify the sum.

\item Compute $\displaystyle{\frac{1}{2!}+\frac{2}{3!}+\frac{3}{4!}+\cdots=\sum_{k=1}^\infty\frac{k}{(k+1)!}}$.\\
\textit{Hint}: Note that $\displaystyle{\frac{k}{(k+1)!}=\frac{k+1-1}{(k+1)!}=\frac{1}{k!}-\frac{1}{(k+1)!}}$, so $\displaystyle{\sum_{k=1}^n \frac{k}{(k+1)!}=\frac{1}{1!}-\frac{1}{(n+1)!}}$.
\end{enumerate}

\end{exercise}

\section{Power Series}

Just as $2$ can be written as an infinite sum, say $2=1+\frac{1}{2}+\frac{1}{4}+\cdots$, some functions can be described in terms of infinite series. For example, some periodic functions can be written as its Fourier expansion, an infinite series involving trigonometric functions. In this section, we consider power series as a way of describing a certain class of functions.

\begin{defi} A \underline{power series}\index{power series} about $x=c$ is a series of the form
$$\sum_{n=0}^\infty a_n(x-c)^n=a_0+a_1(x-c)+a_2(x-c)^2+\cdots+a_n(x-c)^n+\cdots.$$
Here $c$ is called the \underline{center}\index{center of a power series} of the power series.
\end{defi}

\begin{example} $2+(x-1)+\frac{(x-1)^2}{2}+\frac{(x-1)^3}{3}+\frac{(x-1)^4}{4}+\cdots$ is a power series about $x=1$. Here $a_0=2$ and $a_n=\frac{1}{n}$ for $n\geq 1$.
\end{example}

\begin{example} $1+x+\frac{x^2}{2}+\frac{x^3}{6}+\frac{x^4}{24}+\frac{x^5}{120}+\cdots$ is a power series about $x=0$. Here $a_n=\frac{1}{n!}$ for all $n$.
\end{example}

\begin{remark} A power series can be regarded as a polynomial in $(x-c)$ with an \textit{infinite} degree.
\end{remark}

A power series is an infinite series with a variable $x$ in it, so its convergence depends on the value of $x$. Our first interest is in determining the values of $x$ for which a given power series converges.

\begin{example} The power series $\displaystyle{\sum_{n=0}^\infty \frac{x^n}{2^n}}$ converges when $x=-1$ because it is then a geometric series with the common ratio $-\frac{1}{2}$. However, if $x=3$, the series diverges because the common ratio $\frac{3}{2}$ is bigger than $1$.
\end{example}

\begin{problem}\label{firstpower} Find all $x$ such that the power series $\displaystyle{\sum_{n=0}^\infty \frac{x^n}{2^n}}$ converges.
\end{problem}

\begin{answer}
The first few terms of the series are $1+\frac{x}{2}+(\frac{x}{2})^2+(\frac{x}{2})^3+\cdots$, so the given sum is a geometric series with the initial term $1$ and the common ratio $\frac{x}{2}$. By Theorem \ref{geometric series}, it converges if and only if $-2<x<2$, and when $-2<x<2$, the series converges to $\frac{1}{1-\frac{x}{2}}=\frac{2}{2-x}$.
\end{answer}

It is easy to observe that a power series about $x=c$ converges for at least one $x$, say $x=c$. In fact, the following is known:

\begin{thm} Let a power series $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$ be given. Then exactly one of the following is true for the power series:
\begin{enumerate}
\item There is a positive number $R$, called the \underline{radius of convergence}\index{radius of convergence}, such that the series converges for $|x-c|<R$ and diverges for $|x-c|>R$.
\item The series converges only for $x=c$. In this case we say that the radius of convergence $R$ equals 0.
\item The series converges for all values of $x$. In this case we say that the radius of convergence $R$ equals $\infty$.
\end{enumerate}
\end{thm}

\begin{example} The radius of convergence for the power series in Problem \ref{firstpower} is $2$.
\end{example}

\begin{remark}
Let $R$ be the radius of convergence of a power series $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$. By definition, $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$ converges for all $x\in(c-R,c+R)$ and diverges for all $x\in (-\infty, c-R)\cup (c+R,\infty)$. The convergence at the boundary points $\{c-R, c+R\}$ depends on the power series. See Figure \ref{rad of conv}.
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw [->] (-5,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [ultra thick] (-2,0) -- (2,0) ;

\draw [dashed] (2,0) arc (45:135:{sqrt(2)}) ;
\draw [dashed] (0,0) arc (45:135:{sqrt(2)}) ;

\draw [fill=black] (2,0) circle (3pt) ;
\draw [fill=white] (2,0) circle (2.5pt) ;

\draw [fill=black] (-2,0) circle (3pt) ;
\draw [fill=white] (-2,0) circle (2.5pt) ;

\draw [fill=black] (0,0) circle (2pt) ;

\node [below] at (0,0) {$c$} ;
\node [below] at (2,0) {$c+R$} ;
\node [below] at (-2,0) {$c-R$} ;

\node [above] at (1,0.3) {$R$} ;
\node [above] at (-1,0.3) {$R$} ;

\end{tikzpicture}
\end{center}
\caption{Radius of convergence}
\label{rad of conv}
\end{figure}

There is an easy method for computing the radius of convergence.

\begin{thm} Let a power series $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$ be given.
\begin{enumerate}
\item If $\displaystyle{\lim_{n\to \infty}\frac{|a_{n+1}|}{|a_n|}}=\infty$, then $R=0$.
\item If $\displaystyle{\lim_{n\to \infty}\frac{|a_{n+1}|}{|a_n|}}=0$, then $R=\infty$.
\item If $\displaystyle{\lim_{n\to \infty}\frac{|a_{n+1}|}{|a_n|}}=K$, then $\displaystyle{R=\frac{1}{K}}$.
\end{enumerate}
\end{thm}

\begin{example}\label{log}
Consider the power series $$\sum_{n=1}^\infty \frac{(-1)^{n+1}x^n}{n}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots.$$
Since $$\lim_{n\to\infty}\frac{\left|\frac{(-1)^{n+2}}{n+1}\right|}{\left|\frac{(-1)^{n+1}}{n}\right|}=\lim_{n\to \infty}\frac{n}{n+1}=1,$$
the radius of convergence of the power series is equal to $1$.
\end{example}

Let $R$ be the radius of convergence of a power series $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$, then $f(x)=\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$ is a well-defined function on the interval $(c-R,c+R)$. In fact, more is true.

\begin{thm}\label{term by term}
Let $R$ be the radius of convergence of a power series $\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$. For $x\in (c-R, c+R)$, define $f(x)=\displaystyle{\sum_{n=0}^\infty a_n(x-c)^n}$, then
\begin{enumerate}
\item $f$ is differentiable on $(c-R,c+R)$ and $f'(x)=\displaystyle{\sum_{n=1}^\infty na_n(x-c)^{n-1}}$.
\item For any $y\in (c-R,c+R)$, $\displaystyle{\int_{c}^y f(x)\,dx=\sum_{n=0}^\infty \frac{a_n}{n+1}(y-c)^{n+1}}$.
\end{enumerate}
\end{thm}

\begin{remark}
Theorem \ref{term by term} means that power series can be termwise differentiable and integrable.
\end{remark}

\begin{example}
Let $$f(x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}x^n}{n}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots.$$
In Example \ref{log}, we observed that its radius of convergence is $1$. By Theorem \ref{term by term}, $f$ is differentiable on $(-1,1)$ and $f'(x)=1-x+x^2-x^3+\cdots$. Moreover, by Theorem \ref{geometric series}, it follows that
$$f'(x)=1-x+x^2-x^3+\cdots=\frac{1}{1-(-x)}=\frac{1}{1+x}.$$
Since $f(0)=0$, we see that $f$ must be a solution to a differential equation $f'(x)=\frac{1}{1+x}$, $f(0)=0$, so we conclude that $f(x)=\ln (1+x)$.
\end{example}

\begin{exercise}\label{power series exer} \quad
\begin{enumerate}[\bfseries 1.]
\item \label{xplusx2} When does $\displaystyle{\sum_{n=0}^\infty x^n = 1+x+x^2+x^3+\cdots}$ converge? Find the sum.
\item \label{termgeo} Find the radius of convergence of  $\displaystyle{\sum_{n=1}^\infty nx^{n-1} = 1+2x+3x^2+\cdots.}$ Compute the sum.\\
\textit{Hint}: Use the result from Exercise \ref{power series exer}.\ref{xplusx2} and Theorem \ref{term by term}.
\item Prove that $\displaystyle{\sum_{n=1}^\infty \frac{n^3}{2^n}=26}$. \\
\textit{Hint}: Let $f(x)=\displaystyle{\sum_{n=1}^\infty x^n}$. First, consider the derivative of $xf'(x)$. You may want to use Exercise \ref{power series exer}.\ref{termgeo}.
\item Let $X$ have a Geometric Distribution with parameter $p$ so that $P(X=n)=p(1-p)^{n-1}$, $n=1,2,\ldots$. Show that $E(X)=\frac{1}{p}$.\\
\textit{Hint}: Use Exercise \ref{power series exer}.\ref{termgeo}.
\end{enumerate}
\end{exercise}

\section{Taylor Series}
The Taylor series expansion of a function gives a way to approximate the function with a polynomial, which is in most cases easier to handle. In statistics, the so called Delta Method, which can be viewed as a generalized Central Limit Theorem applied to a function of asymptotically normal random variables, relies heavily on the Taylor expansion of the function. To motivate, consider a problem of approximating the graph of $y=f(x)$ by a linear function $y=P_1(x)=c_0+c_1(x-a)$ near $x=a$. In order for $P_1(x)$ to be the best approximation of $f(x)$ near $x=a$, we impose conditions $f(a)=P_1(a)$ (same function values at $x=a$) and $f'(a)=P_1'(a)$ (same derivative at $x=a$) and we get that $c_0=f(a)$ and $c_1=f'(a)$. In other words, the best linear approximation $P_1(x)$ of $f(x)$ near $x=a$ is given by $P_1(x)=f(a)+f'(a)(x-a)$. Now consider a problem of approximating $f(x)$ by a quadratic function $y=P_2(x)=d_0+d_1(x-a)+d_2(x-a)^2$ near $x=a$. By imposing conditions $f(a)=P_2(a)$, $f'(a)=P_2'(a)$, and $f''(a)=P_2''(a)$, we get $d_0=f(a)$, $d_1=f'(a)$, and $d_2=\frac{f''(a)}{2}$. Similarly, the best cubic approximation is obtained (see Figure \ref{taylor fig}). Continuing in this fashion, we can consider a problem of approximating $f(x)$ by a degree $n$ polynomial $P_n(x)$ and it turns out that the best degree $n$ polynomial $P_n(x)$ approximating $f(x)$ near $x=a$ is given by
\begin{align*} P_n(x)&=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n\\
&=\sum_{k=0}^n \frac{f^{(k)}(a)}{k!}(x-a)^k.
\end{align*}
The polynomial $P_n(x)$ is called the \underline{Taylor polynomial of order $n$} about $x=a$. \index{Taylor polynomial} Note that the degree of $P_n(x)$ may not be equal to $n$, because $f^{(n)}(a)$ could equal $0$.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1.2, yscale=1.2, >=triangle 45]
\draw [->] (-3,0) -- (3,0) ;
\node [above right] at (3,0) {$x$} ;

\draw [->] (-2,-1.5) -- (-2,5) ;
\node [above right] at (-2,5) {$y$} ;

\draw [thick, domain=-2.5:1.5] plot (\x, {exp(\x)}) ;

\draw [dashed] (0,1) -- (-2,1);
\draw [dashed] (0,0) -- (0,1);
\node [below] at (0,0) {$a$} ;
\node [left] at (-2,1) {$f(a)$} ;

\draw [color=red, domain=-2.5:1.5] plot (\x, {1+\x}) ;
\draw [color=green, domain=-2.5:1.5] plot (\x, {1+\x+\x*\x/2}) ;
\draw [color=blue, domain=-2.5:1.5] plot (\x, {1+\x+\x*\x/2+\x*\x*\x/6}) ;

\node [right, color=red] at (1.5, 1+1.5) {$y=P_1(x)=f(a)+f'(a)(x-a)$};
\node [right, color=green] at (1.5, 1+1.5+1.5*1.5/2) {$y=P_2(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2$};
\node [right, color=blue] at (1.5, 1+1.5+1.5*1.5/2+1.5*1.5*1.5/6) {$y=P_3(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-a)^3$};
\node [right] at (1.5, 0.2+exp(1.5) {$y=f(x)$};

\draw [fill=black] (0,1) circle (2pt) ;

\end{tikzpicture}
\end{center}
\caption{Taylor polynomials}
\label{taylor fig}
\end{figure}

Taylor polynomial $P_n(x)$ is a good approximation of $f(x)$ near $x=a$, but $f(x)$ is not exactly equal to $P_n(x)$ unless $f$ itself is a polynomial. The next result, however, gives a way to get an equality at the cost of modifying Taylor polynomial: adding a remainder term or having infinitely many terms.

\begin{thm}[Taylor Series Expansion]\index{Taylor Series Expansion}
Let $f$ be a continuous function defined on the closed interval $[a,b]$. Suppose that $f$ is infinitely differentiable on $(a,b)$ and let $x_0\in (a,b)$. Then for every $x\in (a,b)$, $x\neq x_0$ and for every $n$, there is an $x_1$ between $x_0$ and $x$ such that
\begin{equation}\label{with remainder} f(x)=\sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k+\frac{f^{(n+1)}(x_1)}{n!}(x-x_0)^{n+1}. \end{equation}
Moreover, if there exists a constant $M$ such that $|f^{(n)}(x)|\leq M$ for all $n$ and for all $x\in [a,b]$, then
\begin{equation}\label{taylor series} f(x)=\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k. \end{equation}
\end{thm}

(\ref{with remainder}) is often called the \underline{Taylor expansion with remainder} about $x=a$\index{Taylor expansion with remainder} and (\ref{taylor series}) is called the \underline{Taylor series expansion}\index{Taylor series expansion} of $f$ about $x=a$. \\

The following is the collection of Taylor series expansion of some elementary functions about $x=0$ and values at which the expansion is valid.\medskip

\begin{center}
\fbox{
\begin{minipage}{6 in}\vspace{0.3cm}\textbf{Taylor Series Expansion of Some Elementary Functions}\\

${\displaystyle e^x =  1+ x + \frac{x^2}{2!} + \frac{x^3}{3!}
 + \cdots + \frac{x^{n}}{n!} +
\cdots }\quad$ for $-\infty < x < \infty$. \\
\medskip
${\displaystyle \sin x =  x - \frac{x^3}{3!} + \frac{x^5}{5!}
-\frac{x^7}{7!} + \cdots + (-1)^n \, \frac{x^{2n+1}}{(2n+1)!} +
\cdots }\quad$ for $-\infty < x < \infty$. \\
\medskip
${\displaystyle \cos x =  1 - \frac{x^2}{2!} + \frac{x^4}{4!}
-\frac{x^6}{6!} + \cdots + (-1)^n \, \frac{x^{2n}}{(2n)!} + \cdots
}\quad$ for $-\infty < x < \infty$.\\
\medskip
${\displaystyle \frac{1}{1-x} = 1 + x + x^2 + x^3 + \cdots + x^n + \cdots }\quad $ for $-1 < x < 1$.\\
\medskip
${\displaystyle \ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots + (-1)^{n+1} \frac{x^n}{n} + \cdots } \quad $ for $-1 < x \leq 1$.\\
\medskip
${\displaystyle (1+x)^p = 1 + px + \frac{p(p-1)}{2!}x^2 + \frac{p(p-1)(p-2)}{3!}x^3 + \cdots } \quad $ for $-1 < x < 1$.\\
\end{minipage}
 }
\end{center}
\medskip
\begin{example}
Using the Taylor series expansion of $e^x$, we get
$$e=e^1=1+1+\frac{1}{2!}+\frac{1}{3!}+\cdots=2.7182818284\cdots.$$
\end{example}

\begin{example} For a positive constant $\lambda$, let $X$ be a discrete random variable such that
$$P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!},\quad k=0,1,2,3,\ldots.$$
Using the Taylor series expansion of $e^x$, we get $$\sum_{k=0}^\infty P(X=k)=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{k!}=e^{-\lambda}e^\lambda=1.$$
Such a random variable is said to follow \underline{Poisson Distribution}\index{Poisson Distribution} with parameter $\lambda$ and is denoted $X\sim Poisson(\lambda)$.
\end{example}

\begin{problem}
Let $X\sim Poisson(\lambda)$. Compute $E(X)$.
\end{problem}

\begin{answer} Note that
$$E(X)=\sum_{k=0}^\infty k\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^\infty k\frac{\lambda^k}{k!}=e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^k}{(k-1)!}=\lambda e^{-\lambda}\sum_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!}.$$
Using the change of index $j=k-1$, we get
$$E(X)=\lambda e^{-\lambda}\sum_{j=0}^\infty \frac{\lambda^{j}}{j!}=\lambda e^{-\lambda}e^\lambda=\lambda.$$
\end{answer}

\todo{allele freq 0.5 percent requires sample size 460 to detect at least one allele with 99 percent. If allele freq becomes 0.05 percent the required sample size becomes 4600. $(1-p)^{2N} \sim 1-2Np$. See Problem \ref{allelefreq} }

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $X\sim Poisson(\lambda)$. Compute $Var(X)$.\\
\textit{Hint}: Consider $E(X^2-X)=E(X(X-1))$ first.

\item Suppose that a continuous function $f:[-1,1]\to \mathbb{R}$ is thrice differentiable on $(-1,1)$ with
$$ f(-1)=0,\quad f(0)=0,\quad f(1)=1,\quad\text{and}\quad f'(0)=0.$$
Show that there is $x_0\in (-1,1)$ such that $f'''(x_0)\geq 3$.\\
\textit{Hint}: Suppose that $f'''(x)<3$ for all $x\in (-1,1)$. By Taylor Expansion, for any $x\in [-1,1]$, there is $x_0$ between $0$ and $x$ such that
$$f(x)=\frac{f''(0)}{2!}x^2+\frac{f'''(x_0)}{3!}x^3.$$ Substitute $1$ and $-1$ for $x$ and derive a contradiction.

\item Compute $\displaystyle{\sum_{n=0}^\infty \frac{1}{(2n)!}=1+\frac{1}{2!}+\frac{1}{4!}+\frac{1}{6!}+\cdots}$. \\
\textit{Hint}: Consider the Taylor expansion of $e^x$ and $e^{-x}$.

\item Find the Taylor expansion of $f(x)=\ln x$ about $x=1$.\\
\textit{Hint}: Note that $\ln x=\ln (1+(x-1))$.
\end{enumerate}
\end{exercise}

\chapter{Linear Algebra}

\section{Linear System and Matrices}\label{linear system}

Linear algebra is an essential tool in statistics, especially in statistical modelling. The concept of the column space of a matrix in particular plays an important role interpreting linear models in geometric context. In this section, we consider linear systems and their matrix representation.

\begin{defi} A \underline{linear equation}\index{linear equation} in the variables
$x_1,x_2,\ldots,x_n$ is an equation of the form
$$a_1x_1+a_2x_2+\cdots+a_nx_n=b,$$
where $a_1,a_2,\ldots,a_n$ and $b$ are real numbers, called
\underline{coefficients}.\index{coefficients of linear equation}
\end{defi}

\begin{example} $3x_1+2x_2-x_3=5$ is a linear equation having $3$ variables. $2x-y+z^2=-1$ is \textit{not} a linear equation,
because it contains $z^2$.
\end{example}

\begin{defi} A \underline{linear system}\index{linear system} in the variables
$x_1,x_2,\ldots,x_n$ is a collection of linear equations in the variables $x_1,x_2,\ldots,x_n$. A \underline{solution}\index{solution of linear system} of a linear system is a
list $(s_1,s_2,\ldots,s_n)$ that makes each equation in the system
hold when variables $x_1,x_2,\cdots, x_n$ are replaced by
$s_1,s_2,\ldots,s_n$. In this case, we say that $x_1=s_1,
x_2=s_2,\ldots, x_n=s_n$ is a solution to the system. The \underline{solution set}\index{solution set of linear system} is the collection of all possible solutions. Two systems are said to be \underline{equivalent}\index{equivalent linear systems} if they have the
same solution set.
\end{defi}

\begin{example} \begin{equation}\label{first linear system}\left\{\begin{array}{rrrr}
2x_1&-x_2&+\frac{3}{2}x_3=&8
\\ x_1 & &- 4x_3=&-7 \end{array}\right.\end{equation}
is a linear system having $2$ equations and $3$ variables. $x_1=-7, x_2=-22, x_3=0$ is a solution to the system
(\ref{first linear system}). $x_1=5, x_2=6.5, x_3=3$ is another solution to (\ref{first linear system}).
\end{example}

\begin{example}\label{three examples} Consider systems
$$\text{I.}\left\{\begin{array}{rrr}
x_1&-2x_2=&-1
\\ -x_1 &+3x_2=&3 \end{array}\right.\qquad \text{II.}\left\{\begin{array}{rrr}
2x_1&-4x_2=&-2
\\ x_1 &-3x_2=&-3 \end{array}\right. \qquad \text{III.}\left\{\begin{array}{rrr}
x_1&-2x_2=&-1
\\ 2x_1 &-4x_2=&1 \end{array}\right.$$
It is easy to check that I and II are equivalent linear systems: $x_1=3$, $x_2=2$ is a common unique solution to both I and II. One can easily check that III has \textit{no} solutions.
\end{example}

There are linear systems that have infinitely many solutions. In fact, the following is known (see Theorem \ref{criterion}).

\begin{thm}\label{three possibility}
 A linear system has either
\begin{enumerate}
\item no solutions, or
\item exactly one solution, or
\item infinitely many solutions.
\end{enumerate}
\end{thm}

\begin{defi} A system is said to be \underline{consistent}\index{consistent} if
it has either one solution or infinitely many solutions. A system is \underline{inconsistent}\index{inconsistent} if it has no solutions.
\end{defi}

\begin{example}
Linear systems I and II in Example \ref{three examples} are consistent, while III is inconsistent.
\end{example}

A linear system can be conveniently represented by its augmented matrix. First we recall the definition of a matrix.

\begin{defi} An \underline{$m\times n$ matrix}\index{matrix} is a
rectangular array of numbers having $m$ rows and $n$ columns. In particular, an $n\times 1$ matrix is called an \underline{$n$-dimensional column vector}\index{column vector} (or simply \underline{$n$-vector})\index{$n$-vector}. Each
number in a matrix is called an \underline{entry}\index{entry} or \underline{element}\index{element} or \underline{component}\index{component}. The entry at the $i^{\text{th}}$ row and $j^{\text{th}}$ column is called the $(i,j)$-entry. If the $(i,j)$-entry of an $m\times n$ matrix $A$ is given by $a_{ij}$, then we use notation $A=[a_{ij}]_{i,j=1}^{m,n}$ (or simply $A=[a_{ij}]_{i,j=1}^n$ if $m=n$).
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item The set of all $n$-vectors can be identified with $\mathbb{R}^n$ and from now on, we will use $\mathbb{R}^n$ for the notation of the set of all $n$-vectors.
\item In this monograph, column vectors will be denoted using bold face, for example, $\vu,\vv$.
\item An $1\times n$ matrix is called an \underline{$n$-dimensional row vector}\index{row vector}. We often write $(x_1,\ldots,x_n)$ to denote a row vector $\left[\begin{array}{ccc} x_1 & \cdots & x_n \end{array}\right]$.
\end{enumerate}
\end{remark}

\begin{example}
Let $A=\displaystyle{\left[\begin{array}{rrr}2&3&1\\-1&0&7\end{array}\right]}$ and $\vu=\left[\begin{array}{r} 1 \\ -2 \end{array}\right]$. Then $A$ is a $2 \times 3$ matrix and $\vu$  is a $2$-vector.
\end{example}

We use an example to define matrices that are associated with a given linear system. Consider a system
\begin{equation}\label{system4} \left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &2x_2 &-8x_3=&8 \\ -4x_1&+5x_2&+9x_3=&-9 \end{array}\right. .\end{equation}
The \underline{coefficient matrix}\index{coefficient matrix} of the linear system (\ref{system4}) is
$\displaystyle{\left[\begin{array}{rrr}1&-2&1\\0&2&-8
\\-4&5&9\end{array}\right]}$.\\
The \underline{augmented matrix}\index{augmented matrix} of the linear system (\ref{system4}) is
$\displaystyle{\left[\begin{array}{rrr@{\quad}|@{\quad}r} 1 & -2 & 1 & 0 \\ 0 & 2 & -8 & 8 \\ -4 & 5 & 9 & -9 \end{array}\right]}$. Note that the augmented matrix contains all information about the corresponding linear system. In other words, if a matrix $A$ is given, one can recover a unique linear system that has $A$ as the augmented matrix. \\
%% In the augmented matrix above @{\quad}|@{\quad} was used instead of | for a proper spacing

\begin{problem}
Let $A=\displaystyle{\left[\begin{array}{rrrr@{\quad}|@{\quad}r} -2& 0 & 3 & 1 & 7 \\ 1& 0 & -2 & 0 & 4 \\ 5& -4 & 2 & 2 & 1 \end{array}\right]}$ be the augmented matrix of a linear system. What is the system?
\end{problem}

\begin{answer}
From $A$, we recover $\left\{\begin{array}{rrrrr}
-2x_1& &3x_3&+x_4=&7
\\ x_1&  &-2x_3 & =&4 \\ 5x_1&-4x_2&+2x_3&+2x_4=&1 \end{array}\right. .$
\end{answer}

We are mainly interested in solving a given system, that is, finding
the solution set of the system. We will develop several methods to
do so through examples.

\begin{example}\label{system I} Consider systems
$$\text{I.}\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &2x_2 &-8x_3=&8 \\ -4x_1&+5x_2&+9x_3=&-9 \end{array}\right. \quad \text{ and }\quad \text{II.}\left\{\begin{array}{rrrr}
\blue{x_1} &-2x_2&-x_3=&7
\\ &\blue{x_2} &-2x_3=&-10 \\ &&\blue{x_3}=&4 \end{array}\right. . $$
We can easily solve II using
\underline{backward substitution}\index{backward substitution}:
$$\blue{x_3}=4,\quad \blue{x_2}=2x_3-10=2\cdot4-10=-2,\quad \blue{x_1}=2x_2+x_3+7=2\cdot(-2)+4+7=7.$$
\end{example}

Note that system II in Example \ref{system I} is easy to solve because it is \textit{steplike}, by which we mean that the leading variable in the $n^\text{th}$ equation is $x_n$ with coefficient equal to $1$. Therefore, one idea to solve a general and more complicated system like system I is to
transform it into a system like II. When we transform the
original system, however, of course we need to be careful in order \textit{not} to change the solution
set of the system. We introduce three important operations that can be used to transform a general linear system into an easier one without altering the solution set.

\begin{thm}\label{elementary row operations} Let a linear system be given.
\begin{enumerate}
\item \underline{scaling operation}\index{scaling operation}: if we replace an equation in the linear system by a nonzero multiple of the equation, then the solution set remains unchanged.
\item \underline{interchange operation}\index{interchange operation}: if we interchange two equations in the system, the solution set remains unchanged.
\item \underline{replacement operation}\index{replacement operation}: if we replace one equation in the system by the sum of itself and a multiple of
another equation, the solution set remains unchanged.
\end{enumerate}
\end{thm}

\begin{defi} The operations in Theorem \ref{elementary row operations} are called
\underline{elementary row operations}.\index{elementary row operations}
\end{defi}

\begin{example} Let
$$\text{I$'$.}\left\{\begin{array}{rrrr}
2x_1&-4x_2&+2x_3=&0
\\ &2x_2 &-8x_3=&8 \\ -4x_1&+5x_2&+9x_3=&-9 \end{array}\right. \quad \text{ and }\quad \text{I$''$.}\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &x_2 &-4x_3=&4 \\ -4x_1&+5x_2&+9x_3=&-9 \end{array}\right. , $$
then both I$'$ and I$''$ are equivalent to I in Example \ref{system I}. In other words,
systems I, I$'$, and I$''$ are essentially the same. Note that I$'$ is obtained from I by multiplying row $1$ by $2$. Similarly, I$''$ is obtained from I by multiplying row $2$ by $\frac{1}{2}$. To indicate that I$'$ (respectively, I$''$) is obtained from I using the scaling operation described above, we use notation
$\begin{CD} \text{I} @>R_1\mapsto
2R_1>\quad > \text{I$'$} \end{CD}$ (respectively, $\begin{CD} \text{I} @>R_2\mapsto
\frac{1}{2}R_2>\quad > \text{ I$''$}\end{CD}$ ).
\end{example}

\begin{example} Consider
$$\text{I$'''$.}\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0 \\ -4x_1&+5x_2&+9x_3=&-9
\\ &2x_2 &-8x_3=&8  \end{array}\right. , $$
then I$'''$ is equivalent to I in Example \ref{system I}. Note that I$'''$ is obtained from I by interchanging row $2$ and row $3$. To indicate the use of interchange operation, we use notation $\begin{CD} \text{I} @>R_2 \leftrightarrow
R_3>\quad > \text{I$'''$}\end{CD}$.
\end{example}

\begin{example} The system
$$\text{I$''''$.}\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &2x_2 &-8x_3=&8 \\ &-3x_2&+13x_3=&-9 \end{array}\right.$$
is obtained by replacing row 3 of system I in Example \ref{system I} by
the sum of row 3 and $4$ times row $1$ of I. It is easy to check that I and I$''''$ are equivalent. To indicate the use of replacement operation, we use notation $\begin{CD} \text{I} @>R_3 \mapsto R_3+4R_1>\quad > \text{I$''''$} \end{CD}$.
\end{example}

\begin{example}\label{fundamental example} In this example, we explain how to transform a general system into a steplike one. Consider system I in Example \ref{system I} and we apply elementary row operations as in the following:
$$\begin{CD}\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &2x_2 &-8x_3=&8 \\ \blue{-4x_1}&+5x_2&+9x_3=&-9 \end{array}\right. @>R_3 \mapsto R_3+4R_1>> \left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &\blue{2x_2} &-8x_3=&8 \\ &-3x_2&+13x_3=&-9 \end{array}\right.\end{CD}$$
$$\begin{CD} @>R_2\mapsto \frac{1}{2}R_2>> \left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &x_2 &-4x_3=&4 \\ &\blue{-3x_2}&+13x_3=&-9 \end{array}\right. @>R_3\mapsto R_3+3R_2>> \left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &x_2 &-4x_3=&4 \\ &&x_3=&3\end{array}\right.\end{CD}.$$
It is easy to find a (unique) solution to the last system:
$x_3=3$, $x_2=16$,
$x_1=29$. Since elementary row operations do not change the solution set, we see that $x_1=29$, $x_2=16$, $x_3=3$ is a unique solution to I, too. Note that the procedure above can be described using the augmented matrices
$$\begin{CD} \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-2&1& 0\\0&2&-8& 8
\\ \blue{-4}&5&9& -9\end{array}\right] @>R_3 \mapsto R_3+4R_1>> \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-2&1&0\\0&\blue{2}&-8& 8
\\0&-3&13& -9\end{array}\right] \end{CD}$$
$$\begin{CD} @>R_2\mapsto \frac{1}{2}R_2>> \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-2&1& 0 \\ 0&1&-4& 4\\ 0& \blue{-3} &13& -9\end{array}\right]
@>R_3\mapsto R_3+3R_2>>
\left[\begin{array}{rrr@{\quad}|@{\quad}r}1 & -2 & 1 & 0\\ 0 & 1 & -4& 4\\0 & 0 & 1 &  3 \end{array}\right]
\end{CD},$$
where the last matrix can be decoded to produce the corresponding linear system $$\left\{\begin{array}{rrrr}
x_1&-2x_2&+x_3=&0
\\ &x_2 &-4x_3=&4 \\ &&x_3=&3\end{array}\right.,$$
yielding the same solution $x_1=29$, $x_2=16$, $x_3=3$ as expected. Note that instead of stopping at the last matrix, we can perform additional elementary row operations to get
$$\begin{CD}
\left[\begin{array}{rrr@{\quad}|@{\quad}r}1&\blue{-2}&1& 0\\0&1&-4& 4
\\0&0&1& 3\end{array}\right] @>R_1\mapsto R_1+2R_2>> \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&0&\blue{-7}&8\\0&1&\blue{-4}& 4
\\0&0&1& 3\end{array}\right] @>R_1\mapsto
R_1+7R_3>R_2\mapsto R_2+4R_3>
\left[\begin{array}{rrr@{\quad}|@{\quad}r}1&0&0& 29\\0&1&0& 16
\\0&0&1& 3\end{array}\right] \end{CD},$$
which, after recovering the corresponding linear system, yields the solution $x_1=29$, $x_2=16$, $x_3=3$  right away.
\end{example}

\begin{remark} The procedure described in Example \ref{fundamental example} is called the \ul{Gauss-Jordan
elimination}.\index{Gauss-Jordan elimination}
\end{remark}

\begin{example} Consider the system
$$\left\{\begin{array}{rrrr}
&x_2&-4x_3=&8
\\ 2x_1&-3x_2 &+2x_3=&1 \\ 5x_1&-8x_2&+7x_3=&1 \end{array}\right. .$$
Note that the corresponding augmented matrix can be transformed as
$$\begin{CD}
\left[\begin{array}{rrr@{\quad}|@{\quad}r}0&1&-4& 8\\2&-3&2& 1
\\5&-8&7& 1\end{array}\right] @>R_1\leftrightarrow R_2>>
\left[\begin{array}{rrr@{\quad}|@{\quad}r}2&-3&2& 1
\\ 0&1&-4& 8\\5&-8&7& 1\end{array}\right] @>R_1\mapsto \frac{1}{2}R_1>> \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-\frac{3}{2}&1& \frac{1}{2}
\\ 0&1&-4& 8\\5&-8&7& 1\end{array}\right]\end{CD}$$
$$\begin{CD} @>R_3 \mapsto R_3-5R_1>>  \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-\frac{3}{2}&1& \frac{1}{2}
\\ 0&1&-4& 8\\0&-\frac{1}{2}&2& -\frac{3}{2}\end{array}\right] @>R_3 \mapsto R_3+\frac{1}{2}R_2>> \left[\begin{array}{rrr@{\quad}|@{\quad}r}1&-\frac{3}{2}&1& \frac{1}{2}
\\ 0&1&-4& 8\\0&0&0& \frac{5}{2}\end{array}\right]
\end{CD}$$
from which we recover a system
$$\left\{\begin{array}{rrrr}
x_1&-\frac{3}{2}x_2&+x_3=&\frac{1}{2}
\\  & x_2 &-4x_3=&8\\  & &0=&\frac{5}{2} \end{array}\right. .$$
Therefore, the system is inconsistent.
\end{example}

\begin{example}\label{first infinite} Consider a linear system
$$\left\{\begin{array}{rrrrrr}
x_1&+6x_2&&+3x_4&&=0 \\
&&x_3&-4x_4&&=5 \\&&&&x_5&=7
 \end{array}\right.$$
which has $5$ variables and $3$ equations. Obviously $x_5=7$. How about $x_4$? As
one can see, $x_4$ need not be a fixed number. In other words, $x_4$
is \underline{free}\index{free variable}. However, once $x_4$ is chosen, $x_3$ is
completely determined because of equation 2. That is, $x_3=4x_4+5$. Looking at the top equation, we see that $x_2$ is another free variable, independent of $x_4$. Note, however, that the values of $x_4$ and $x_2$ completely determine $x_1$, say,
$x_1=-6x_2-3x_4$. In summary, the solution set of the system is
$$\left\{\begin{array}{rl}x_1&=-6x_2-3x_4 \\ x_2&\text{ is free} \\ x_3&=4x_4+5 \\ x_4&\text{ is free} \\ x_5&=7\end{array}\right..$$
If we take $x_2=3$ and $x_4=1$, then we have a solution
$$x_1=-21,\quad x_2=3,\quad
x_3=9,\quad x_4=1,\quad
x_5=7.$$ Setting $x_2=-1$ and $x_4=0$ gives
$$x_1=6,\quad x_2=-1,\quad
x_3=5,\quad x_4=0,\quad
x_5=7.$$ In this way, we can supply infinitely many
solutions of the given linear system. Note that presence of a free variable makes the system have infinitely many solutions.
\end{example}

\begin{defi} A variable which is not free is called
\underline{basic}.\index{basic variable}
\end{defi}

\begin{example} In Example \ref{first infinite}, $x_1,x_3,x_5$ are basic variables and $x_2,x_4$ are free variables.
\end{example}

So far we have observed that if a system is obtained from another using elementary row operations, then the systems are equivalent to each other. Since the augmented matrix of a linear system completely describes the system, this observation leads to the following definition.

\begin{defi} Two matrices are \underline{row equivalent}\index{row equivalent} if
there is a sequence of elementary row operations that transforms
one matrix into the other.
\end{defi}

\begin{remark}\label{system equals augmented} It is easy to check that two systems are equivalent if and only if the corresponding augmented matrices are row equivalent.
\end{remark}

By Remark \ref{system equals augmented}, solving a linear system boils down to transforming its augmented matrix into a steplike one. We make precise what it means by a steplike matrix.

\begin{defi}\label{rref} A matrix is said to be \ul{in a reduced row echelon form}\index{reduced row echelon form} or \ul{a reduced row echelon matrix}\index{reduced row echelon matrix} if it has the following properties:
\begin{enumerate}
\item All nonzero rows are above any rows of all zeros.
\item Each leading entry of a row (i.e., the leftmost nonzero entry in the row) is $1$ and it is in a column to the right of the leading entry of the row above it. In other words, the leading $1$'s move to the right.
\item The leading entry is the only nonzero number in its column.
\end{enumerate}
\end{defi}

\begin{example} $\left[\begin{array}{rrrr} 1 & 0 & 0 & 0
\\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0
\end{array}\right]$ is in a reduced row echelon form.
\end{example}

\begin{example} $\left[\begin{array}{rrrr} 1 & 2 & 0 & 0
\\ \blue{0} & \blue{0} & \blue{0} & \blue{0} \\ 0 & 0 & 1 & 0 \end{array}\right]$ is \textit{not} in a reduced row echelon form, because the row of all zeros is above the last row, which contains a nonzero number. If row $2$ and $3$ are switched, then it would be in the reduced row echelon form.
\end{example}

\begin{example} $\left[\begin{array}{rrrr} 1 & \blue{2} & 3 & \blue{4}
\\ 0 & 1 & 1 & \blue{1} \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0
\end{array}\right]$ is \textit{not} in a reduced raw echelon form, because the leading entries of rows $2$ and $3$ are not the only nonzero number in their respective columns.
\end{example}

\begin{example} $\left[\begin{array}{rrrr} 1 & 0 & \blue{3} & 0
\\ 0 & 1 & \blue{1} & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0
\end{array}\right]$ is in a reduced row echelon form. Note that the $(2,3)$-entry is \textit{not} the leading entry in row $2$, so having $3$ above it does \textit{not} violate the conditions in Definition \ref{rref}.
\end{example}

As seen in Example \ref{fundamental example}, the importance of a reduced row echelon form is in that the linear system associated with a matrix in a reduced row echelon form is easy to solve. The next theorem states that it is always possible to transform a general matrix into a unique reduced echelon matrix.

\begin{thm}\label{unique rref} Each matrix $A$ is row equivalent to one and only
one reduced echelon matrix. This unique matrix is called \textit{the} reduced row echelon form of $A$.
\end{thm}

\begin{example}\label{unique rref} Consider the reduced row echelon form of
$A=\left[\begin{array}{rrrr}1&-2&1&0\\0&2&-8&8
\\-4&5&9&-9\end{array}\right]$. In Example \ref{fundamental example}, we observed that $A$ is row equivalent to $\left[\begin{array}{rrrr}1&0&0&29\\0&1&0&16 \\ 0&0&1&3\end{array}\right]$, which is in a reduced row echelon form. By Theorem \ref{unique rref}, we see that $\left[\begin{array}{rrrr}1&0&0&29\\0&1&0&16 \\ 0&0&1&3\end{array}\right]$ is the reduced row echelon form of $A$.
\end{example}

\begin{problem}\label{reduced example} Find the reduced row echelon form of $\left[\begin{array}{rrrr}0 & 1 & 2 & -4 \\ 1 & 0 & 2 & -2 \\ -2 & 4 & 6 & -4 \end{array}\right]$.
\end{problem}

\begin{answer}
$$\begin{CD}
\left[\begin{array}{rrrr}0 & 1 & 2 & -4 \\ 1 & 0 & 2 & -2 \\ -2 & 4 & 6 & -4 \end{array}\right] @>R_1\leftrightarrow R_2 > R_3\mapsto -\frac{1}{2}R_3>  \left[\begin{array}{rrrr}1 & 0 & 2 & -2 \\ 0 & 1 & 2 & -4 \\ 1 & -2 & -3 & 2 \end{array}\right] @>R_3\mapsto R_3-R_1 >>  \left[\begin{array}{rrrr}1 & 0 & 2 & -2 \\ 0 & 1 & 2 & -4 \\ 0 & -2 & -5 & 4 \end{array}\right]\\
 @>R_3\mapsto R_3+2R_2 >>  \left[\begin{array}{rrrr}1 & 0 & 2 & -2 \\ 0 & 1 & 2 & -4 \\ 0 & 0 & -1 & -4 \end{array}\right]
 @>R_3\mapsto -R_3 >>  \left[\begin{array}{rrrr}1 & 0 & 2 & -2 \\ 0 & 1 & 2 & -4 \\ 0 & 0 & 1 & 4 \end{array}\right]\\
 @>R_1\mapsto R_1-2R_3 >R_2\mapsto R_2-2R_3>  \left[\begin{array}{rrrr}1 & 0 & 0 & -10 \\ 0 & 1 & 0 & -12 \\ 0 & 0 & 1 & 4 \end{array}\right].
\end{CD}$$
\end{answer}

\begin{problem}
Solve a linear system $$\left\{\begin{array}{rrrr}
&x_2&+2x_3=&-4
\\ x_1&  &+2x_3=&-2 \\ -2x_1&+4x_2&+6x_3=&-4 \end{array}\right. .$$
\end{problem}

\begin{answer}
Note that the augmented matrix of the system is given in Problem \ref{reduced example}. Since it is row equivalent to $\left[\begin{array}{rrrr}1 & 0 & 0 & -10 \\ 0 & 1 & 0 & -12 \\ 0 & 0 & 1 & 4 \end{array}\right]$, we conclude that $x_1=-10$, $x_2=-12$ , and $x_3=4$.
\end{answer}

We are now ready to define the rank of a matrix.

\begin{defi}\label{rank def defi} A \underline{pivot position}\index{pivot position} in a matrix $A$ is
a location in $A$ that corresponds to a leading 1 in the reduced row
echelon form of $A$. A \underline{pivot column}\index{pivot column} is a
column of $A$ that contains a pivot position. The number of pivot positions of a matrix $A$ is
called the \underline{rank}\index{rank} of $A$ and denoted by
$\rank (A)$.
\end{defi}

\begin{example} In Example \ref{unique rref}, we had three pivot
columns: columns $1$, $2$, and $3$. In particular, its rank equals $3$.
\end{example}

\begin{remark}
The rank of a matrix has an important meaning regarding the matrix. See Theorem \ref{pivotbasis} for example.
\end{remark}

Next theorem gives a way to determine whether a given system has no solutions, a unique solution, or infinitely many solution (see Theorem \ref{three possibility}).

\begin{thm}\label{criterion} A linear system is inconsistent if and only if the
rightmost column of the augmented matrix is a pivot column. When a
system is consistent (i.e., the rightmost column of its augmented
matrix is not pivotal), it contains either (i) a unique solution,
when there are no free variables, or (ii) infinitely many solutions,
when there is at least one free variable.
\end{thm}

\begin{remark}\label{criterion remark}
Let a consistent system be given, so it has either a unique solution or infinitely many solutions. Note that pivot columns of the coefficient matrix of the system correspond to basic variables, while non-pivot columns correspond to free variables (see Example \ref{first infinite}). We conclude that a consistent system has a unique solution if and only if all columns of the coefficient matrix of the system are pivotal.
\end{remark}

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]

\item Solve the following system:
$$\left\{\begin{array}{rrrrrr}
x_1& &-2x_3 &-5x_4& &=8 \\
 &x_2& &+2x_4& &=2 \\
& & & &x_5&=1
 \end{array}\right. .$$

\item Solve the following system:
$$\left\{\begin{array}{rrrr}
-x_1&+x_2&-x_3=&2
\\ x_1&-x_2 &+2x_3=&0 \\ 2x_1&-3x_2&+2x_3=&-2 \end{array}\right. .$$

\item Solve the following system:
$$\left\{\begin{array}{rrrrrr}
x_1&+6x_2&-x_3&+3x_4&+2x_5&=-2 \\
-2x_1&+x_2&x_3&-2x_4&+x_5&=3 \\&x_2&&-2x_4&x_5&=-10
 \end{array}\right. .$$

\item Find the reduced row echelon form of the matrix $\left[\begin{array}{rrrr}
2 & 4 & 6 & -2 \\ 1 & 0 & -3 & 1
\\ 1 & -1 & 3 & 0 \end{array}\right]$ and using it, solve
$$\left\{\begin{array}{rrrr}
2x&+4y&+6z=&-2
\\ x&&-3z=&1 \\ x&-y&+3z=&0 \end{array}\right..$$

\item Find the rank of each of the following matrices.
\begin{multicols}{2}
\begin{enumerate}
\item $\left[\begin{array}{rrrr}
-1 & 0 & 0 & 2 \\ 5 & 0 & 2 & 7
\\ 1 & -3 & 1 & -2 \end{array}\right]$
\item $\left[\begin{array}{rrrrrr}
0 & -5 & 2 & -2 & 5 & 1\\ 1 & 3 & -3 & 0 & -3 & 1
\\ -5 & 4 & 1 & -1 & 2& 0 \end{array}\right]$
\end{enumerate}
\end{multicols}

\end{enumerate}
\end{exercise}

\section{Basics of Matrix Algebra}

In this section, we discuss basic operations and analytic properties of matrices that will play a key role in handling multidimensional data.

\begin{defi} Two $m\times n$ matrices are said to be \underline{equal}\index{matrix!equality}
if their corresponding entries are equal. The \underline{sum}\index{matrix!sum}
$A+B$ of two $m\times n$ matrices $A,B$ is an $m \times n$ matrix obtained by adding corresponding entries.  The \underline{scalar multiple}\index{scalar multiple of a matrix} $cA$ of an $m \times n$ matrix
$A$ by $c$ is the $m \times n$ matrix obtained by multiplying each entry
in $A$ by $c$. The \underline{difference}\index{matrix!difference} $A-B$ of two $m \times n$ matrices $A,B$ is defined by $A+(-1)B$. In other words, $A-B$ is obtained by computing entrywise difference. The \underline{transpose}\index{transpose} $A^t$ of an $m\times n$ matrix $A$ is the $n\times m$ matrix whose columns are formed from the corresponding rows of $A$.
\end{defi}

\begin{example} If $\left[\begin{array}{cc} x-2 & 2 \\ 5 & y+2
\end{array}\right]=\left[\begin{array}{rr} 3 & 2 \\ x & -1
\end{array}\right]$, then $x=5$ and $y=-3$.
\end{example}

\begin{example} For $\vu=\left[\begin{array}{r} 3 \\ 4 \\ -2
\end{array}\right]$, $7\vu=\left[\begin{array}{r} 21 \\ 28 \\ -14
\end{array}\right]$ and $\frac{1}{2}\vu=\left[\begin{array}{r} \frac{3}{2} \\ 2 \\
-1 \end{array}\right]$.
\end{example}

\begin{problem} Compute $3\vu-2\vv$, where $\vu=\left[\begin{array}{r}
3 \\ 4 \\ -2
\end{array}\right]$  and $\vv=\left[\begin{array}{r} 1 \\ -2 \\ 5
\end{array}\right]$.
\end{problem}

\begin{answer}
$$3\vu-2\vv=3\left[\begin{array}{r} 3 \\ 4 \\ -2 \end{array}\right]-2\left[\begin{array}{r} 1 \\ -2 \\ 5 \end{array}\right]=\left[\begin{array}{r} 7 \\ 16 \\ -16 \end{array}\right].$$
\end{answer}

\begin{example}
If $A=\left[\begin{array}{rr} \blue{1} & \red{0} \\ \blue{-2} & \red{5} \\ \blue{5} & \red{1}
\end{array}\right]$, then $A^t=\left[\begin{array}{rrr} \blue{1} & \blue{-2} & \blue{5} \\ \red{0} & \red{5} & \red{1}
\end{array}\right]$.
\end{example}

\begin{remark}
It is straightforward to check that $(A^t)^t=A$.
\end{remark}

\begin{defi} The \underline{zero matrix}\index{matrix!zero matrix}, denoted by $O$, is the matrix whose entries are all equal to $0$. The $n\times 1$ zero matrix is called the \underline{$n$-dimensional zero vector}\index{zero vector} and denoted by
$\veczero$. A matrix is called a \underline{square matrix}\index{matrix!square matrix} if it has the same number of rows and columns. A square matrix is said to be \underline{upper triangular}\index{matrix!upper triangular matrix} (respectively, \underline{lower triangular}\index{matrix!lower triangular matrix}), if its $(i,j)$-entry is zero for all $i,j$ with $i>j$ (respectively, $i<j$).  A square matrix is called a \underline{diagonal matrix}\index{matrix!diagonal matrix} if its $(i,j)$-entry is zero for all $i,j$ with $i\neq j$, that is, if all off diagonal entries are zero. The \underline{$n\times n$ identity matrix}\index{matrix!identity matrix}, denoted by $I_n$, is the $n\times n$ diagonal matrix with $1$ along the diagonal.
\end{defi}

\begin{example}
$\left[\begin{array}{r} 0 \\ 0 \\ 0
\end{array}\right]$ is the $3$-dimensional zero vector and $I_2=\left[\begin{array}{rr} 1 & 0 \\ 0 & 1
\end{array}\right]$ is the $2\times 2$ identity matrix.
\end{example}

The following properties follow immediately.

\begin{thm} \label{matrix addition formula} For any $m\times n$ matrices $A,B,C$ and for any scalars $c,d\in
\mathbb{R}$, the following are true:
\begin{multicols}{2}
\begin{enumerate}
\item $A+B=B+A$
\item $(A+B)+C=A+(B+C) $
\item $A+O=O+A=A$
\item $ A+(-A)=-A+A=O$
\item $  c(A+B)=cA+cB $
\item $ (c+d)A=cA+dA $
\item $ c(dA)=(cd)A$
\item $ 1A=A$
\end{enumerate}
\end{multicols}
\end{thm}

Theorem \ref{matrix addition formula} has nice geometric explanations when the matrices are replaced by $n$-vectors. First note that an $n$-vector may be identified with an arrow starting from the origin. See Figure \ref{column vector geometry}. 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw[->] (-2,0) -- (4,0) ;
\node [above right] at (4,0) {$x$};
\draw[->] (0,-2) -- (0,4) ;
\node [above right] at (0,4) {$y$};

\node [above left] at (0,0) {$\veczero$};
\draw [fill=black] (2.5,2) circle (2pt);
\node [right] at (2.5,2) {$\vu=\left[\begin{array}{r} a\\ b \end{array}\right]$};
\draw [dashed] (2.5,0) -- (2.5,2) -- (0,2);

\draw [->, thick, blue] (0,0) -- (2.5,2) ;

\node [below] at (2.5,0) {$a$};
\node [left] at (0,2) {$b$};

\begin{scope}[xshift=8cm]
\draw[->] (-2,0) -- (4,0) ;
\node [above right] at (4,0) {$x$};
\draw[->] (0,-2) -- (0,4) ;
\node [above right] at (0,4) {$z$};
\draw[->] (-0.9,-1.8) -- (1.8,3.6) ;
\node [above right] at (1.8,3.6) {$y$};

\node [above left] at (0,0) {$\veczero$};

\draw [fill=black] (3,3) circle (2pt);
\node [right] at (3,3) {$\vv=\left[\begin{array}{r} a\\ b \\ c \end{array}\right]$};

\draw [dashed] (3,3)--(3,1)--(2.5,0) ;
\draw [dashed] (0.5,3)--(0.5,1)--(3,1) ;
\draw [dashed] (0,2)--(0.5,3)--(3,3)--(2.5,2)--(2.5,0) ;
\draw [dashed] (2.5,2) -- (0,2);

\draw [->, thick, blue] (0,0)--(3,3);

\node [below] at (2.5,0) {$a$};
\node [left] at (0.5,1) {$b$};
\node [left] at (0,2) {$c$};
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Geometric interpretation of a $2$-vector and a $3$-vector}
\label{column vector geometry}
\end{figure}

The sum, scaling, and additive inverse of $n$-vectors are described in Figure \ref{geometric addition}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [->, thick, red] (0,0) -- (4,3) ;
\draw [->, thick, cyan] (0,0) -- (-3,2) ;
\draw [->, thick, magenta] (0,0) -- (1.5,-1) ;
\draw [->, thick, blue] (0,0) -- (-1.5,1) ;

\draw [->, dashed, thick, blue] (4,3) -- (2.5,4) ;
\draw [->, dashed, thick, red] (-1.5,1) -- (2.5,4) ;
\draw [->, thick, green] (0,0) -- (2.5,4) ;

\node[above, green] at (2.5,4) {$\vu+\vv$};
\node[below left] at (-1.5,1) {$\vu$};
\node[left, cyan] at (-3,2) {$2\vu$};
\node[right] at (4,3) {$\vv$};
\node[right, magenta] at (1.5,-1) {$-\vu$};

\draw [fill=black] (0,0) circle (2pt);
\node [below left] at (0,0) {$\veczero$};

\end{tikzpicture}
\end{center}
\caption{Geometric interpretation of vector addition, scaling, and additive inverse}
\label{geometric addition}
\end{figure}

Next, we consider matrix multiplication. To define the product of two matrices, we begin with the dot product of two column vectors.

\begin{defi} Let $\vu=\left[\begin{array}{c} a_1 \\ \vdots
\\ a_n \end{array}\right]$ and $\vv=\left[\begin{array}{c} b_1 \\ \vdots
\\ b_n \end{array}\right]$. The \underline{dot product}\index{dot product} (or \underline{inner product}\index{inner product} or \underline{scalar
product}\index{scalar product}) of $\vu$ and $\vv$, denoted by $\langle \vu,\vv\rangle$ or $\vu\cdot\vv$, is the
number given by $\sum_{i=1}^n a_ib_i=a_1b_1+\cdots+a_nb_n$.
\end{defi}

\begin{example}$\left[\begin{array}{r} 1 \\ -2 \\ 3 \\ -1
\end{array}\right]\cdot \left[\begin{array}{r} 0 \\ 2 \\ 5 \\ -2
\end{array}\right]=1\cdot 0 + (-2)\cdot 2 + 3\cdot 5 + (-1)\cdot (-2) =13$.
\end{example}

It is easy to verify the following properties of inner product.

\begin{thm}\label{inner product properties} For all $\vu,\vv,\vw\in \mathbb{R}^n$,
\begin{enumerate}
\item $\langle \vu, \vv \rangle=\langle \vv, \vu \rangle$.
\item $\langle  \vu+\vv , \vw \rangle=\langle \vu, \vw \rangle+\langle \vv, \vw\rangle$.
\item $\langle c\vu, \vv \rangle=c\langle \vu, \vv\rangle=\langle \vu,c\vv\rangle$.
\item $\langle \vu, \vu\rangle \geq 0$, and $\langle \vu, \vu\rangle=0$ if and only if $\vu=\veczero$.
\end{enumerate}
\end{thm}

 Note that, by Pythagorean Theorem, the length of the arrow $\vv$ in Figure \ref{column vector geometry} is given by $\sqrt{a^2+b^2+c^2}=\sqrt{\langle \vv, \vv\rangle}$. This justifies the following definitions.

\begin{defi}\label{rngeo} Let $\vu,\vv\in \mathbb{R}^n$.
\begin{enumerate}
\item The \underline{length}\index{length of a vector} of $\vu$, denoted by $\|\vu\|$, is defined by $\|\vu\|=\sqrt{\langle \vu,\vu\rangle}$.
\item The \underline{distance}\index{distance} between $\vu$ and $\vv$ is defined to be $\|\vu-\vv\|$.
\item $\vu$ is called a \underline{unit vector}\index{unit vector} if $\|\vu\|=1$.
\end{enumerate}
\end{defi}

Before we proceed, one of the fundamental inequalities in linear algebra is in order.

\begin{thm}[Cauchy-Schwarz Inequality]\index{Cauchy-Schwarz Inequality}  For all $\vu,\vv \in \mathbb{R}^n$,
$$|\langle \vu,\vv \rangle|\leq \|\vu\|\|\vv\|.$$
\end{thm}

Cauchy-Schwarz Inequality opens a way to define the concept of an angle between $\vu$ and $\vv$ in $\mathbb{R}^n$, even when $n>3$. Indeed, for nonzero vectors $\vu$ and $\vv$ in $\mathbb{R}^n$, the \underline{angle}\index{angle between vectors} $\angle(\vu,\vv)$ between $\vu$ and $\vv$ is defined to be
$$\angle(\vu,\vv)=\arccos\left(\frac{\langle \vu,\vv\rangle}{\|\vu\|\|\vv\|}\right).$$
Note that $\frac{\langle \vu,\vv\rangle}{\|\vu\|\|\vv\|}$ is between $-1$ and $1$ by Cauchy-Schwarz Inequality, so $\angle(\vu,\vv)$ ranges from $0$ to $\pi$. In particular, if $\langle \vu,\vv\rangle=0$, then $\angle(\vu,\vv)=\frac{\pi}{2}$. In this case, $\vu$ and $\vv$ are said to be \ul{orthogonal}\index{orthogonal vectors} or \ul{perpendicular}.\index{perpendicular vectors}

\begin{example}[sample correlation coefficient]
For paired data points $(X_i,Y_i)$, $1\leq i \leq N$, the \underline{sample correlation coefficient}\index{sample correlation coefficient} $r$ is defined to be
$$r=\frac{\sum_{i=1}^N (X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^N (X_i-\bar{X})^2}\sqrt{\sum_{i=1}^N (Y_i-\bar{Y})^2}},$$ where $\bar{X}$ (respectively, $\bar{Y}$) is the sample mean of $(X_i)_{i=1}^N$ (respectively, $(Y_i)_{i=1}^N$ ). Define $\vu,\vv\in \mathbb{R}^N$ by
$$\vu=\left[\begin{array}{c} X_1-\bar{X}\\ \vdots \\ X_N-\bar{X} \end{array}\right]\quad\text{and}\quad \vv=\left[\begin{array}{c} Y_1-\bar{Y}\\ \vdots \\ Y_N-\bar{Y} \end{array}\right],$$ then it is easy to verify that $r=\frac{\langle \vu,\vv\rangle}{\|\vu\|\|\vv\|}$ and therefore $-1\leq r \leq 1$.
\end{example}

Now we define the product of matrices.

\begin{defi} Let $A$ be an $\ell\times m$ matrix, $B$ an
$m\times n$ matrix. Then their \underline{product}\index{matrix!product} $AB$ is an $\ell\times n$
matrix whose $(i,j)$-entry is given by $\sum_{k=1}^m a_{ik}b_{kj}$,
where $a_{ik}$ is the $(i,k)$-entry of $A$ and $b_{kj}$ is the
$(k,j)$-entry of $B$. In other words, $(i,j)$-entry of $AB$ equals
the inner product of the $i^{\text{th}}$ row of $A$ and
$j^{\text{th}}$ column of $B$.
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item For $AB$ to be defined, the number of columns of $A$ must equal the number of rows of $B$.
\item The inner product $\langle \vu,\vv \rangle$ between two $n$-vectors $\vu$ and $\vv$ is the same as $\vu^t\vv$.
\end{enumerate}
\end{remark}

\begin{example} The product of a $2\times 3$ matrix and a $3\times 2$ matrix is a $2\times 2$ matrix. For example,
\begin{align*} \left[\begin{array}{rrr} \blue{1} & \blue{2} & \blue{-1} \\
\red{0} & \red{-5} & \red{3}
\end{array}\right]\left[\begin{array}{rr} \green{4} & 2 \\ \green{3} & 1 \\ \green{7} & 0
\end{array}\right]&=\left[\begin{array}{rr} \blue{1}\cdot \green{4}+\blue{2}\cdot \green{3} +\blue{(-1)}\cdot \green{7} & \blue{1}\cdot 2 +\blue{2}\cdot 1 +\blue{(-1)}\cdot 0 \\ \red{0}\cdot \green{4} +\red{(-5)}\cdot \green{3} +\red{3}\cdot \green{7} & \red{0}\cdot 2 +\red{(-5)}\cdot 1 + \red{3}\cdot 0
\end{array}\right]\\&=\left[\begin{array}{rr} 3 & 4 \\ 6 & -5
\end{array}\right].
\end{align*}
\end{example}

\begin{example} The product of a $2\times 3$ matrix and a $3$-vector is a $2$-vector. For example,
\begin{align*}\left[\begin{array}{rrr} 5 & 4 & 1 \\ 1 & 1 & 0
\end{array}\right]\left[\begin{array}{r} 2 \\ 4 \\ 3
\end{array}\right]&=\left[\begin{array}{r} 5\cdot 2 +4\cdot 4 +1\cdot 3  \\ 1\cdot 2 + 1\cdot 4 +0\cdot 3 \end{array}\right]\\
&=\left[\begin{array}{c} 29 \\ 6
\end{array}\right].\end{align*}
\end{example}

\begin{example}
$\left[\begin{array}{rr} 1 & 2 \\ -1 & 0
\end{array}\right]\left[\begin{array}{r} 2 \\ 4 \\ -1
\end{array}\right]$ is \textit{not} defined, because the number of columns of the former, $2$, is different from the number of rows of the latter, $3$.
\end{example}

\begin{example}
$\left[\begin{array}{rrr} -2 & 0 & 1 \\ 1 & 2 & 3 \\
0 & 0 & 1 \end{array}\right]\left[\begin{array}{r} x_1 \\ x_2 \\
x_3 \end{array}\right]=\left[\begin{array}{c} -2x_1 + x_3 \\ x_1+2x_2+3x_3 \\ x_3
\end{array}\right]=x_1\vv_1+x_2\vv_2+x_3\vv_3$, where $\vv_i$ denotes the $i^{\text{th}}$ column of the former matrix. See Example \ref{column theorem}.
\end{example}

\begin{example}
Matrix multiplication can be used to represent a linear system. Consider the system
$$\left\{\begin{array}{rrrr} x_1&-2x_2&+x_3=&0 \\ &2x_2 &-8x_3=&8 \\ -4x_1&+5x_2&+9x_3=&-9 \end{array}\right.,$$
then it can be described as $A\vx=\vb$, where $A=\left[\begin{array}{rrr}1 & -2 & 1 \\ 0 & 2 & -8
\\ -4 & 5 & 9  \end{array}\right]$, $\vx=\left[\begin{array}{r} x_1 \\ x_2 \\ x_3 \end{array}\right]$, and $\vb=\left[\begin{array}{r} 0 \\ 8 \\ -9 \end{array}\right]$.
\end{example}

\begin{problem}\label{homo consistent}
A system of the form $A\vx=\veczero$ is said to be \ul{homogeneous}\index{homogeneous}. Prove that every homogeneous system is consistent.
\end{problem}

\begin{answer}
It suffices to provide a solution to $A\vx=\veczero$. Indeed $\vx=\veczero$ works. For another approach, note that the rightmost column of the augmented matrix of the system $A\vx=\veczero$, being a column of all zeros, cannot be pivotal, so the result follows from Theorem \ref{criterion}.
\end{answer}

\begin{example}\label{extract row col} Matrix multiplication can be used to extract a column or a row in a matrix. Let $A=[a_{ij}]_{i,j=1}^n$ and  $\bm{e}_i=\left[\begin{array}{c} 0 \\ \vdots \\ 0 \\1 \\ 0 \\ \vdots
\\ 0\end{array}\right]$, a unit column vector with $1$ at the $i^{\text{th}}$ position and $0$ elsewhere. Then $A\bm{e}_i$ equals the $i^{\text{th}}$ column of $A$, while $\bm{e}_i^tA$ gives the $i^{\text{th}}$ row of $A$. In particular, $\bm{e}_i^tA\bm{e}_j=\langle \bm{e}_i,A\bm{e}_j\rangle$ gives the $(i,j)$-element $a_{ij}$.
\end{example}

\begin{problem}\label{transfer}
Let $A=[a_{ij}]_{i,j=1}^n$, $\vu=\left[\begin{array}{c} u_1 \\ \vdots
\\ u_n \end{array}\right]$, and $\vv=\left[\begin{array}{c} v_1 \\ \vdots
\\ v_n \end{array}\right]$. Prove that $\langle A\vu, \vv\rangle=\langle \vu, A^t\vv\rangle$.
\end{problem}

\begin{answer}
Since the $k^{\text{th}}$ component of $A\vu$ is given by $\sum_{i=1}^n a_{ki}u_i$, we get
$$\langle A\vu, \vv\rangle = \sum_{k=1}^n \left(\sum_{i=1}^n a_{ki}u_i\right)v_k. $$
Note that the $k^{\text{th}}$ component of $A^t\vv$ is given by $\sum_{i=1}^n a_{ik}v_i$, so it follows that
$$\langle \vu, A^t\vv\rangle = \sum_{k=1}^n u_k \left(\sum_{i=1}^n a_{ik}v_i\right).$$
The assertion follows from simple index change. In fact, the result holds even when $A$ is a non-square matrix. See Exercise \ref{basics of matrix exer}.\ref{general transfer}.
\end{answer}

\begin{remark}\quad
\begin{enumerate}
\item When $AB$ is defined, $BA$ is not necessarily defined. For example, if $A$ is a $3\times 2$ matrix and $B$ is a $2\times 4$ matrix, then $AB$ is a well-defined $3\times 4$ matrix while $BA$ is not defined.
\item Even when both $AB$ and $BA$ are defined, they might not have the same
size. For example, if $A$ is a $3\times 2$ matrix and $B$ is a $2\times 3$ matrix, then $AB$ is a $3\times 3$ matrix while $BA$ is a $2\times 2$ matrix.
\item Even if $A,B$ are both $n\times n$ matrices so that $AB$ and $BA$ have the same size, $AB$ is not necessarily equal to $BA$. Let $A=\left[\begin{array}{rr} 1 & 2 \\ 0 & 1 \end{array}\right]$ and $B=\left[\begin{array}{rr} 2 & 1 \\ 1 & 2 \end{array}\right]$, then $AB=\left[\begin{array}{rr} 4 & 5 \\ 1 & 2 \end{array}\right]$, while $BA=\left[\begin{array}{rr} 2 & 5 \\ 1 & 4 \end{array}\right]$.
\item For any $m\times n$ matrix $A$, $I_mA=A=AI_n$.
\end{enumerate}
\end{remark}

\begin{thm} If $A$ is an $\ell\times m$ matrix and $B,C$ are $m\times n$ matrices and $c$ is a real number, then
\begin{enumerate}
\item $A(B+C)=AB+AC$
\item $A(cB)=c(AB)=(cA)B$
\end{enumerate}
\end{thm}

Now we define the concept of a norm of a matrix. Roughly speaking, a norm measures the \textit{size} of a matrix. Since an $m\times n$ matrix can be viewed as a collection of ($n$ many $m$ dimensional) data points, a norm of a matrix can be viewed as a measure of the size of data.

\begin{defi}
Let $A$ be an $m\times n$ matrix. A nonnegative function $\|\cdot\|$ defined on the set of all $m\times n$ matrices is called a \underline{norm}\index{norm} if it has the following properties for all $m\times n$ matrices $A,B$ and all constant $c$.
\begin{enumerate}
\item $\|A+B\|\leq \|A\|+\|B\|$,
\item $\|cA\|=|c|\|A\|$, and
\item $\|A\|=0$ if and only if $A=O$, the zero matrix.
\end{enumerate}
\end{defi}

\begin{remark}\label{norms}\quad
\begin{enumerate}
\item For an $m\times n$ matrix $A=[a_{ij}]_{i,j=1}^{m,n}$, define
$$\|A\|_{HS}=\left(\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|^2\right)^{1/2}.$$ It is well known that $\|\cdot\|_{HS}$ defines a norm. $\|\cdot\|_{HS}$ is called the \underline{Hilbert-Schmidt norm}\index{Hilbert-Schmidt norm} or \underline{Frobenius norm}\index{Frobenius norm}). Note that $\|A\|_{HS}^2$ is the sum of the squares of the norm of columns of $A$.
\item Note that an $m\times n$ matrix $A$ can be viewed as a function from $\mathbb{R}^n$ to $\mathbb{R}^m$, via multiplication: $\vx\mapsto A\vx$. Define $\|\cdot\|_{op}$ on $A$ by
$$\|A\|_{op}=\max \{\|A\vx\|:\|\vx\|=1\}.$$
It is also well known that $\|\cdot\|_{op}$ defines a norm. $\|\cdot\|_{op}$ is called the \underline{operator norm}\index{operator norm}.
\end{enumerate}
\end{remark}

\begin{defi}
The \underline{trace}\index{trace} of an $n\times n$ square matrix $M=[m_{ij}]_{i,j=1}^n$, denoted by $tr(M)$, is defined to be the sum of all diagonal entries of $M$. In other words,
$$tr(M)=\sum_{i=1}^n m_{ii}.$$
\end{defi}

\begin{example}\label{trace and norm}
Let $A=[a_{ij}]_{i,j=1}^{m,n}$ be an $m\times n$ matrix, then both $AA^t$ and $A^tA$ are well-defined and it is easy to verify that
$$tr(AA^t)=tr(A^tA)=\|A\|_{HS}^2.$$
\end{example}

\begin{remark}\label{trace variance} The notion of the Hilbert-Schmidt norm and trace can be interpreted in statistical context. For $1\leq i \leq N$, let $\bm{X}_i$ be a multivariate data point in $\mathbb{R}^p$ and $X=\left[\begin{array}{cccc} | & | &   & | \\ \bm{X}_1 & \bm{X}_2 & \cdots & \bm{X}_N \\ | & | &  & | \end{array}\right]$ be the corresponding $p\times N$  matrix of observations. Let $\bar{\bm{X}}=\frac{1}{N}\sum_{i=1}^N \bm{X}_i$ denote the sample mean of the observations and let $B$ be the \underline{mean-deviation form}\index{mean-deviation form} of $X$ defined by
$$B=\left[\begin{array}{cccc} | & | &   & | \\ \bm{X}_1-\bar{\bm{X}} & \bm{X}_2-\bar{\bm{X}} & \cdots & \bm{X}_N-\bar{\bm{X}} \\ | & | &  & | \end{array}\right],$$
then the \underline{total variance}\index{total variance} of $\{\bm{X}_1,\ldots, \bm{X}_N\}$ is defined to be $\frac{1}{N-1}tr(B^tB)=\frac{1}{N-1}\|B\|^2_{HS}$. See Figure \ref{hs meaning} for a geometric interpretation of the Hilbert-Schmidt norm.
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\def\xone{3}
\def\yone{3}
\def\xtwo{2}
\def\ytwo{-2}
\def\xeye{-3.5}
\def\yeye{3.5}
\def\xenn{-2}
\def\yenn{-1}

\draw [fill=yellow!30, yellow!30] (-5,-3) -- (2,-3) -- (5,3) -- (-2,3) -- cycle ;
%\draw [fill=yellow!30, yellow!30] (-4-1.8,-3.6) -- (4-1.8,-3.6) -- (4+1.8,3.6) -- (-4+1.8,3.6) -- cycle ;

\draw (-4,0) -- (4,0) ;
\draw (0,-4) -- (0,4) ;
\draw (-1.8,-3.6) -- (1.8,3.6) ;

\draw [fill=black] (1,1) circle (2pt);
\node [above] at (1,1) {$\bar{\bm{X}}$};

\draw [fill=black] (\xone,\yone) circle (2pt);
\node [above] at (\xone,\yone) {$\bm{X}_1$};
\draw [blue, dashed] (\xone,\yone) -- (1,1) ;
\node [below right] at ({(\xone+1)/2}, {(\yone+1)/2}) {\blue{$\|\bm{X}_1-\bar{\bm{X}}\|$}};

\draw [fill=black] (\xtwo,\ytwo) circle (2pt);
\node [below] at (\xtwo,\ytwo) {$\bm{X}_2$};
\draw [blue, dashed] (\xtwo,\ytwo) -- (1,1) ;
\node [right] at ({(\xtwo+1)/2}, {(\ytwo+1)/2}) {\blue{$\|\bm{X}_2-\bar{\bm{X}}\|$}};

\draw [fill=black] (\xeye,\yeye) circle (2pt);
\node [above] at (\xeye,\yeye) {$\bm{X}_i$};
\draw [blue, dashed] (\xeye,\yeye) -- (1,1) ;
\node [below left] at ({(\xeye+1)/2}, {(\yeye+1)/2}) {\blue{$\|\bm{X}_i-\bar{\bm{X}}\|$}};

\draw [fill=black] (\xenn,\yenn) circle (2pt);
\node [below] at (\xenn,\yenn) {$\bm{X}_N$};
\draw [blue, dashed] (\xenn,\yenn) -- (1,1) ;
\node [above left] at ({(\xenn+1)/2}, {(\yenn+1)/2}) {\blue{$\|\bm{X}_N-\bar{\bm{X}}\|$}};

\end{tikzpicture}
\end{center}
\caption{Geometric meaning of $\|B\|_{HS}$. Note that $\displaystyle{\|B\|_{HS}^2=\sum_{i=1}^N \blue{\|\bm{X}_i-\bar{\bm{X}}\|}^2}$.}
\label{hs meaning}
\end{figure}

We close this section with the inverse of a matrix.

\begin{defi}
Let $A$ be an $n\times n$ square matrix. $A$ is said to be \underline{invertible}\index{matrix!invertible} if there exists an $n\times n$ matrix $B$ such that $AB=I_n=BA$, where $I_n$ is the $n\times n$ identity matrix.
\end{defi}

\begin{remark}
If $A$ is invertible, then there is only one $B$ such that $AB=I_n=BA$ (see Exercise \ref{basics of matrix exer}.\ref{unique inv}). In this case, $B$ is called the \underline{inverse}\index{matrix!inverse} of $A$ and denoted by $A^{-1}$.
\end{remark}

\begin{problem}\label{inverse transpose}
Let $A$ be an invertible $n\times n$ matrix. Show that $A^t$ is also invertible and $(A^t)^{-1}=(A^{-1})^t$.
\end{problem}

\begin{answer}
It is sufficient to show that $(A^{-1})^tA^t=A^t(A^{-1})^t=I_n$. Indeed, by Exercise \ref{basics of matrix exer}.\ref{transpose change},
$$(A^{-1})^tA^t=(AA^{-1})^t=I_n^t=I_n$$
and
$$A^t(A^{-1})^t=(A^{-1}A)^t=I_n^t=I_n.$$
\end{answer}

\begin{exercise}\label{basics of matrix exer} \quad
\begin{enumerate}[\bfseries 1.]
\item Let $\vx,\vy \in \mathbb{R}^n$. Suppose that $\langle \vx, \vz\rangle=\langle \vy, \vz\rangle$ for all $\vz\in \mathbb{R}^n$. Show that $\vx=\vy$.\\\textit{Hint}: It is sufficient to show that $\langle \vx-\vy, \vx-\vy\rangle=0$. Take $\vz=\vx-\vy$.

\item The goal of this exercise is to prove Cauchy-Schwarz Inequality. Let $\vu,\vv\in \mathbb{R}^n$.
\begin{enumerate}
\item Show that Cauchy-Schwarz Inequality holds when $\vu=\veczero$, so it remains to show the inequality for $\vu\neq \veczero$.
\item \label{CS part b} Let $a,b,c$ be constants with $a>0$. Show that $ax^2-2bx+c\geq 0$ for all $x\in \mathbb{R}$ if and only if $b^2\leq ac$.\\\textit{Hint}: Consider the vertex of the parabola $y=ax^2-2bx+c=a(x-\frac{b}{a})^2+\frac{ac-b^2}{a}$.
\item Show that $\|\vu\|^2 x^2-2\langle \vu,\vv\rangle x+\|\vv\|^2\geq 0$ for all $x\in \mathbb{R}$. \\\textit{Hint}: Expand $\langle x\vu-\vv,x\vu-\vv\rangle$.
\item Use (b) and (c) to conclude that $(\langle \vu,\vv \rangle)^2\leq \|\vu\|^2\|\vv\|^2$ from which Cauchy-Schwarz Inequality follows.
\end{enumerate}

\item\label{unique inv} Suppose that $A,B,C$ are $n\times n$ matrices such that $AB=BA=AC=CA=I_n$. Show that $B=C$.
\item Suppose that $A$ is an $n\times n$ square matrix such that $A^3=O$, the zero matrix.
\begin{enumerate}
\item Show that $A$ is not invertible.
\item Show that $I_n-A$ is invertible. Here $I_n$ denotes the $n\times n$ identity matrix. \\
\textit{Hint}: Consider $(I_n-A)(I_n+A+A^2)$.
\end{enumerate}
\item\label{transpose change} Show that if $AB$ is defined, then so is $B^tA^t$ and $(AB)^t=B^tA^t$. In general, if $A_1A_2\cdots A_k$ is defined, then show that $(A_1A_2\cdots A_k)^t=(A_k^t\cdots A_2^tA_1^t)$.
\item\label{tracedef} Let $tr(M)$ denote the trace of a square matrix $M$.
\begin{enumerate}
\item Show that $tr(M)=tr(M^t)$.
\item Show that $tr(A+B)=tr(A)+tr(B)$, provided that $A$ and $B$ are square matrices of the same dimensions.
\item Show that $tr(AB)=tr(BA)$, provided both $AB$ and $BA$ are square matrices, not necessarily of same size.
\item Show that $tr(A^tA)=0$ if and only if $A=O$, the zero matrix. \\ \textit{Hint}: See Example \ref{trace and norm}.
\end{enumerate}
\item\label{general transfer} This exercise generalizes Problem \ref{transfer}. Let $A$ be an $m\times n$ matrix, $\vu\in \mathbb{R}^n$, and $\vv\in \mathbb{R}^m$. Prove that $\langle A\vu,\vv\rangle =\langle \vu, A^t\vu\rangle$.
\end{enumerate}
\end{exercise}

\section{Vector Space and Matrix}

Recall that the set of all $n$-vectors is denoted by $\mathbb{R}^n$.

\begin{defi} \label{subspace defi}
Let $H\subseteq \mathbb{R}^n$. $H$ is called a \underline{subspace}\index{subspace} of $\mathbb{R}^n$ if it has the following properties:
\begin{enumerate}
\item $\veczero\in H$,
\item If $\vu,\vv\in H$, then $\vu+\vv\in H$, and
\item If $\vu\in H$ and $c\in \mathbb{R}$, then $c\vu\in H$.
\end{enumerate}
\end{defi}

\begin{example} Let $H=\left\{\left[\begin{array}{r} x \\ 0 \end{array}\right]:x\in
\mathbb{R}\right\}$. Clearly $\veczero =\left[\begin{array}{r} 0 \\ 0 \end{array}\right] \in H$. Let $\vu=\left[\begin{array}{r} x \\ 0 \end{array}\right]$ and $\vv=\left[\begin{array}{r} y \\ 0 \end{array}\right]$, then $\vu+\vv=\left[\begin{array}{c} x+y \\ 0 \end{array}\right]$ belongs to $H$. Finally, for any constant $c$ and for any $\vu=\left[\begin{array}{r} x \\ 0 \end{array}\right]$ in $H$, $c\vu=\left[\begin{array}{c} cx \\ 0 \end{array}\right]$ belongs to $H$, too. This shows that $H$ is a subspace of $\mathbb{R}^2$.
\end{example}

\begin{example}
$\{\veczero\}$ is a subspace of $\mathbb{R}^n$ and called the \underline{trivial subspace}\index{subspace!trivial subspace} of $\mathbb{R}^n$. $\mathbb{R}^n$ itself is a subspace of $\mathbb{R}^n$.
\end{example}

\begin{problem} Again consider $(\mathbb{R}^2,+,\cdot)$. Let
$K=\left\{\left[\begin{array}{c} x \\ x^2 \end{array}\right]:x\in
\mathbb{R}\right\}$. Is $K$ a subspace of
$\mathbb{R}^2$?
\end{problem}

\begin{answer}
No. $\left[\begin{array}{c} 1 \\ 1 \end{array}\right]$ and $\left[\begin{array}{c} 2 \\ 4 \end{array}\right]$ both belong to $K$, but their sum $\left[\begin{array}{c} 3 \\ 5 \end{array}\right]$ does not.
\end{answer}

\begin{defi} Given $\vv_1,\vv_2,\ldots,\vv_p\in \mathbb{R}^n$
and $c_1,c_2,\ldots,c_p \in \mathbb{R}$, the vector
$\vu=c_1\vv_1+c_2\vv_2+\cdots+c_p\vv_p$ is called a
\underline{linear combination}\index{linear combination} of $\vv_1,\vv_2,\ldots,\vv_p$ with
\underline{weights}\index{weights of linear combination} (or \underline{coefficients})\index{coefficients of linear combination}
$c_1,c_2,\ldots,c_p$. The \underline{span}\index{span} of $\{\vv_1,\vv_2,\ldots,\vv_p\}$, denoted by $\Span\{\vv_1,\vv_2,\ldots,\vv_p\}$, is
the collection of all linear combinations of vectors $\vv_1,\vv_2,\ldots,\vv_p$.
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item What is the difference between $\{\vv_1,\vv_2,\cdots,\vv_p\}$ and Span$\{\vv_1,\vv_2,\cdots,\vv_p\}$? The former contains only finitely many vectors but the second has infinitely many vectors in general. In fact, the former is a subset of the latter.
\item Let $S$ and $T$ be finite subsets of $\mathbb{R}^n$. Suppose that $S$ is a subset of $T$. Since $T$ contains more vectors than $S$, the set of linear combinations of vectors in $T$ contains that of $S$, that is, $\Span S \subseteq \Span T$. Exercise \ref{matalgexer}.\ref{SinT} is to show this rigorously.
\end{enumerate}
\end{remark}

Geometrically, $\Span S$ is the collection of all points that can be obtained by scaling and adding vectors in $S$. Figure \ref{geo span} illustrates geometric interpretation of the span. 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.8, yscale=0.8, >=triangle 45]

\draw [very thick, yellow!30] (-3,-1.5)--(3,1.5) ;
\node [below right] at (-2,-1) {$\Span \{\vu\}$};

\draw [fill=blue, blue] (2,1) circle (2pt) ;
\draw [thick, blue,->] (0,0) -- (2,1) ;
\node [blue, above] at (2,1) {$\vu$};

\draw [fill] (0,0) circle (2pt) ;
\node [below] at (0,0) {$\veczero$} ;


\begin{scope}[xshift=6.5cm]
\draw [fill=yellow!30, yellow!30] (-3,-3) -- (-2,2) -- (3,3) -- (2,-2) --  cycle;

\draw [fill=blue, blue] (2,1) circle (2pt) ;
\draw [thick, blue,->] (0,0) -- (2,1) ;
\node [blue, right] at (2,1) {$\vu$};

\draw [fill=red, red] (-1.5,1) circle (2pt) ;
\draw [thick, red,->] (0,0) -- (-1.5,1) ;
\node [red, left] at (-1.5,1) {$\vv$};
\node at (0.5,-1.5) {$\Span \{\vu,\vv\}$};

\draw [fill] (0,0) circle (2pt) ;
\node [below left] at (0,0) {$\veczero$} ;

\end{scope}

\begin{scope}[xshift=13cm]
\draw [fill=yellow!30, yellow!30] (-3,1) -- (-2,3) -- (3,3) -- (3,-1) -- (2,-3) -- (-3,-3) -- cycle;

\draw [fill=blue, blue] (2,1) circle (2pt) ;
\draw [thick, blue,->] (0,0) -- (2,1) ;
\node [blue, right] at (2,1) {$\vu$};

\draw [fill=red, red] (-1.5,1) circle (2pt) ;
\draw [thick, red,->] (0,0) -- (-1.5,1) ;
\node [red, left] at (-1.5,1) {$\vv$};

\draw [fill=green, green] (-0.5,-0.8) circle (2pt) ;
\draw [thick, green,->] (0,0) -- (-0.5,-0.8) ;
\node [green, below] at (-0.5,-0.8) {$\vw$};

\node at (0.5,-2) {$\Span \{\vu,\vv,\vw\}$};

\draw [fill] (0,0) circle (2pt) ;
\node [below left] at (0,0) {$\veczero$} ;

\end{scope}

\end{tikzpicture}
\end{center}
\caption{Geometric interpretation of span}
\label{geo span}
\end{figure}

\begin{example} Let $\vv_1=\left[\begin{array}{r} 1 \\ 2
\end{array}\right]$ and $\vv_2=\left[\begin{array}{r} -1 \\ 0
\end{array}\right]$. Then $\vu=\left[\begin{array}{r} -1 \\ 4
\end{array}\right]$ can be written as
$2\vv_1+3\vv_2$, so $\vu$ is a linear combination
of $\vv_1,\vv_2$, or in other words $\vu\in \Span\{\vv_1,\vv_2\}$. $\veczero=\left[\begin{array}{r} 0 \\ 0
\end{array}\right]$ is in Span$\{\vv_1,\vv_2\}$ as well,
because $\left[\begin{array}{r} 0 \\ 0
\end{array}\right]=0\vv_1+0\vv_2$.
\end{example}

\begin{example}\label{column theorem}
Let $A=\left[\begin{array}{cccc} | & | &   & | \\ \vv_1 & \vv_2 & \cdots & \vv_n \\ | & | &  & |
\end{array}\right]$ be an $m\times n$ matrix so that $\vv_i\in \mathbb{R}^m$ for $1\leq i \leq n$. Let $\vx=\left[\begin{array}{c} x_1 \\ \vdots
\\ x_n \end{array}\right]$, then $A\vx=x_1\vv_1+x_2\vv_2+\cdots x_n\vv_n$. In other words, $A\vx$ is a linear combination of columns of $A$.
\end{example}

\begin{problem} Let $\vv_1=\left[\begin{array}{r} 1 \\ -2 \\
-5
\end{array}\right]$ and $\vv_2=\left[\begin{array}{r} 2 \\ 5 \\ 6
\end{array}\right]$. Is $\vu=\left[\begin{array}{r} 7 \\ 4 \\ -3
\end{array}\right]$ in Span$\{\vv_1,\vv_2\}$ ?
\end{problem}

\begin{answer} Note that
\begin{align*}
&~\vu \text{ is in } \Span\{\vv_1,\vv_2\}\\
\Longleftrightarrow &~\text{there are numbers }x_1,x_2 \text{ such that }\vu=x_1\vv_1+x_2\vv_2\\
\Longleftrightarrow &~\text{the system }\left\{\begin{array}{rrr}
x_1&+2x_2=&7 \\
-2x_2 &+5x_2=&4\\
-5x_1&+6x_2=&-3 \end{array}\right. \text{ is consistent}\\
\Longleftrightarrow &~\text{the augmented matrix }\left[\begin{array}{rr@{\quad}|@{\quad}r} 1 & 2 &   7 \\ -2 & 5 &  4
\\ -5 & 6 &  -3 \end{array}\right] \text{ is consistent}
\end{align*}
Since $\left[\begin{array}{rr@{\quad}|@{\quad}r} 1 & 2 &  7 \\ -2 & 5  & 4
\\ -5 & 6  & -3 \end{array}\right]$ reduces to $\left[\begin{array}{rr@{\quad}|@{\quad}r} 1 & 0  & 3 \\ 0 & 1 & 2
\\ 0 & 0 &  0 \end{array}\right]$, we conclude that $\vu \in \Span\{\vv_1,\vv_2\}$. Moreover, we get $\vu=3\vv_1+2\vv_2$.
\end{answer}

\begin{thm}\label{span is subsp} If $\vu_1,\vu_2,\ldots, \vu_p$ are in $\mathbb{R}^n$, then Span$\{\vu_1,\vu_2,\ldots, \vu_p\}$ is a
subspace of $\mathbb{R}^n$.
\end{thm}

\begin{remark}\label{span converse}\quad
\begin{enumerate}
\item The subspace in Theorem \ref{span is subsp} is called the \ul{subspace spanned by $\vu_1,\vu_2,\ldots, \vu_p$}\index{subspace!spanned by a set of vectors}.
\item In fact, if $W$ is a subspace of $\mathbb{R}^n$, then there exists vectors $\vu_1,\vu_2,\ldots, \vu_k$ in $\mathbb{R}^n$ such that $W=\Span\{\vu_1,\vu_2,\ldots, \vu_k\}$.
\end{enumerate}
\end{remark}

\begin{defi}\label{ind definition}
Let $S=\{\vv_1,\ldots,\vv_p\}$ be a subset of $\mathbb{R}^n$. Let's consider a linear combination
$c_1\vv_1+\cdots+c_p\vv_p$ of $\vv_1,\ldots,\vv_p$. We can ask
ourselves the following question: for which values of
$c_1,\ldots,c_p$ does the linear combination
$c_1\vv_1+\cdots+c_p\vv_p$ become $\veczero$? Of
course, if we take $c_1=c_2=\cdots=c_p=0$, then clearly
$c_1\vv_1+\cdots+c_p\vv_p=\veczero$. So we have two possibilities:\\\\
Case 1: Taking $c_1=c_2=\cdots=c_p=0$ is the \textit{only} way
to make $c_1\vv_1+\cdots+c_p\vv_p=\veczero$.\\\\
Case 2: There are other choices of $c_1,c_2,\ldots,c_p$,
\textit{not all zero}, such that $c_1\vv_1+\cdots+c_p\vv_p=\veczero$.\\\\
When Case 1 happens, the set $S=\{\vv_1,\ldots,\vv_p\}$ is said to
be \underline{linearly independent}.\index{linearly independent}
When Case 2 happens, the set $S=\{\vv_1,\ldots,\vv_p\}$ is said to
be \underline{linearly dependent}.\index{linearly dependent}
\end{defi}

\begin{problem}\label{first lin ind}
Show that $\left\{\left[\begin{array}{r} 2 \\ 1
\\ 0 \\ 0 \\ 0 \end{array}\right],\left[\begin{array}{r} 1 \\ 0
\\ -2 \\ 1 \\ 0 \end{array}\right],\left[\begin{array}{r} -3 \\ 0
\\ 2 \\ 0 \\ 1 \end{array}\right]\right\}$ is linearly independent.
\end{problem}

\begin{answer}
Suppose $c_1\left[\begin{array}{r} 2 \\ 1
\\ 0 \\ 0 \\ 0 \end{array}\right]+c_2\left[\begin{array}{r} 1 \\ 0
\\ -2 \\ 1 \\ 0 \end{array}\right]+c_3\left[\begin{array}{r} -3 \\ 0
\\ 2 \\ 0 \\ 1 \end{array}\right]=\left[\begin{array}{r} 0 \\ 0
\\ 0 \\ 0 \\ 0 \end{array}\right]$, then
$$2c_1+c_2-3c_3=c_1=-2c_2+2c_3=c_2=c_3=0,$$
from which it follows that $c_1=c_2=c_3=0$. We therefore conclude that the given set of vectors is linearly independent.
\end{answer}

\begin{problem}\label{first lin dep}
Show that $\left\{\left[\begin{array}{r} 1 \\ 0 \end{array}\right],\left[\begin{array}{r} 0 \\ 2
\end{array}\right],\left[\begin{array}{r} 4 \\ 6
\end{array}\right]\right\}$ is linearly dependent.
\end{problem}

\begin{answer}
Since $4\left[\begin{array}{r} 1 \\
0 \end{array}\right]+3\left[\begin{array}{r} 0 \\ 2
\end{array}\right]-\left[\begin{array}{r} 4 \\ 6
\end{array}\right]=\left[\begin{array}{r} 0 \\ 0
\end{array}\right]$, the given set of vectors is linearly dependent.
\end{answer}

\begin{remark} \quad
\begin{enumerate}
\item If $S=\{\vv_1,\ldots,\vv_p\}$ is linearly
dependent, then we can find a vector $\vv_i\in S$ such that $\vv_i$ can
be expressed as a linear combination of remaining $p-1$ vectors.
\item If $S=\{\vv_1,\ldots,\vv_p\}$ is linearly independent, no vector in
$S$ can be written as a linear combination of remaining
$p-1$ vectors.
\item If $S$ is linearly independent and $T\subseteq S$, then $T$ is linearly independent, too.
\item If $S$ is linearly dependent and $S\subseteq T$, then $T$ is linearly dependent, too.
\end{enumerate}
\end{remark}

\begin{defi}\label{basis def} Let $H$ be a subspace of $\mathbb{R}^n$ ($H$ could be the whole $\mathbb{R}^n$). A set $\mathscr{B}=\{\vu_1,\ldots,\vu_p\}$
in $\mathbb{R}^n$ is said to be a
\underline{basis}\index{basis} for $H$ if
\begin{enumerate}
\item\label{basis indep} $\mathscr{B}$ is linearly independent, and
\item\label{basis span} $\Span\mathscr{B}=H$.
\end{enumerate}
\end{defi}

\begin{remark}
Condition \ref{basis indep} in Definition \ref{basis def} states that $\mathscr{B}$ should not be too large, as large sets tend to be more likely to be linearly dependent. On the contrary, condition \ref{basis span} in Definition \ref{basis def} states that $\mathscr{B}$ should be large enough to cover all vectors in $H$. Thus a basis of $H$ is an optimal set of vectors whose span covers all of $H$.
\end{remark}

\begin{example}\label{stdbasisr2} Let $\mathscr{B}=\left\{\left[\begin{array}{r} 1 \\
0 \end{array}\right],\left[\begin{array}{r} 0 \\ 1
\end{array}\right]\right\}$. We will show that this is a basis for $\mathbb{R}^2$.
First, we claim that the set $\mathscr{B}$ is linearly independent. Suppose that $c_1\left[\begin{array}{r} 1 \\
0 \end{array}\right]+c_2\left[\begin{array}{r} 0 \\
1 \end{array}\right]=\left[\begin{array}{r} 0 \\
0 \end{array}\right]$, then it follows that $c_1=c_2=0$. Now we show that $\Span\mathscr{B}=\mathbb{R}^2$. Indeed, for any $\left[\begin{array}{r} x \\
y \end{array}\right]\in \mathbb{R}^2$, we get $\left[\begin{array}{r} x \\
y \end{array}\right]=x\left[\begin{array}{r} 1 \\
0 \end{array}\right]+y\left[\begin{array}{r} 0 \\
1 \end{array}\right]$, a linear combination of $\left[\begin{array}{r} 1 \\
0 \end{array}\right]$ and $\left[\begin{array}{r} 0 \\
1 \end{array}\right]$.
\end{example}

\begin{problem}\label{anotherbasis}
Show that $\mathscr{B}'=\left\{\left[\begin{array}{r} 1 \\
0 \end{array}\right],\left[\begin{array}{r} 1 \\ 1
\end{array}\right]\right\}$ is a basis for $\mathbb{R}^2$.
\end{problem}

\begin{answer}
Suppose that $c_1\left[\begin{array}{r} 1 \\
0 \end{array}\right]+c_2\left[\begin{array}{r} 1 \\
1 \end{array}\right]=\left[\begin{array}{r} 0 \\
0 \end{array}\right]$, then it follows that $c_1+c_2=c_2=0$, so consequently $c_1=c_2=0$. This shows that $\mathscr{B}'$ is linearly independent. Now for any $\left[\begin{array}{r} x \\
y \end{array}\right]\in \mathbb{R}^2$, we get $\left[\begin{array}{r} x \\
y \end{array}\right]=(x-y)\left[\begin{array}{r} 1 \\
0 \end{array}\right]+y\left[\begin{array}{r} 1 \\
1 \end{array}\right]$, a linear combination of $\left[\begin{array}{r} 1 \\
0 \end{array}\right]$ and $\left[\begin{array}{r} 1 \\
1 \end{array}\right]$. This completes the proof that $\mathscr{B}'$ is a basis for $\mathbb{R}^2$.
\end{answer}

\begin{example} The set $S=\left\{\left[\begin{array}{r} 1 \\
0 \\ 0 \end{array}\right],\left[\begin{array}{r} 2 \\ 1 \\ 0
\end{array}\right]\right\}$ is not a basis for
$\mathbb{R}^3$, because, for example, $\left[\begin{array}{r} 0 \\ 0 \\ 1
\end{array}\right]$ cannot be written as a linear combination of $\left[\begin{array}{r} 1 \\
0 \\ 0 \end{array}\right]$ and $\left[\begin{array}{r} 2 \\ 1 \\ 0
\end{array}\right]$.
\end{example}

\begin{problem}\label{too many} Is the set $T=\left\{\left[\begin{array}{r} 1 \\
0 \end{array}\right],\left[\begin{array}{r} 0 \\ 2
\end{array}\right],\left[\begin{array}{r} 4 \\ 6
\end{array}\right]\right\}$ a basis for $\mathbb{R}^2$?
\end{problem}

\begin{answer}
No. It is enough to show that $T$ is linearly dependent, which was already shown in Problem \ref{first lin dep}.
\end{answer}

Note that $T$ in Problem \ref{too many} spans $\mathbb{R}^2$, so $T$ fails to be a basis for $\mathbb{R}^2$ because it has too many vectors in it. One way to get a basis for $\mathbb{R}^2$ from $T$ is to remove some unnecessary vectors, which is described in the following in more general setting.

\begin{thm}[The Spanning Set Theorem]\index{Spanning Set Theorem} Let $S=\{\vv_1,\cdots,\vv_p\}$ be
a subset of $\mathbb{R}^n$, and let
$H=\Span\{\vv_1,\cdots,\vv_p\}$. If one of vectors in $S$, say
$\vv_k$, is a linear combination of the remaining vectors in $S$,
then the set formed from $S$ by removing $\vv_k$ still spans $H$.
That is,
$$H=\Span S=\Span(S-\{\vv_k\})=\Span\{\vv_1,\vv_2,\ldots,\vv_{k-1},\vv_{k+1},\ldots,\vv_p\}.$$
\end{thm}

\begin{remark}\label{W as span} Let $S$ be as in the Spanning Set Theorem. If $\vv_k$ in $S$ is a linear combination of the remaining vectors in $S$, then $S$ must be linearly dependent. So
$S$ cannot be a basis for $H=\Span S$. However, $S-\{\vv_k\}$ still spans $H$. Keep removing vectors in $S$
which are unnecessary to span $H$ until no element can be written as a linear combination of others, and this will produce a basis for
$H=\Span S$. This, combined with Remark \ref{span converse}, shows that any subspace $W$ of $\mathbb{R}^n$ can be written as $W=\Span\{\vu_1,\ldots,\vu_k\}$ for some linearly independent set $\{\vu_1,\ldots,\vu_k\}$ in $\mathbb{R}^n$ and in this case $\{\vu_1,\ldots,\vu_k\}$ is a basis for $W$.
\end{remark}

\begin{problem}\label{sst example} Let's consider a set $S=\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right],\left[\begin{array}{r} 1 \\ 3 \\ 0
\end{array}\right],\left[\begin{array}{r} -1 \\ 0 \\ 0
\end{array}\right]\right\}$ in $\mathbb{R}^3$. Find a basis for $\Span S$.
\end{problem}

\begin{answer}
Since $\left[\begin{array}{r} 1 \\ 3 \\ 0
\end{array}\right]$ and $\left[\begin{array}{r} -1 \\ 0 \\ 0
\end{array}\right]$ can be written as a linear combination of $\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right]$ and $\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]$, by the Spanning Set Theorem, $\Span S=\Span \left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]\right\}$. Now one can easily verify that $\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]\right\}$ is linearly independent, so $\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]\right\}$ is a basis for $\Span S$.
\end{answer}

As seen in Example \ref{stdbasisr2} and Problem \ref{anotherbasis}, basis is not unique. However, both $\mathscr{B}$ and $\mathscr{B}'$  in Example \ref{stdbasisr2} and Problem \ref{anotherbasis} contain the same number of vectors. This is not a coincidence. In fact, we have the following result.

\begin{thm} Let $H$ be a subspace of $\mathbb{R}^n$. If $H$ has a basis of $n$ vectors,
then every basis of $H$ must consist of exactly $n$ vectors.
\end{thm}

\begin{defi}
The number of vectors in a basis for $H$ is called the \underline{dimension}\index{dimension} of $H$ and is denoted by $\dim H$.
\end{defi}

\begin{example} It is easy to check that $\left\{\left[\begin{array}{r} 1 \\ 0 \\ \vdots \\ 0
\end{array}\right], \left[\begin{array}{r} 0 \\ 1 \\ \vdots \\ 0
\end{array}\right],\ldots, \left[\begin{array}{r} 0 \\ 0 \\ \vdots \\ 1
\end{array}\right] \right\}$ is a basis for $\mathbb{R}^n$, so it follows that $\dim \mathbb{R}^n=n$.
\end{example}

\begin{example} Let $H=\Span\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right],\left[\begin{array}{r} 1 \\ 3 \\ 0
\end{array}\right],\left[\begin{array}{r} -1 \\ 0 \\ 0
\end{array}\right]\right\}$. What is the dimension of $H$?
\end{example}

\begin{answer}
Note that the answer is \textit{not} $4$: since $\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right],\left[\begin{array}{r} 1 \\ 3 \\ 0
\end{array}\right],\left[\begin{array}{r} -1 \\ 0 \\ 0
\end{array}\right]\right\}$ is linearly dependent, so it is not a basis for $H$. Note that in Problem \ref{sst example} we show that $H=\Span\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]\right\}$ and $\left\{\left[\begin{array}{r} 1 \\ 0 \\ 0
\end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right]\right\}$ is a basis for $H$. It follows that $\dim H=2$.
\end{answer}

\begin{remark}\label{subdim} In general, if $H$ is a subspace of $\mathbb{R}^n$, then $\dim H \leq n$ and $\dim H=n$ if and only if $H=\mathbb{R}^n$.
\end{remark}

Now we study vector spaces associated with a given matrix. We begin with the null space of a given matrix.

\begin{defi} The \underline{null space}\index{null space} of an $m\times n$
matrix $A$, written as $\Nul A$, is the set of all solutions to the
linear system $A\vx=\veczero$. In set notations,
$$\Nul A=\{\vx\in \mathbb{R}^n: A\vx=\veczero\}.$$
\end{defi}

\begin{example} Let $A=\left[\begin{array}{rrr} 1 & -3 & -2 \\ -5 & 9
& 1 \end{array}\right]$, then $\vu=\left[\begin{array}{r} 5 \\ 3
\\ -2 \end{array}\right]$ is in $\Nul A$ since $A\vu=\veczero$.
\end{example}

The following can be easily shown using Definition \ref{subspace defi}.

\begin{thm} The null space of an $m\times n$ matrix is a
subspace of $\mathbb{R}^n$.
\end{thm}

\begin{problem}\label{nullsp example} Let $A=\left[\begin{array}{rrrrr} -3 & 6 & -1 & 1 & -7
\\ 1 & -2 & 2 & 3 & -1 \\ 2 & -4 & 5 & 8 & -4
\end{array}\right]$. Find $\Nul A$. Describe $\Nul A$ as the span of a set of vectors. What is the $\dim \Nul A$?
\end{problem}

\begin{answer}
We start with solving a linear system $A\vx=\veczero$:
$$\left\{\begin{array}{rrrrrr}
-3x_1&+6x_2&-x_3&+x_4&-7x_5=&0 \\
x_1&-2x_2&+2x_3&+3x_4&-x_5=&0 \\
2x_1&-4x_2&+5x_3&+8x_4&-4x_5=&0
\end{array}\right. .$$
Since the reduced row echelon form of $A$ is $\left[\begin{array}{rrrrr} 1 & -2 & 0 & -1 & 3
\\ 0 & 0 & 1 & 2 & -2 \\ 0 & 0 & 0 & 0 & 0
\end{array}\right]$, $\Nul A$ is given by
$$\left\{\begin{array}{rl}x_1&=2x_2+x_4-3x_5 \\ x_2&\text{ is free} \\ x_3&=-2x_4+2x_5 \\ x_4&\text{ is free} \\ x_5&\text{ is free} \end{array}\right..$$
Note that this solution set can be more compactly described in the following way:
\begin{align*}\Nul A&=\left\{\left[\begin{array}{r} x_1 \\ x_2
\\ x_3 \\ x_4 \\ x_5 \end{array}\right]:  \left[\begin{array}{r} x_1 \\ x_2
\\ x_3 \\ x_4 \\ x_5 \end{array}\right]=\left[\begin{array}{c} 2r+s-3t \\ r
\\ -2s+2t \\ s \\ t \end{array}\right], r,s,t\in \mathbb{R} \right\}\\
&=\left\{\left[\begin{array}{r} x_1 \\ x_2
\\ x_3 \\ x_4 \\ x_5 \end{array}\right]:  \left[\begin{array}{r} x_1 \\ x_2
\\ x_3 \\ x_4 \\ x_5 \end{array}\right]=r\left[\begin{array}{r} 2 \\ 1
\\ 0 \\ 0 \\ 0 \end{array}\right]+s\left[\begin{array}{r} 1 \\ 0
\\ -2 \\ 1 \\ 0 \end{array}\right]+t\left[\begin{array}{r} -3 \\ 0
\\ 2 \\ 0 \\ 1 \end{array}\right], r,s,t\in \mathbb{R} \right\}\\
&=\Span \left\{\left[\begin{array}{r} 2 \\ 1
\\ 0 \\ 0 \\ 0 \end{array}\right],\left[\begin{array}{r} 1 \\ 0
\\ -2 \\ 1 \\ 0 \end{array}\right],\left[\begin{array}{r} -3 \\ 0
\\ 2 \\ 0 \\ 1 \end{array}\right]\right\}.
\end{align*}
It is easy to check that $\left\{\left[\begin{array}{r} 2 \\ 1
\\ 0 \\ 0 \\ 0 \end{array}\right],\left[\begin{array}{r} 1 \\ 0
\\ -2 \\ 1 \\ 0 \end{array}\right],\left[\begin{array}{r} -3 \\ 0
\\ 2 \\ 0 \\ 1 \end{array}\right]\right\}$ is linearly independent (see Problem \ref{first lin ind}), so we conclude that $\dim \Nul A=3$.
\end{answer}

\begin{remark}\label{nullity eq nonpivot} For a matrix $A$, $\dim \Nul A$ is called the \underline{nullity}\index{nullity} of $A$ and denoted by $\nullity(A)$. In Problem \ref{nullsp example}, the nullity of $A$ turns out to be the number of free variables, $3$, of the system $A\vx=\veczero$. In fact, this is true in general, that is,
\begin{align*}\nullity(A)&=\text{number of free variables of the system }A\vx=\veczero\\
(\text{Problem }\ref{homo consistent}, \text{Remark }\ref{criterion remark})&=\text{number of non-pivot columns of }A.
\end{align*}
\end{remark}

Next we consider another important vector space associated with a matrix.

\begin{defi} For an $m\times n$ matrix $A=\left[\begin{array}{cccc} | & | &   & | \\ \vv_1 & \vv_2 & \cdots & \vv_n \\ | & | &  & |
\end{array}\right]$, the \underline{column space}\index{column space}
of $A$, written as $\Coll A$, is $\Span \{\vv_1,\vv_2,\ldots,
\vv_n\}$, where $\vv_1,\vv_2,\cdots, \vv_n$
are in $\mathbb{R}^m$.
\end{defi}

\begin{remark} By Theorem \ref{span is subsp}, the column space of an $m\times n$ matrix is a
subspace of $\mathbb{R}^m$.
\end{remark}

\begin{example} Let $A=\left[\begin{array}{rr} 5 & 4 \\ 0 & 1 \\ -1 &
0 \end{array}\right]$, then $\Coll A=\Span\left\{\left[\begin{array}{r}
5 \\ 0 \\ -1 \end{array}\right], \left[\begin{array}{r}
4 \\ 1 \\ 0 \end{array}\right]\right\}$, which is a subspace of $\mathbb{R}^3$. To see if $\vb=\left[\begin{array}{r}
-1 \\ 1 \\ 1 \end{array}\right]$ is in $\Coll A$, we check if the system $A\vx=\vb$ is consistent. Note that the augmented matrix
$\left[\begin{array}{rr@{\quad}|@{\quad}r} 5 & 4 & -1 \\ 0 & 1 & 1 \\ -1 & 0 & 1 \end{array}\right]$ reduces to $\left[\begin{array}{rr@{\quad}|@{\quad}r} 1 & 0 & -1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 \end{array}\right]$, so by Theorem \ref{criterion}, the system $A\vx=\vb$ is consistent and hence we conclude that $\vb$ is sitting in $\Coll A$. Indeed, $\left[\begin{array}{r}
-1 \\ 1 \\ 1 \end{array}\right]=-\left[\begin{array}{r}
5 \\ 0 \\ -1 \end{array}\right]+\left[\begin{array}{r}
4 \\ 1 \\ 0 \end{array}\right]$.
\end{example}

\begin{remark} What we have learned so far can be summarized in the following. Let $A$ be an $m\times n$ matrix. Then
\begin{enumerate}
\item $\vu\in \mathbb{R}^n$ is in $\Nul A$ if and only if
$A\vu=\veczero$.
\item $\vu\in \mathbb{R}^m$ is in $\Coll A$ if and only if the
system $A\vx=\vu$ is consistent.
\end{enumerate}
\end{remark}

If $A$ is an $m\times n$ matrix, then $\Coll A$ is a subspace of $\mathbb{R}^m$. How can we find a basis for $\Coll
A$? Roughly speaking, what are the \textit{essential} columns of $A$ when constructing $\Coll A$?

\begin{thm}\label{pivotbasis} The pivot columns of a matrix $A$ form a basis
for $\Coll A$. In particular, $\dim \Coll A=\rank(A)$ (see Definition \ref{rank def defi}).
\end{thm}

\begin{example}\label{nulcolmat} Let $A=\left[\begin{array}{rrrrr} 1 & 4 & 0 & 2 & -1
\\ 3 & 12 & 1 & 5 & 5 \\ 2 & 8 & 1 & 3 & 2 \\ 5 & 20 & 2 & 8 & 8
\end{array}\right]$, then it reduces to
$\left[\begin{array}{rrrrr} 1 & 4 & 0 & 2 & 0 \\ 0 & 0 & 1 & -1 & 0
\\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0
\end{array}\right]$. Therefore, by Theorem \ref{pivotbasis}, $S=\left\{\left[\begin{array}{r} 1 \\ 3 \\
2 \\ 5 \end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\
1 \\ 2 \end{array}\right],\left[\begin{array}{r} -1 \\ 5 \\
2 \\ 8 \end{array}\right]\right\}$ is a basis for $\Coll A$. In
other words, $S$ is linearly independent and $\Span S=\Coll
A$.
\end{example}

We collect some properties of rank.

\begin{thm}\label{rank properties}
We have the following properties involving rank.
\begin{enumerate}
\item For an $m\times n$ matrix $A$, $\rank(A)\leq \min\{m,n\}$.
\item If $[A|B]$ denotes a matrix that is formed by attaching a matrix $B$ to $A$ side-by-side (so $A$ and $B$ have a same number of rows), then $rank(A)\leq \rank([A|B])$.
\item If $A$ is an $m\times k$ matrix and $B$ is a $k\times n$ matrix, then
$$\rank(A)+\rank(B)-k\leq \rank(AB)\leq \min\{\rank(A),\rank(B)\}.$$
\item If $A,B$ are matrices of the same dimensions, then $\rank(A+B)\leq \rank(A)+\rank(B)$.
\item $\rank (A)=\rank(A^t)$.
\item $\rank(A^tA)=\rank(A)$.
\end{enumerate}
\end{thm}

\begin{example} How to find a basis for $\Nul A$? Note that this was already covered in Problem \ref{nullsp example}. First of all, to
find a basis for $\Nul A$, one needs to find $\Nul A$. For
example, for the matrix in Example \ref{nulcolmat} above, we see that the
general solution is given by
$$\left\{\begin{array}{rrl} x_1 & = & -4x_2-2x_4 \\ x_2 & & \text{is free} \\ x_3 &=& x_4 \\ x_4 & & \text{is free} \\ x_5&=&0\end{array}\right. .$$
That is, $\Nul A=\left\{x_2\left[\begin{array}{r} -4 \\ 1 \\ 0 \\
0
\\ 0
\end{array}\right]+x_4\left[\begin{array}{r} -2 \\ 0 \\ 1 \\
1 \\ 0
\end{array}\right]:x_2,x_4 \text{ are free}\right\}=\Span\left\{\left[\begin{array}{r} -4 \\ 1 \\ 0 \\
0
\\ 0
\end{array}\right],\left[\begin{array}{r} -2 \\ 0 \\ 1 \\
1 \\ 0
\end{array}\right]\right\}$. We can show that the set $\left\{\left[\begin{array}{r} -4 \\ 1 \\ 0 \\
0
\\ 0
\end{array}\right],\left[\begin{array}{r} -2 \\ 0 \\ 1 \\
1 \\ 0
\end{array}\right]\right\}$ is linearly independent and hence we conclude that $\left\{\left[\begin{array}{r} -4 \\ 1 \\ 0 \\
0
\\ 0
\end{array}\right],\left[\begin{array}{r} -2 \\ 0 \\ 1 \\
1 \\ 0
\end{array}\right]\right\}$ is a basis for $\Nul A$.
\end{example}

We close this section with the relation between $\nullity(A)$ and $\rank(A)$, which is an immediate consequence of Remark \ref{homo consistent} and Theorem \ref{pivotbasis}, since any column of $A$ is either pivotal or non-pivotal.

\begin{thm}[Rank-Nullity Theorem] If $A$ is an $m\times n$ matrix, then
$$\nullity(A)+\rank(A)=n.$$
\end{thm}

\begin{exercise}\label{matalgexer}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $L=\left\{\left[\begin{array}{c} x \\ y \end{array}\right]:x\geq 0 \text{ and } y\geq 0\right\}$. Is this
a subspace of $\mathbb{R}^2$?
\item\label{SinT} Let $S$ and $T$ be finite subsets of $\mathbb{R}^n$ with $S\subseteq T$. Show that $\Span S\subseteq \Span T$.
\item Let $\{\bm{x}_1,\ldots,\bm{x}_p\}$ be a linearly independent set of vectors in $\mathbb{R}^n$. Let $A$ be an $n\times n$ invertible matrix. Show that $\{A\bm{x}_1,\ldots, A\bm{x}_p\}$ is linearly independent.

\item Let $\{\vu_1,\ldots,\vu_k\}$ be a linearly independent subset of $\mathbb{R}^n$. Suppose $\vv \in\mathbb{R}^n - \Span \{\vu_1,\ldots,\vu_k\}$. Show that $\{\vu_1,\ldots,\vu_k, \vv\}$ is also linearly independent.

\item Let $K=\Span\left\{\left[\begin{array}{r} 1 \\ 0 \\
0 \end{array}\right],\left[\begin{array}{r} 0 \\ 1 \\ 0
\end{array}\right],\left[\begin{array}{r} -2 \\ 1 \\ 0
\end{array}\right]\right\}$. What is the dimension of
$K$?

\item Find the null space of $\left[\begin{array}{rrr} 1 & 1 & 1 \\
0 & 0 & 0
\\ 0 & 0 & 0 \end{array}\right]$.

\item Let $B=\left[\begin{array}{rrrrr} 1 & 4 & 0 & 2 \\
3 & 12 & 1 & 5 \\ 2 & 8 & 1 & 3 \\ 5 & 20 & 2 & 8
\end{array}\right]$. Find a basis for $\Coll B$.

\item\label{npdep} Let $S=\{\vu_1,\ldots,\vu_p\}$ be a subset of $\mathbb{R}^n$ with $p>n$. Show that $S$ is linearly dependent. \\
\textit{Hint}: Suppose that $S$ is linearly independent. Let $A=\left[\begin{array}{cccc} | & | &   & | \\ \vu_1 & \vu_2 & \cdots & \vu_p \\ | & | &  & | \end{array}\right]$, then $A$ is an $n\times p$ matrix and the system $A\vx=\veczero$ has a unique solution, say $\vx=\veczero$. This means that all columns of $A$ are pivotal, implying that $\rank(A)=p$ (see Remark \ref{criterion remark}). You may want to apply Remark \ref{subdim} to derive a contradiction.

\item
\begin{enumerate}
\item Show that $\rank(A^tA)=\rank(AA^t)$.
\item Show that $\rank(AB)\neq \rank(BA)$ in general.
\end{enumerate}

\end{enumerate}
\end{exercise}

\section{Determinants}

\begin{defi}\label{detdefi} Let $A=[a_{ij}]_{i,j=1}^n$ be an $n\times n$ matrix. The \underline{determinant}\index{determinant} of $A$, denoted by $\det A$ or $|A|$, is the number defined  by
$$\det A=\sum_{\sigma\in S_n} \operatorname{sgn}(\sigma)\prod_{i=1}^n a_{i\sigma(i)},$$ where $S_n$ is the symmetric group on $n$ letters and $\operatorname{sgn}(\sigma)$ denotes the \underline{signature}\index{signature} of $\sigma$, that is, $\operatorname{sgn}(\sigma)=(-1)^{\text{the number of inversions of }\sigma}$ (see Remark \ref{Sndef}).
\end{defi}

\begin{example}\label{2by2det}
Let $A=\left[\begin{array}{rr} a_{11} & a_{12} \\ a_{21}& a_{22} \end{array}\right]$, then $\det A= a_{11}a_{22}-a_{12}a_{21}$. To see that this is consistent with Definition \ref{detdefi}, we note that $S_2=\{\iota, \tau\}$, where $\iota(1)=1$, $\iota(2)=2$, $\tau(1)=2$, and $\tau(2)=1$. Since $\iota$ has no inversions and $\tau$ has only one inversion, it follows that $\operatorname{sgn}(\iota)=(-1)^0=1$ and $\operatorname{sgn}(\tau)=(-1)^1=-1$, and that
$$\det A=\operatorname{sgn}(\iota)a_{1\iota(1)}a_{2\iota(2)}+\operatorname{sgn}(\tau)a_{1\tau(1)}a_{2\tau(2)}= a_{11}a_{22}-a_{12}a_{21}.$$
Exercise \ref{det exer}.\ref{3by3det} asks to compute the determinant of a $3\times 3$ matrix using Definition \ref{detdefi}.
\end{example}

For the determinant of a square matrix of size $4\times 4$ or bigger, Definition \ref{detdefi} has little practical value, in which case we can make use of an inductive property of the determinant.

\begin{defi}
Let $A$ be an $n\times n$ matrix. The \underline{$(i,j)$-minor}\index{minor} of $A$ is the determinant of the $(n-1) \times (n-1)$ submatrix formed by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$. The \underline{$(i,j)$-cofactor} is the number defined by $(-1)^{i+j}M_{ij}$, where $M_{ij}$ denotes the $(i,j)$-minor of $A$.
\end{defi}

\begin{thm}[Cofactor Expansion of the Determinant]
Let $A$ be an $n\times n$ matrix. Let $C_{ij}$ denote the $(i,j)$-cofactor of $A$, then
\begin{enumerate}
\item For any $i$, $\det A=\sum_{j=1}^n a_{ij}C_{ij}$. That is, $\det A$ is obtained by the cofactor expansion along the $i^{\text{th}}$ row, for any $i$.
\item For any $j$, $\det A=\sum_{i=1}^n a_{ij}C_{ij}$. That is, $\det A$ is obtained by the cofactor expansion along the $j^{\text{th}}$ column, for any $j$.
\end{enumerate}
\end{thm}

\begin{problem}
Let $A=\left[\begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array}\right]$. Compute $\det A$ using the cofactor expansion along
\begin{enumerate}
\item the first row.
\item the second column.
\end{enumerate}
\end{problem}

\begin{answer} Using the formula for the determinant of a $2\times 2$ matrix (Example \ref{2by2det}) and the cofactor expansion along the first row, we have
\begin{align*}\det A&=a\cdot(-1)^{1+1} \det\left[\begin{array}{rr} e & f \\ h & i \end{array}\right] + b\cdot(-1)^{1+2} \det\left[\begin{array}{rr} d & f \\ g & i \end{array}\right] +c\cdot(-1)^{1+3} \det\left[\begin{array}{rr} d & e \\ g & h \end{array}\right] \\
&=a(ei-fh)-b(di-fg)+c(dh-eg)\\
&=aei+bfg+cdh-afh-bdi-ceg.
\end{align*}
Similarly, the cofactor expansion along the second column gives
\begin{align*}\det A&=b\cdot(-1)^{1+2} \det\left[\begin{array}{rr} d & f \\ g & i \end{array}\right] + e\cdot(-1)^{2+2} \det\left[\begin{array}{rr} a & c \\ g & i \end{array}\right] +h\cdot(-1)^{3+2} \det\left[\begin{array}{rr} a & c \\ d & f \end{array}\right] \\
&=-b(di-fg)+e(ai-cg)-h(af-cd)\\
&=aei+bfg+cdh-afh-bdi-ceg.
\end{align*}
Note that these two expansions yield the same result, as expected.
\end{answer}

\begin{problem}
 Compute $\det A$, where
$$A=\left[\begin{array}{rrrr} 4 & 1 & 2 & 0 \\ 0 & 0 & -2 & 1 \\ 0 & 3 & 0 & 0 \\ 5 & -2 & 1 & 2 \end{array}\right].$$
\end{problem}

\begin{answer}
Note that the third row contains three zeros, so it will be most beneficial to use the cofactor expansion along with the third row. Indeed,
$$\det A=3\cdot(-1)^{3+2}\det \left[\begin{array}{rrr} 4 & 2 & 0 \\ 0 & -2 & 1 \\ 5 & 1 & 2 \end{array}\right]=(-3)\cdot\det \left[\begin{array}{rrr} 4 & 2 & 0 \\ 0 & -2 & 1 \\ 5 & 1 & 2 \end{array}\right]$$ and the problem boils down to finding $\det \left[\begin{array}{rrr} 4 & 2 & 0 \\ 0 & -2 & 1 \\ 5 & 1 & 2 \end{array}\right]$. Using the cofactor expansion along with the first row, we obtain
$$\det \left[\begin{array}{rrr} 4 & 2 & 0 \\ 0 & -2 & 1 \\ 5 & 1 & 2 \end{array}\right]=4\cdot((-2)\cdot 2 -1\cdot 1)-2\cdot(0\cdot 2 -1\cdot 5)=-10$$ and finally we have
$\det A=(-3)\cdot (-10)=30$.
\end{answer}

\begin{example}\label{triangular det}
Let $A=\left[\begin{array}{rrr} 4 & -1 & 9 \\ 0 & 1 & 2 \\ 0 & 0 & -2  \end{array}\right]$, an upper triangular matrix. The cofactor expansion along the first column gives
$\det A=4\cdot\det \left[\begin{array}{rr} 1 & 2 \\ 0 & -2  \end{array}\right]=4\cdot 1 \cdot (-2)$, the product of all diagonal elements of $A$. More generally, an application of mathematical induction combined with the cofactor expansion along the first column shows that the determinant of an upper triangular matrix is the product of all diagonal elements. In particular, the determinant of a diagonal matrix is the product of all diagonal elements. Similarly, the cofactor expansion along the first row shows that the determinant of a lower triangular matrix is the product of all diagonal elements.
\end{example}

When a given matrix does not contain many zeros, even the cofactor expansion has limited use. This difficulty, however, can be overcome by transforming the matrix into one for which the determinant is easy to compute. To explain this idea, we start with the following result that investigates the affect of elementary row operations on the determinant.

\begin{thm}\label{rowop and det} Let $A$ be a square matrix.
\begin{enumerate}
\item If one row (or one column) of $A$ is multiplied by $c$ to produce $B$, then $\det B=c\cdot \det A$.
\item If two rows (or two columns) of $A$ are interchanged to produce $B$, then $\det B=-\det A$.
\item\label{replacement no change} If a multiple of one row (respectively, column) of $A$ is added to another row (respectively, column) to produce a matrix $B$, then $\det B=\det A$.
\end{enumerate}
\end{thm}

\begin{problem}
 Compute $\det A$, where $A=\left[\begin{array}{rrrr} 3 & -2 & 0 & 5 \\ -3 & 5 & 5 & 2 \\ 6 & -1 & -1 & 13 \\ 3 & -2 & 0 & 7 \end{array}\right]$.
\end{problem}

\begin{answer}
Let $B$ be the matrix obtained by applying replacement operations $R_2\mapsto R_2+R_1$, $R_3\mapsto R_3-2R_1$, and $R_4\mapsto R_4-R_1$ to $A$, then $B=\left[\begin{array}{rrrr} 3 & -2 & 0 & 5 \\ 0 & 3 & 5 & 7 \\ 0 & 3 & -1 & 3 \\ 0 & 0 & 0 & 2 \end{array}\right]$ and $\det B=\det A$ by Theorem \ref{rowop and det}.\ref{replacement no change}. Let $C$ be the matrix obtained from $B$ by applying the replacement operation $R_3\mapsto R_3-R_2$, then $C=\left[\begin{array}{rrrr} 3 & -2 & 0 & 5 \\ 0 & 3 & 5 & 7 \\ 0 & 0 & -6 & -4 \\ 0 & 0 & 0 & 2 \end{array}\right]$ and $\det C=\det B$. Since $C$ is an upper triangular matrix, by Example \ref{triangular det}, we see that $\det A=\det B=\det C=3\cdot 3\cdot (-6)\cdot 2=-108$.

\end{answer}

\begin{thm}\label{det properties}
For any $n\times n$ square matrices $A$ and $B$, we have the following properties:
\begin{enumerate}
\item $\det(AB)=\det(A)\det(B)=\det(BA)$.
\item $\det A^t=\det A$.
\item $\det (cA)=c^n \det A$ for any scalar $c$.
\end{enumerate}
\end{thm}

\begin{thm}[The Invertible Matrix Theorem]\index{Invertible Matrix Theorem}
Let $A$ be an $n\times n$ matrix. The following statements are equivalent:
\begin{enumerate}
\item $A$ is invertible.
\item $\det A\neq 0$.
\item The system $A\vx=\vb$ has a unique solution for all $n\time 1$ vector $\vb$.
\item $\veczero$ is the only solution to the homogeneous system $A\vx=\veczero$.
\item $A^t$ is invertible.
\item Columns of $A$ are linearly independent.
\item $\rank (A)=n$.
\end{enumerate}
\end{thm}

\begin{problem}
Suppose that $A$ is an invertible $n\times n$ matrix. Show that $\displaystyle{\det A^{-1}=\frac{1}{\det A}}$.
\end{problem}

\begin{answer}
First we note that $\det A\neq 0$. Since $A^{-1}A=I_n$, we have $\det(A^{-1}A)=\det I_n=1$. By Theorem \ref{det properties}, $\det (A^{-1})\cdot \det A=\det(A^{-1}A)=1$ and the result follows.
\end{answer}

\begin{problem} Suppose that $A$ is an invertible $n\times n$ matrix. What is $\Nul A$? What is $\Coll A$?
\end{problem}

\begin{answer}
If $A$ is invertible, then $\veczero$ is the only solution to the system $A\vx=\veczero$, so $\Nul A=\{\veczero\}$. On the other hand, if $A$ is invertible, then for any $\vb \in \mathbb{R}^n$, the system $A\vx=\vb$ is consistent, since $\vx$ is then given by $\vx=A^{-1}\vb$. This shows that $\Coll A=\mathbb{R}^n$.
\end{answer}

\begin{exercise}\label{det exer}\quad
\begin{enumerate}[\bfseries 1.]

\item\label{3by3det} Let $A=\left[\begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array}\right]$. Using Definition \ref{detdefi}, show that
$$\det A=aei+bfg+cdh-afh-bdi-ceg.$$

\item Verify the following \underline{Vandermonde determinant}\index{Vandermonde determinant}:
$$\det\left[\begin{array}{ccc} 1 & a & a^2 \\ 1 & b & b^2 \\ 1 & c & c^2 \end{array}\right]=(a-b)(b-c)(c-a).$$
\textit{Hint}: Use Theorem \ref{rowop and det} to show that
$$\det\left[\begin{array}{ccc} 1 & a & a^2 \\ 1 & b & b^2 \\ 1 & c & c^2 \end{array}\right]=\det\left[\begin{array}{ccc} 1 & a & a^2 \\ 0 & b-a & b^2-a^2 \\ 0 & c-b & c^2-b^2 \end{array}\right]=(b-a)(c-b)\det\left[\begin{array}{ccc} 1 & a & a^2 \\ 0 & 1 & b+a \\ 0 & 1 & c+b \end{array}\right].$$

\item Compute $\det \left[\begin{array}{rrrr} 1 & -3 & 1 & -2
\\ 2 & -5 & -1 & -2 \\ 0 & -4 & 5 & 1 \\ -3 & 10 & -6 & 9
\end{array}\right]$.

\item This exercise is related with the use of dummy variables in multiple linear regression with categorical predictors. Determine whether $X^tX$ is invertible where $$X=\left[\begin{array}{rrrr} 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 0 & 1 \\ 1 & 1 & 0 & 0  \end{array}\right].$$ What if $X=\left[\begin{array}{rrrr} 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0  \end{array}\right]$? \\\textit{Hint}: You may want to use the Invertible Matrix Theorem combined with Theorem \ref{rank properties}.
\end{enumerate}
\end{exercise}

\section{Orthogonal Projection}

In this section, we look further into the geometric interpretation of statistics. First we begin with a projection of a vector onto another. Let $\vx,\vy\in \mathbb{R}^n$. Consider a problem of finding the projection $\operatorname{proj}_{\vy}$ of $\vx$ onto $\vy$ (see Figure \ref{projection onto a vector}). 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [->, thick, blue] (0,0) -- (1,4) ;
\draw [->, thick, red] (0,0) -- (4,3) ;

\draw [->, dashed] (1,4) -- (16/25*4,16/25*3) ;

\draw [->, thick, green] (0,0) -- (16/25*4,16/25*3) ;

\node[right] at (1,4) {\blue{$\vx$}};
\node[right] at (4,3) {\red{$\vy$}};
\node[below right] at (16/25*4,16/25*3) {\green{$\operatorname{proj}_{\vy} \vx$}} ;
\node[below right] at (0,0) {$\veczero$} ; 
\draw [fill=black] (0,0) circle (2pt);

\begin{scope}[xshift=10cm, yshift=2cm]

\draw [->, thick, blue] (0,0) -- (-3,1) ;
\draw [->, thick, red] (0,0) -- (2/3*4,2/3*3) ;
\draw [dashed, thick, red] (0,0) -- (-2/3*4,-2/3*3) ;

\draw [->, dashed] (-3,1) -- (-9/25*4,-9/25*3) ;
\draw [->, thick, green] (0,0) -- (-9/25*4,-9/25*3) ;

\node[left] at (-3,1) {\blue{$\vx$}};
\node[right] at (2/3*4,2/3*3) {\red{$\vy$}};
\node[below right] at (-9/25*4,-9/25*3) {\green{$\operatorname{proj}_{\vy} \vx$}} ;
\node[below right] at (0,0) {$\veczero$} ; 
\draw [fill=black] (0,0) circle (2pt);

\end{scope}

\end{tikzpicture}
\end{center}
\caption{Projection onto a vector}
\label{projection onto a vector}
\end{figure}

Let $\vz$ be the projection of $\vx$ onto $\vy$. Since $\vz$ is parallel to $\vy$, there is a constant $c$ such that $\vz=c\vy$. Since $\vz-\vx$ (the broken arrow in Figure \ref{projection onto a vector}) should be orthogonal to $\vz$, we get
$$\langle \vz-\vx, \vz \rangle=0\quad \text{that is}\quad c^2\langle \vy,\vy\rangle = c \langle \vx,\vy\rangle$$ from which a nontrivial solution $c=\frac{\langle \vx,\vy\rangle}{\langle \vy,\vy\rangle}$ follows. In summary, $\operatorname{proj}_{\vy} \vx=\frac{\langle \vx,\vy\rangle}{\langle \vy,\vy\rangle}\vy$.\\

The projection onto a subspace of $\mathbb{R}^n$ is now in order. Let $W$ be a subspace of $\mathbb{R}^n$. The \underline{orthogonal complement}\index{orthogonal complement} of $W$, denoted $W^\perp$, is defined to be
$$W^\perp=\{\vx\in \mathbb{R}^n: \langle \vx,\vw\rangle=0 \text{ for all }\vw\in W\}.$$
It is not difficult to show that $W^\perp$ is also a subspace of $\mathbb{R}^n$.

\begin{example}\label{first ortho}
Let $W=\Span \left\{\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right]\right\}$, then it is a subspace of $\mathbb{R}^3$ and
\begin{align*}W^\perp&=\left\{\left[\begin{array}{r} x \\ y \\ z \end{array}\right]:  \left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right]\cdot \left[\begin{array}{r} x \\ y \\ z \end{array}\right]=0, x,y,z\in \mathbb{R}\right\}\\
&=\left\{\left[\begin{array}{r} x \\ y \\ z \end{array}\right]:  x=0,y,z\in \mathbb{R}\right\}\\
&=\left\{\left[\begin{array}{r} 0 \\ y \\ z \end{array}\right]:  y,z\in \mathbb{R}\right\}\\
&=\Span \left\{\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right],\left[\begin{array}{r} 0 \\ 0 \\ 1 \end{array}\right] \right\}.
\end{align*}
\end{example}

\begin{thm}\label{orthdecthm}
Let $W$ be a subspace of $\mathbb{R}^n$ and $W^\perp$ be its orthogonal complement. Let $\vx\in \mathbb{R}^n$, then there is a unique pair $(\vw, \vw^\perp)$ such that $\vw\in W$, $\vw^\perp\in W^\perp$, and $\vx=\vw+\vw^\perp$.
\end{thm}

\begin{defi}
Let $\vx,\vw$, and $\vw^\perp$ be as in Theorem \ref{orthdecthm}. $\vw$ (respectively, $
\vw^\perp$) is called the \underline{orthogonal projection}\index{orthogonal projection} of $\vx$ onto $W$ (respectively, onto $W^\perp$) and is denoted by $\vw=\operatorname{proj}_W \vx$ (respectively, $\vw^\perp=\operatorname{proj}_{W^\perp} \vx$).
The unique pair $(\vw, \vw^\perp)$ is called the \underline{orthogonal decomposition}\index{orthogonal decomposition} of $\vx$ with respect to $(W,W^\perp)$. See Figure \ref{odecom figure}.
\end{defi}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [fill=yellow!30, yellow!30] (-5,-3) -- (-2,2) -- (5,3) -- (2,-2) --  cycle;

\node [above right] at (5,3) {$W$} ;
%\draw [->, dashed] (-4,0) -- (5,0) ;
%\node [above right] at (5,0) {$x$} ;
%\draw [dashed, ->, domain=-2.2:3] plot (\x, {4*\x/3});
%\node [right] at (3,4*3/3) {$y$} ;
%\draw [->, dashed] (0,-3) -- (0,4.5) ;


\draw (-2,4) -- (1.5,-3);

\draw [fill=black] (0,0) circle (2pt) ;
\node [left] at (-2,4) {$W^\perp$} ;

\draw [fill=black] (0.5,4) circle (2pt) ;
\draw [dashed] (0.5,4) -- (2,1);
\draw [dashed] (0.5,4) -- (-1.5,3);
\draw [very thick, ->] (0,0) -- (0.5,4) ;
\node [above] at (0.5,4) {$\vx$} ;
\node [below left] at (0,0) {$\bm{0}$} ;

\draw [fill=red, red] (2,1) circle (2pt) ;
\draw [very thick, red,->] (0,0) -- (2,1) ;

\draw [fill=blue, blue] (-1.5,3) circle (2pt) ;
\draw [very thick, blue,->] (0,0) -- (-1.5,3) ;

\node [red, right] at (2,1) {$\vw= \operatorname{proj}_{W}\vx$};
\node [blue, left] at (-1.5,3) {$\vw^\perp=\operatorname{proj}_{W^\perp} \vx$};

\end{tikzpicture}
\end{center}
\caption{Orthogonal decomposition}
\label{odecom figure}
\end{figure}

\begin{example}
Let $W$ be as in Example \ref{first ortho}, then the orthogonal decomposition of $\left[\begin{array}{r} 2 \\ 3 \\ 1 \end{array}\right]$ with respect to $(W,W^\perp)$ is $\left(\left[\begin{array}{r} 2 \\ 0 \\ 0 \end{array}\right], \left[\begin{array}{r} 0 \\ 3 \\ 1 \end{array}\right]\right)$.
\end{example}

We now consider a problem of finding the orthogonal decomposition in a more general setting.

\begin{problem}\label{gensol}
Let $W$ be a subspace of $\mathbb{R}^n$. Show that there is a unique $n\times n$ matrix $P$ such that $P\vz=\operatorname{proj}_W \vz$ for all $\vz \in \mathbb{R}^n$.
\end{problem}

\begin{answer}
By Remark \ref{W as span}, there is a linearly independent set $\{\vu_1,\ldots,\vu_k\}$ such that $W=\Span\{\vu_1,\ldots,\vu_k\}$ (so it is necessary that $k\leq n$ -- see Exercise \ref{matalgexer}.\ref{npdep}). Define $A=\left[\begin{array}{cccc} | & | &   & | \\ \vu_1 & \vu_2 & \cdots & \vu_k \\ | & | &  & | \end{array}\right]$, then $W=\Coll A$. Let $\vz\in \mathbb{R}^n$. Since $\vw=\operatorname{proj}_W \vz \in W$, there is $\vx\in \mathbb{R}^k$ such that $A\vx=\vw$. Since $\vu_i\in W$ for all $i$, $1\leq i \leq k$, it follows that $\langle \vu_i, \vw^\perp\rangle =\langle \vu_i, \vz-A\vx \rangle= \langle \vu_i, \vz\rangle -\langle \vu_i, A\vx \rangle=0$ for all $i$, or equivalently, $A^t\vz=A^tA\vx$. Since $\rank(A^tA)=\rank (A)=\dim W=k$, $A^tA$ is an invertible $k \times k$ matrix and hence $\vw=A\vx=P\vz$, where $P=A(A^tA)^{-1}A^t$. Uniqueness of $P$ follows from the fact that the $i^{\text{th}}$ column of $P$ is given by $P\bm{e}_i$, where $\bm{e}_i$ is as in Example \ref{extract row col}, and equals $\operatorname{proj}_W \bm{e}_i$.
\end{answer}

\begin{remark}\label{projmat}
Let $P_W$ denote the matrix $A(A^tA)^{-1}A^t$ as in the answer of Problem \ref{gensol}. $P_W$ is called the \underline{projection matrix}\index{projection matrix} corresponding to $W$. Problem \ref{gensol} shows that the orthogonal projection of $\vz\in \mathbb{R}^n$ onto $W$ can be obtained by taking the product between $P_W$ and $\vz$.
\end{remark}

\begin{problem}\label{proj matrix prop}
Let $W$ be a subspace of $\mathbb{R}^n$ and let $P_W$ denote the corresponding projection matrix. Show that $P_W=P_W^t$ and $P_W^2=P_W$.
\end{problem}

\begin{answer}
By Remark \ref{W as span}, there is a linearly independent set $\{\vu_1,\ldots,\vu_k\}$ such that $W=\Span\{\vu_1,\ldots,\vu_k\}$. Let $A=\left[\begin{array}{cccc} | & | &   & | \\ \vu_1 & \vu_2 & \cdots & \vu_k \\ | & | &  & | \end{array}\right]$, then $P_W=A(A^tA)^{-1}A^t$ (see Remark \ref{projmat}). Now it is easy to check that (see Exercise \ref{basics of matrix exer}.\ref{transpose change} and Problem \ref{inverse transpose})
$$P_W^t=(A(A^tA)^{-1}A^t)^t=(A^t)^t((A^tA)^{-1})^tA^t=A((A^tA)^t)^{-1}A^t=P_W$$
and
$$P_W^2=(A(A^tA)^{-1}\blue{A^t})(\blue{A}(A^tA)^{-1}A^t)=A(A^tA)^{-1}(\blue{A^tA})(A^tA)^{-1}A^t=A(A^tA)^{-1}A^t=P_W.$$
\end{answer}

The orthogonal projection has a nice geometric characterization that makes it a useful tool in linear regression.

\begin{thm}\label{closest thm}
Let $W$ be a subspace of $\mathbb{R}^n$ and let $\vx \in \mathbb{R}^n$. Then
$$\|\vx-\operatorname{proj}_W \vx\|< \|\vx-\vw\|$$
for all $\vw \in W$, $\vw\neq \operatorname{proj}_W \vx$. In other words, $\operatorname{proj}_W \vx$ is the closest point in $W$ to $\vx$.
\end{thm}

Theorem \ref{closest thm} is visualized in Figure \ref{closest pt}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\draw (-1,0) -- (6,0) ;
\node [above right] at (6,0) {$W$} ;

\draw (0,-1) -- (0,4) ;
\node [above right] at (0,4) {$W^\perp$} ;

\draw [red, very thick, dashed] (4,0) -- (4,3) ;
\draw [very thick, dashed] (5,0) -- (4,3) ;
\draw [very thick, dashed] (2.5,0) -- (4,3) ;

\node [above right] at (4,3) {$\vx$};
\node [red, below] at (4,0) {$\operatorname{proj}_W \vx$};

\node [below left] at (0,0) {$\veczero$};

\draw [fill=black] (4,3) circle (2.5pt) ;
\draw [red, fill=red] (4,0) circle (2.5pt) ;
\draw [fill=black] (5,0) circle (2.5pt) ;
\draw [fill=black] (2.5,0) circle (2.5pt) ;

\end{tikzpicture}
\end{center}
\caption{Geometric meaning of the orthogonal projection}
\label{closest pt}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7, >=triangle 45]

\draw [->] (-1,0)--(13,0);
\draw [->] (0,-1)--(0,8);

\node [above right] at (13,0) {$x$} ;
\node [above right] at (0,8) {$y$} ;
\node [below left] at (0,0) {$O$} ;

\node [above] at (3,3) {$(x_1,y_1)$} ;
\node [below] at (4,2) {$(x_2,y_2)$} ;
\node [above] at (5,4) {$(x_3,y_3)$} ;
\node [above] at (9,6) {$(x_i,y_i)$} ;
\node [below] at (12,5) {$(x_n,y_n)$} ;

\draw [red] (-1,1.4077-0.3822)--(13,13*0.3822+1.4077);

\node [right] at (13,13*0.3822+1.4077) {$y=\beta_0+\beta_1 x$};

\draw [blue, dashed] (3,3)--(3,0.3822*3+1.4077) ;
\draw [blue, dashed] (4,2)--(4,0.3822*4+1.4077) ;
\draw [blue, dashed] (5,4)--(5,0.3822*5+1.4077) ;
\draw [blue, dashed] (9,6)--(9,0.3822*9+1.4077) ;
\draw [blue, dashed] (12,5)--(12,0.3822*12+1.4077) ;

\draw [fill=black] (3,3) circle (3pt) ;
\draw [fill=black] (4,2) circle (3pt) ;
\draw [fill=black] (5,4) circle (3pt) ;
\draw [fill=black] (9,6) circle (3pt) ;
\draw [fill=black] (12,5) circle (3pt) ;

\end{tikzpicture}
\end{center}
\caption{Least square fitting line}
\label{least square fit}
\end{figure}

\begin{example}[Simple Linear Regression]\index{simple linear regression}\label{lsexample}
Given a set of points in $\mathbb{R}^2$, consider a problem of finding $\beta_0$ and $\beta_1$ so that the line $y=\beta_0+\beta_1 x$ describes the trend in the set (see Figure \ref{least square fit}). The criterion to determine $\beta_0$ and $\beta_1$ is to minimize the \underline{residual sum of squares}\index{residual sum of squares}
\begin{equation}\label{lsformula} \sum_{i=1}^n |y_i-(\beta_0+\beta_1 x_i)|^2. \end{equation}
Define $n$-vectors $\bm{1}=\left[\begin{array}{c} 1 \\ \vdots \\ 1
\end{array}\right]$, $\vx=\left[\begin{array}{c} x_1 \\ \vdots \\ x_n
\end{array}\right]$, and  $\vy=\left[\begin{array}{c} y_1 \\ \vdots \\ y_n
\end{array}\right]$, then (\ref{lsformula}) can be viewed as $\|\vy-(\beta_0\bm{1}+\beta_1\vx)\|^2$ and the problem of finding $\beta_0$ and $\beta_1$ boils down to finding a vector $\vw_0$ in $W$ that is closest to $\vy$, where $W=\Span \{\bm{1},\vx\}$. By Theorem \ref{closest thm} and Problem \ref{gensol}, we get that $\vw_0=\operatorname{proj}_W \vy=A(A^tA)^{-1}A^t\vy$, where $A=\left[\begin{array}{cc} | & |  \\ \bm{1} & \vx \\ | & |  \end{array}\right]=\left[\begin{array}{cc} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n  \end{array}\right]$. Let $\bar{x}=\frac{\sum_{i=1}^n x_i}{n}$ and $\bar{y}=\frac{\sum_{i=1}^n y_i}{n}$, then a simple calculation shows that
$$A^tA=\left[\begin{array}{cc} n & n\bar{x}  \\ n\bar{x} & \sum_{i=1}^n x_i^2 \end{array}\right]$$ and that
\begin{align*}\left[\begin{array}{c} \beta_0 \\ \beta_1
\end{array}\right]&=\red{(A^tA)^{-1}}\blue{A^t\vy}\\
&=\red{\frac{1}{n\sum_{i=1}^n x_i^2-n^2\bar{x}^2}\left[\begin{array}{cc} \sum_{i=1}^n x_i^2  & -n\bar{x}  \\ -n\bar{x} & n \end{array}\right]}\blue{\left[\begin{array}{c} n\bar{y}  \\ \sum_{i=1}^n x_iy_i \end{array}\right]}\\
&=\left[\begin{array}{c} \bar{y}-\frac{SXY}{SXX}\bar{x} \\ \frac{SXY}{SXX} \end{array}\right],
\end{align*}
where
$$SXX=\sum_{i=1}^n (x_i-\bar{x})^2=\sum_{i=1}^n x_i^2-n\bar{x}^2$$
and
$$SXY=\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^n x_iy_i-n\bar{x}\bar{y}.$$ See Example \ref{same with sdt} for a different approach to the problem.
\end{example}

We can extend the idea in Example \ref{lsexample}.

\begin{example}[Multiple Linear Regression]\index{multiple linear regression}
Let $\vy=\left[\begin{array}{c} y_1 \\ \vdots \\ y_n \end{array}\right]$. Define an $n\times (p+1)$ matrix $X$ by $X=\left[\begin{array}{cccc} 1 & x_{1,1} & \cdots &  x_{1,p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n,1} & \cdots &  x_{n,p} \end{array}\right]$. We want to find $\bm{\beta}=\left[\begin{array}{c} \beta_0 \\ \vdots \\ \beta_p \end{array}\right]$ that minimizes the residual sum of squares
$$ \sum_{i=1}^n |y_i-(\beta_0+\beta_1 x_{i,1}+\cdots+\beta_p x_{i,p})|^2 =\|\vy-X\bm{\beta}\|^2.$$
Following the argument as in Example \ref{lsexample}, one can see that the minimizing $\bm{\beta}$ is given by
$$\bm{\beta}=(X^tX)^{-1}X^t\vy.$$
\end{example}

\begin{exercise} \quad \label{ortho exer}
\begin{enumerate}[\bfseries 1.]
\item Let $\vx=\left[\begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array}\right]$ and $\bm{1}=\left[\begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array}\right]$.
\begin{enumerate}
\item Find $\operatorname{proj}_{\bm{1}}\vx$. 
\item Compute $\vx-\operatorname{proj}_{\bm{1}}\vx$.
\item Express the sample mean of $x_1,x_2,\ldots,x_n$ in terms of $\|\vx-\operatorname{proj}_{\bm{1}}\vx\|$.
\end{enumerate}

\item Let $W$ be a subspace of $\mathbb{R}^n$ and $W^\perp$ be its orthogonal complement. Show that $W\cap W^\perp=\{\veczero\}$.

\item Let $W$ be a subspace of $\mathbb{R}^n$. Show that $W^{\perp\perp}$, the orthogonal complement of $W^{\perp}$, equals $W$.

\item\label{ptp2p} Let $P$ be an $n\times n$ matrix such that $P^2=P=P^t$. Let $W=\{P\vx:\vx\in \mathbb{R}^n\}$. Show that $P_W=P$.\\
\textit{Hint}: For any $\vx\in \mathbb{R}^n$, show that $\vx=P\vx+(I_n-P)\vx$ and that $\langle P\vx, (I_n-P)\vx\rangle=0$.

\item\label{w=Pw} Let $W$ be a subspace of $\mathbb{R}^n$. Let $P_W$ denote the projection matrix corresponding to $W$.
\begin{enumerate}
\item Prove that $\vw\in W$ if and only if $\vw=P_W\vw$.
\item Prove that $W=\{P_W\vx:\vx \in \mathbb{R}^n\}$.
\item Prove that $\dim W=\rank (P_W)$.\\
\textit{Hint}: Use Theorem \ref{rank properties}.
\end{enumerate}

\item Let $V$ and $W$ be subspaces of $\mathbb{R}^n$. Show that $\langle \vv, \vw\rangle=0$ for all $\vv\in V$ and for all $\vw\in W$ if and only if $P_{V}P_{W}=O$, the zero matrix. Here $P_V$ (respectively, $P_W$) denotes the projection matrix corresponding to $V$ (respectively, $W$). \\
\textit{Hint}: Note that $\langle \vv, \vw\rangle=\langle P_V\vv, P_W\vw\rangle$ for $\vv\in V$ and $\vw\in W$ (see Exercise \ref{ortho exer}.\ref{w=Pw}). You may want to use Example \ref{extract row col} to get the $(i,j)$-element of $P_{V}P_{W}$.

\item Let $W$ be a subspace of $\mathbb{R}^n$ and $\vx\in \mathbb{R}^n$. Prove that
$$\|\vx-\operatorname{proj}_W \vx\|=\max\{|\langle \vx,\vy\rangle|: \vy \in W^\perp, \|\vy\|\leq 1 \}.$$

\end{enumerate}
\end{exercise}

\section{Eigenvectors and Eigenvalues}

\textbf{Definition} An \underline{eigenvector}\index{eigenvector} of an $n\times n$
matrix $A$ is a nonzero vector $\vxi$ such that $A\vxi=\lambda \vxi$
for some scalar $\lambda$. A scalar $\lambda$ is called an
\underline{eigenvalue}\index{eigenvalue} of $A$ if there is a nontrivial solution
$\vxi$ of $A \vxi=\lambda \vxi$; such a $\vxi$ is called an
eigenvector corresponding to $\lambda$.

\begin{remark} By definition, an eigenvector must be a \textit{nonzero} vector, but eigenvalue could be zero.
\end{remark}

\begin{example} Let $A=\left[\begin{array}{rr} 3 & -2 \\ 1 & 0
\end{array}\right]$ and $\vxi=\left[\begin{array}{r} 2 \\ 1 \end{array}\right]$, then $A\vxi=\left[\begin{array}{r}4 \\ 2\end{array}\right]=2\vxi$. Therefore
$2$ is an eigenvalue of $A$ and $\vxi$ is an
eigenvector of $A$ corresponding to the eigenvalue $2$.
Note that $\left[\begin{array}{r} 2 \\
1 \end{array}\right]$ is \textit{not} the only eigenvector of $A$
corresponding to the eigenvalue 2. For example, $\left[\begin{array}{r} 6 \\
3 \end{array}\right]$ is another eigenvector of $A$ corresponding to
$2$. In fact, if $\vxi$ is an eigenvector of $A$ corresponding to an
eigenvalue $\lambda$, then so is any nonzero multiple $c\vxi$
of $\vxi$.
\end{example}

\begin{problem}\label{evalue problem} Let $B=\left[\begin{array}{rr} 1 & 6 \\
5 & 2
\end{array}\right]$. Is $\vxi=\left[\begin{array}{r}  6 \\ -5
\end{array}\right]$ an eigenvector of $B$? How about $\veta=\left[\begin{array}{r}  3 \\
-2 \end{array}\right]$?
\end{problem}

\begin{answer}
Since $B\vxi=\left[\begin{array}{rr} 1 & 6 \\ 5 & 2
\end{array}\right]\left[\begin{array}{r} 6 \\
-5 \end{array}\right]=\left[\begin{array}{r} -24 \\
20 \end{array}\right]=-4\left[\begin{array}{r} 6 \\
-5 \end{array}\right]=-4\vxi$, we see that $\vxi$ is an eigenvector of $B$ corresponding to the eigenvalue $-4$. However,
$B\veta=\left[\begin{array}{rr} 1 & 6 \\ 5 & 2
\end{array}\right]\left[\begin{array}{r} 3 \\
-2 \end{array}\right]=\left[\begin{array}{r} -9 \\
11 \end{array}\right]$ is not a scalar multiple of $\veta$, so $\veta$ is \textit{not} an eigenvector of $B$.
\end{answer}

A matrix may have more than one eigenvalue.

\begin{example}\label{second evalue} In Problem \ref{evalue problem} above, we observed that
$\lambda=-4$ is an eigenvalue of $B$. However, $\bm{\zeta}=\left[\begin{array}{r} 1 \\
1 \end{array}\right]$ is an eigenvector of $B$ corresponding to the
eigenvalue $7$, since
$$B\bm{\zeta}=\left[\begin{array}{rr} 1 & 6 \\ 5 & 2
\end{array}\right]\left[\begin{array}{r} 1 \\
1 \end{array}\right]=\left[\begin{array}{r} 7 \\
7 \end{array}\right]=7\left[\begin{array}{r} 1 \\
1 \end{array}\right]=7\bm{\zeta}.$$
\end{example}

\begin{problem} \label{ABBA same eigen}
Let $A$ and $B$ be $n\times n$ matrices. Suppose that $\lambda$ is an eigenvalue of $AB$. Show that $\lambda$ is an eigenvalue of $BA$.
\end{problem}

\begin{answer}
First, suppose $\lambda=0$, then there is $\vxi\neq \veczero$ such that $AB\vxi=\veczero$. By Invertible Matrix Theorem, $\det(AB)=0$. Therefore, $\det(BA)=0$ and again by Invertible Matrix Theorem, there is $\veta\neq \veczero$ such that $BA\veta=\veczero$, in other words, $0$ is an eigenvalue of $BA$. Now suppose $\lambda\neq 0$, then there is $\vxi\neq \veczero$ such that $AB\vxi=\lambda\vxi$. Let $\veta=B\vxi$, then $\veta\neq \veczero$ and it follows that $BA\veta=BAB\vxi=\lambda B\vxi=\lambda\veta$, and this shows that $\lambda$ is an eigenvalue of $BA$.
\end{answer}

Suppose $\lambda$ is an eigenvalue of $A$. How do
we find all the eigenvectors corresponding to $\lambda$?

\begin{defi} Let $A$ be an $n\times n$ square matrix and
$\lambda$ be an eigenvalue of $A$. $\Nul (A-\lambda I_n)$, the null space of $A-\lambda I_n$, is called
the \underline{eigenspace}\index{eigenspace} of $A$ corresponding to
$\lambda$. Therefore, the eigenspace of $A$ corresponding to $\lambda$ is a subspace of $\mathbb{R}^n$. The \underline{geometric multiplicity}\index{geometric multiplicity} of
$\lambda$ is the dimension of the eigenspace of $A$ corresponding to $\lambda$.
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item The geometric multiplicity of an eigenvalue is always greater than or equal to 1.
\item Any nonzero vector in the eigenspace corresponding to $\lambda$ is an eigenvector corresponding to $\lambda$.
\end{enumerate}
\end{remark}

\begin{thm} If $\vxi_1,\vxi_2,\cdots,\vxi_r$ are eigenvectors that
correspond to distinct eigenvalues
$\lambda_1,\lambda_2,\cdots,\lambda_r$ of an $n\times n$ matrix $A$,
then the set $\{\vxi_1,\vxi_2,\cdots,\vxi_r\}$ is linearly
independent.
\end{thm}

\begin{example} From Problem \ref{evalue problem} and Example \ref{second evalue} above, we see that $\left[\begin{array}{r} 6
\\ -5 \end{array}\right]$ and $\left[\begin{array}{r} 1 \\ 1 \end{array}\right]$ are eigenvectors of $B=\left[\begin{array}{rr} 1 & 6 \\ 5 & 2 \end{array}\right]$ corresponding to $-4$ and $7$, respectively. So $\left\{\left[\begin{array}{r} 6
\\ -5 \end{array}\right],\left[\begin{array}{r} 1 \\ 1 \end{array}\right]\right\}$ is linearly independent.
\end{example}

\begin{exercise}\label{evev exer}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $A=\left[\begin{array}{rrr} 4 & -1 & 6 \\
2 & 1 & 6 \\ 2 & -1 & 8 \end{array}\right]$. Show that $2$ is an
eigenvalue of $A$ with an eigenvector $\left[\begin{array}{r} 3 \\
0 \\ -1
\end{array}\right]$. Find a basis for the eigenspace of $A$
corresponding to $2$.

\item An $n\times n$ matrix $A=[a_{ij}]_{i,j=1}^n$ is called a \underline{Markov matrix}\index{Markov matrix} if each row sum equals $1$, that is, $\displaystyle{\sum_{j=1}^n a_{ij}=1}$ for all $i$. Show that $1$ is an eigenvalue for every Markov matrix.\\
\textit{Hint}: Try to find an eigenvector.

\item \label{poly eigenvalue} Suppose that $\lambda$ is an eigenvalue of $A$. For any polynomial $p$, show that $p(\lambda)$ is an eigenvalue of $p(A)$.\\
\textit{Hint}: Let $\vxi$ be an eigenvector of $A$ corresponding to $\lambda$. Show that $\vxi$ is an eigenvector of $p(A)$ corresponding to $p(\lambda)$.

\item\label{proj 1 or 0} Let $S$ be a square matrix such that $S^2=S$. Show that every eigenvalue of $S$ must be either $1$ or $0$. 

\item Suppose that $\lambda$ is an eigenvalue of an invertible matrix $A$.
\begin{enumerate}
\item Explain why $\lambda$ cannot be equal to $0$.
\item Show that $\frac{1}{\lambda}$ is an eigenvalue of $A^{-1}$.\\
\textit{Hint}: Let $\vxi$ be an eigenvector of $A$ corresponding to $\lambda$. Show that $\vxi$ is an eigenvector of $A^{-1}$ corresponding to $\frac{1}{\lambda}$.
\end{enumerate}

\item\label{evpositive} Let $A$ be an $m\times n$ matrix. Show that all eigenvalues of $B=A^tA$ are nonnegative real numbers. \\
\textit{Hint}: You may want to use Exercise \ref{basics of matrix exer}.\ref{general transfer}.
\end{enumerate}
\end{exercise}

\section{The Characteristic Equation}

Let $A=\left[\begin{array}{rr} 2 & 3 \\ 3 & -6 \end{array}\right]$. We want to find all eigenvalues of
$A$. By definition, $\lambda$ is an eigenvalue of $A$ if and only if
$A\vxi=\lambda\vxi$ for some nonzero vector $\vxi$, which is the same
as saying that the homogeneous system $(A-\lambda I_2)\vxi=\veczero$
has a nontrivial solution. Therefore by the Invertible Matrix
Theorem, we see that $\lambda$ is an eigenvalue of $A$ if and only
if the matrix $A-\lambda I_2$ is singular, that
is, if and only if $\det (A-\lambda I_2)=0$. Now $A-\lambda I_2=\left[\begin{array}{cc} 2-\lambda & 3 \\
3 & -6-\lambda \end{array}\right]$ and hence $\det (A-\lambda
I_2)=(2-\lambda)(-6-\lambda)-9=\lambda^2+4\lambda-21=(\lambda+7)(\lambda-3)$.
Therefore exactly $-7$ and $3$ are eigenvalues of $A$. In general, to find all the eigenvalues of a given $n\times n$
matrix $A$,
\begin{enumerate}
\item First, find $A-\lambda I_n$;
\item Second, compute $\det (A-\lambda I_n)$, which is a polynomial
in $\lambda$ of degree $n$;
\item Third, finally solve the equation $\det (A-\lambda I_n)=0$. The
solution set of this equation is exactly the set of eigenvalues of
$A$.
\end{enumerate}

\begin{remark}
Since eigenvalues are roots of a polynomial, they may not be real numbers. However, all eigenvalues of a \textit{symmetric} matrix are real. See Theorem \ref{all real}.
\end{remark}

\begin{defi} The polynomial $\det (A-\lambda I_n)$ is called
the \underline{characteristic polynomial}\index{characteristic polynomial} of $A$. The equation $\det
(A-\lambda I_n)=0$ is called the
\underline{characteristic equation}\index{characteristic equation} of $A$.
\end{defi}

\begin{problem} Find all the eigenvalues of
$A=\left[\begin{array}{rrrr} 5 & -2 & 6 & -1 \\ 0 & 3 & -8 & 0 \\
0 & 0 & 5 & 4 \\ 0 & 0 & 0 & 1 \end{array}\right]$.
\end{problem}

\begin{answer}
The characteristic polynomial of $A$ is given by the determinant of
$$A-\lambda I_n=\left[\begin{array}{cccc} 5-\lambda & -2 & 6 & -1 \\ 0 & 3-\lambda & -8 & 0 \\
0 & 0 & 5-\lambda & 4 \\ 0 & 0 & 0 & 1-\lambda \end{array}\right].$$
Since $A-\lambda I_n$ is an upper triangle matrix, by Example \ref{triangular det}, we have $\det(A-\lambda I_n)=(5-\lambda)^2(3-\lambda)(1-\lambda)$. This gives three distinct eigenvalues $5$, $3$, and $1$.
\end{answer}

\begin{remark}
In general, the eigenvalues of a triangular matrix are precisely the
diagonal entries of the matrix.
\end{remark}

\begin{defi} Let $A$ and $B$ be $n\times n$ matrices. $A$ is
said to be \underline{similar}\index{similar} to $B$ if there is an invertible
matrix $P$ such that $P^{-1}AP=B$. When $A$ is similar to $B$, then we write $A\sim B$.
\end{defi}

\begin{example} $A=\left[\begin{array}{rr} 1 & 2 \\ 1 & 4
\end{array}\right]$ is similar to $B=\left[\begin{array}{rr} 2 & 4 \\ 1 &
3 \end{array}\right]$ because with $P=\left[\begin{array}{rr} 1 & -1
\\ 0 & 1 \end{array}\right]$ (so $P^{-1}=\left[\begin{array}{rr} 1 &
1 \\ 0 & 1
\end{array}\right]$)
$$P^{-1}AP=\left[\begin{array}{rr} 1 & 1 \\ 0 & 1
\end{array}\right]\left[\begin{array}{rr} 1 & 2 \\ 1 & 4
\end{array}\right]\left[\begin{array}{rr} 1 &
-1 \\ 0 & 1 \end{array}\right]=\left[\begin{array}{rr} 2 & 6 \\ 1 &
4 \end{array}\right]\left[\begin{array}{rr} 1 & -1 \\ 0 & 1
\end{array}\right]=\left[\begin{array}{rr} 2 & 4 \\ 1 &
3 \end{array}\right]=B.$$
\end{example}

\begin{remark} \quad
\begin{enumerate}
\item If $A$ is similar to $B$, then $B$ is similar
to $A$ (i.e., $A\sim B \Longrightarrow B\sim A$).
\item $A$ is similar to $A$ itself ($A\sim A$).
\item If $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is
similar to $C$ ($A\sim B \text{ and } B\sim C \Longrightarrow
A\sim C$).
\end{enumerate}
\end{remark}

\begin{thm}\label{similar same} If $A$ and $B$ are similar $n\times n$ matrices,
then they have the same characteristic polynomials and
hence the same eigenvalues.
\end{thm}

\begin{exercise}\quad \label{char eq exer}
\begin{enumerate}[\bfseries 1.]

\item Find all eigenvalues of each of the following matrices:

\begin{enumerate}
\item $A=\left[\begin{array}{rr} 5 & 1 \\ 3 & 3 \end{array}\right]$
\item $B=\left[\begin{array}{rrr} 1 & 2 & 1 \\ 0 & 1 & 8 \\ 0 & 2 & 1\end{array}\right]$
\end{enumerate}


\item \label{prod equals det} Let $\lambda_1,\ldots,\lambda_n$ (not necessarily distinct) be eigenvalues of $A$. Prove that $\det A=\prod_{i=1}^n\lambda_i$.

\item Let $A$ be an $n\times n$ matrix. Let $M$ be an invertible $n\times n$ matrix.
\begin{enumerate}
\item Show that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $M^{-1}AM$ (see Theorem \ref{similar same}).
\item Show that $tr(A)=tr(M^{-1}AM)$. \\\textit{Hint}: See Exercise \ref{basics of matrix exer}.\ref{tracedef}.
\end{enumerate}
\end{enumerate}
\end{exercise}

\section{Diagonalization}
There are many practical applications in which we need to
compute $A^k$ for large $k$. This requires a lot of computations for
general $n\times n$ matrix $A$. When $A$ is diagonal, however,
the computation is quite simple.

\begin{example} If $D=\left[\begin{array}{rr} 5 & 0 \\ 0 & 3
\end{array}\right]$, then $D^2=\left[\begin{array}{cr} 25 & 0 \\ 0 &
9
\end{array}\right]$. In general, $D^n=\left[\begin{array}{cc} 5^n & 0 \\ 0 &
3^n
\end{array}\right]$.
\end{example}

\begin{example}\label{first diago} We will compute $A^{10}$, where $A=\left[\begin{array}{rr} 7
& 2 \\ -4 & 1 \end{array}\right]$. Note that computing $A^{10}$ with bare hands requires considerable amount of time, so we need a trick to get this done. To this end, define a
matrix $P=\left[\begin{array}{rr} 1 & 1 \\ -1 & -2
\end{array}\right]$, then $P$ is invertible with $P^{-1}=\left[\begin{array}{rr} 2 & 1 \\ -1 &
-1 \end{array}\right]$ and
$$P^{-1}AP=\left[\begin{array}{rr} 2 & 1 \\ -1 &
-1 \end{array}\right]\left[\begin{array}{rr} 7 & 2 \\ -4 & 1
\end{array}\right]\left[\begin{array}{rr} 1 & 1 \\ -1 & -2
\end{array}\right]=\left[\begin{array}{rr} 10 & 5 \\ -3 &
-3 \end{array}\right]\left[\begin{array}{rr} 1 & 1 \\ -1 & -2
\end{array}\right]=\left[\begin{array}{rr} 5 &0 \\
0 & 3 \end{array}\right].$$
In other words, $A$ is similar to a diagonal matrix $D=\left[\begin{array}{rr} 5 & 0 \\ 0 & 3
\end{array}\right]$. Taking the 10$^{\text{th}}$ power of the both
sides gives
\begin{enumerate}
\item[RHS:] $D^{10}=\left[\begin{array}{cc} 5^{10} & 0 \\ 0 & 3^{10}
\end{array}\right]$.
\item[LHS:] $(P^{-1}AP)^{10}=\underbrace{(P^{-1}AP)(P^{-1}AP)\cdots
(P^{-1}AP)}_{10 \text{ times}}=P^{-1}A(PP^{-1})A(PP^{-1})A\cdots
(PP^{-1})AP$, which reduces to $P^{-1}A^{10}P$.
\end{enumerate}

Therefore we conclude that $P^{-1}A^{10}P=\left[\begin{array}{cc}
5^{10} & 0 \\ 0 & 3^{10}
\end{array}\right]$, or
\begin{align*}A^{10}&=P\left[\begin{array}{cc}
5^{10} & 0 \\ 0 & 3^{10}
\end{array}\right]P^{-1}\\
&=\left[\begin{array}{rr} 1 & 1 \\ -1 & -2
\end{array}\right]\left[\begin{array}{cc}
5^{10} & 0 \\ 0 & 3^{10}
\end{array}\right]\left[\begin{array}{rr}
2 & 1 \\ -1 & -1
\end{array}\right]\\
&=\left[\begin{array}{cc}
2\cdot5^{10}-3^{10} & 5^{10}-3^{10} \\ -2\cdot5^{10}+2\cdot3^{10} &
-5^{10}+2\cdot3^{10}
\end{array}\right].\end{align*}
In general, $$A^{n}=\left[\begin{array}{cc}
2\cdot5^{n}-3^{n} & 5^{n}-3^{n} \\ -2\cdot5^{n}+2\cdot3^{n} &
-5^{n}+2\cdot3^{n}
\end{array}\right]$$
for any $n\in \mathbb{N}$.
\end{example}

Given $A$, is it always possible to find an
invertible matrix $P$ such that $P^{-1}AP$ is diagonal? The answer is \textit{no}.

\begin{defi} A square matrix $A$ is said to be
\underline{diagonalizable}\index{diagonalizable} if $A$ is similar to a diagonal matrix,
that is, if $P^{-1}AP=D$ for some invertible matrix $P$ and some diagonal matrix $D$.
\end{defi}

\begin{remark}
Suppose that $A$ is diagonalizable with $P^{-1}AP=D$. By Theorem \ref{similar same}, $A$ and $D$ have the same eigenvalues and it follows that the diagonal entries of $D$ are eigenvalues of $A$.
\end{remark}

\begin{example} The matrix $A=\left[\begin{array}{rr} 7 & 2 \\ -4 &
1 \end{array}\right]$ in Example \ref{first diago} is diagonalizable.
\end{example}

If $A$ is diagonalizable, how can we find a
matrix $P$ such that $P^{-1}AP$ is diagonal? The next theorems characterize diagonalizable matrices and states how to find $P$ such that $P^{-1}AP$ is diagonal for a given diagonalizable matrix $A$.

\begin{thm}[The Diagonalization Theorem, Part 1]\index{Diagonalization Theorem!part 1}An $n\times n$ matrix
$A$ is diagonalizable if and only if $A$ has $n$ linearly
independent eigenvectors. In fact, $P^{-1}AP=D$, with $D$ diagonal,
if and only if the columns of $P$ are $n$ linearly independent
eigenvectors of $A$. In this case, the diagonal entries of $D$ are
eigenvalues of $A$ that correspond, respectively, to the
eigenvectors (i.e. columns) in $P$.
\end{thm}

\begin{thm}[The Diagonalization Theorem, Part 2]\index{Diagonalization Theorem!part 2} Let $A$ be an $n\times
n$ matrix with \textit{distinct} eigenvalues $\lambda_1,\cdots, \lambda_p$.
\begin{enumerate}
\item For $1\leq k\leq p$, the dimension of the eigenspace
corresponding to $\lambda_k$ (that is, the geometric multiplicity of
$\lambda_k$) is always greater than equal to $1$ and less than or
equal to the algebraic multiplicity of the eigenvalue $\lambda_k$ as
a zero of
the characteristic polynomial of $A$.
\item The matrix $A$ is diagonalizable if and only if the
geometric multiplicity of $\lambda_k$ equals the
algebraic multiplicity of $\lambda_k$ for \textit{each} $k$. In particular, if $A$ has $n$ distinct
eigenvalues, then $A$ is diagonalizable.
\item If $\mathcal{B}_j$ (resp. $\mathcal{B}_k$) is a basis
for the eigenspace corresponding to $\lambda_j$ (resp. $\lambda_k$), then the set $\mathcal{B}_j \cup \mathcal{B}_k$ is linearly independent.
\end{enumerate}
\end{thm}

\begin{example} We go back to Example \ref{first diago} and see how
we can obtain the matrix $P$. $A=\left[\begin{array}{rr} 7 & 2 \\
-4 & 1
\end{array}\right]$, the eigenvalues of $A$ are the zeros of the
characteristic polynomial of $A$: $\det(A-\lambda
I_2)=(7-\lambda)(1-\lambda)+8=\lambda^2-8\lambda+15=(\lambda-3)(\lambda-5)$. To Find an eigenvector corresponding to $\lambda=5$, we solve $(A-5I_2)\vxi=\veczero$ and this gives $\vxi=\left[\begin{array}{r} 1 \\ -1 \end{array}\right]$. Similarly, an eigenvector $\left[\begin{array}{r} 1 \\ -2 \end{array}\right]$ corresponding to $\lambda=3$ is obtained by solving $(A-3I_2)\vxi=\veczero$. Therefore, we can take $P=\left[\begin{array}{rr} 1 &  1 \\ -1 & -2 \end{array}\right]$.
\end{example}

\begin{problem} Diagonalize $A$ (that is, find an invertible matrix $P$
and a diagonal matrix $D$
such that $P^{-1}AP=D$), if possible, where $A=\left[\begin{array}{rrr} 1 & 3 & 3 \\
-3 & -5 & -3
\\ 3 & 3 & 1
\end{array}\right]$.
\end{problem}

\begin{answer}
Note that $A-\lambda I_3=\left[\begin{array}{ccc} 1-\lambda & 3 & 3 \\
-3 & -5-\lambda & -3
\\ 3 & 3 & 1-\lambda \end{array}\right]$ does not contain zeros, so computing its determinant is not handy. To compute the determinant easily, we produce zeros by applying row operations $R_3\mapsto R_3+R_2$ and $R_2\mapsto R_2+R_1$. These row operations and the cofactor expansion along the first column gives
\begin{align*}\det(A-\lambda I_3)&=\det\left(\left[\begin{array}{ccc} 1-\lambda & 3 & 3 \\ -2-\lambda & -2-\lambda & 0
\\ 0 & -2-\lambda & -2-\lambda \end{array}\right]\right) \\
&=(1-\lambda)\det \left[\begin{array}{cc} -2-\lambda & 0 \\ -2-\lambda & -2-\lambda \end{array}\right]+(2+\lambda)\underbrace{\det \left[\begin{array}{cc} 3 & 3 \\ -2-\lambda & -2-\lambda \end{array}\right]}_{0}\\
&=(1-\lambda)(2+\lambda)^2,
\end{align*}
so we have two distinct eigenvalues $1$ and $-2$. Since $\lambda=1$ has algebraic multiplicity $1$, its geometric multiplicity is also equal to $1$. Note, however, that $\lambda=-2$ has algebraic multiplicity $2$, so we need to check the dimension of $\Nul (A+2I_3)$. Indeed, $A+2I_3=\left[\begin{array}{rrr} 3 & 3 & 3 \\
-3 & -3 & -3
\\ 3 & 3 & 3 \end{array}\right]$ reduces to $\left[\begin{array}{rrr} 1 & 1 & 1 \\
0 & 0 & 0
\\ 0 & 0 & 0 \end{array}\right]$ and it follows that $\Nul (A+2I_3)=\Span \left\{\left[\begin{array}{r} -1 \\ 1 \\ 0 \end{array}\right],\left[\begin{array}{r} -1 \\ 0 \\ 1 \end{array}\right] \right\}$ and that $\dim \Nul (A+2I_3)=2$. This shows that $A$ is diagonalizable.
An eigenvector $\bm{\xi}_1$ corresponding to $\lambda=1$ is obtained by finding a nonzero vector in the null space of $A-I_3=\left[\begin{array}{rrr} 0 & 3 & 3 \\
-3 & -6 & -3
\\ 3 & 3 & 0
\end{array}\right]$. Since $\Nul (A-I_3)=\Span \left\{\left[\begin{array}{r} 1 \\ -1 \\ 1 \end{array}\right]\right\}$, one can simply take $\bm{\xi}_1=\left[\begin{array}{r} 1 \\ -1 \\ 1 \end{array}\right]$. For eigenvectors corresponding to $\lambda=-2$, we can choose $\bm{\xi}_{2,1}=\left[\begin{array}{r} -1 \\ 1 \\ 0 \end{array}\right]$ and $\bm{\xi}_{2,2}=\left[\begin{array}{r} -1 \\ 0 \\ 1 \end{array}\right]$. Finally, with $P=\left[\begin{array}{rrr} 1 & -1 & -1 \\ -1 & 1 & 0 \\ 1 & 0 & 1 \end{array}\right]$ and $D=\left[\begin{array}{rrr} 1 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & -2 \end{array}\right]$, we get $P^{-1}AP=D$.
\end{answer}

\begin{example} Not all matrices are diagonalizable. We show that $B=\left[\begin{array}{rr} 1 & 1 \\
0 & 1
\end{array}\right]$ is \textit{not} diagonalizable. Suppose for a contradiction that $B$ is diagonalizable. Since the characteristic equation of $B$ is $(1-\lambda)^2=0$, $1$ is the only eigenvalue of $B$, so it follows by the Diagonalization Theorem, Part 1, that $B$ is similar to the identity matrix $I_2$, which would imply that $B=I_2$.
\end{example}

\begin{example}[Application to the Fibonacci sequence] Consider the
sequence defined by $$a_1=1,\quad a_2=1,\quad
a_{n}=a_{n-1}+a_{n-2}\text{ for }n\geq 3.$$ We are interested in the
closed form of $a_n$. Note that for $n\geq 3$
$$\left[\begin{array}{c} a_{n} \\
a_{n-1} \end{array}\right]=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]\left[\begin{array}{c} a_{n-1} \\
a_{n-2} \end{array}\right].$$ Similarly, if $n$ is large enough,
then
$$\left[\begin{array}{c} a_{n-1} \\
a_{n-2} \end{array}\right]=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]\left[\begin{array}{c} a_{n-2} \\
a_{n-3} \end{array}\right]\quad\text{and}\quad \left[\begin{array}{c} a_{n-2} \\
a_{n-3} \end{array}\right]=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]\left[\begin{array}{c} a_{n-3} \\
a_{n-4} \end{array}\right]$$ so
$$\left[\begin{array}{c} a_{n} \\
a_{n-1} \end{array}\right]=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]^2\left[\begin{array}{c} a_{n-2} \\
a_{n-3} \end{array}\right]=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]^3\left[\begin{array}{c} a_{n-3} \\
a_{n-4} \end{array}\right]=\cdots=\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]^{n-2}\left[\begin{array}{c} a_2 \\
a_1 \end{array}\right].$$ Consider $\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]^{n-2}$. The characteristic polynomial of $\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]$ is
$\lambda^2-\lambda-1$ and eigenvalues are $\frac{1\pm \sqrt{5}}{2}$. Thus the matrix $\left[\begin{array}{cc} 1 & 1  \\
1 & 0 \end{array}\right]$ is diagonalizable.
One eigenvector corresponding to $\lambda_1=\frac{1+\sqrt{5}}{2}$ is
$\left[\begin{array}{c} 1 \\
-\frac{1-\sqrt{5}}{2} \end{array}\right]$. Similarly, $\left[\begin{array}{c} 1 \\
-\frac{1+\sqrt{5}}{2} \end{array}\right]$
 is an eigenvector corresponding to
 $\lambda_2=\frac{1-\sqrt{5}}{2}$. Let $P=\left[\begin{array}{cc} 1 &	1  \\
-\frac{1-\sqrt{5}}{2} & -\frac{1+\sqrt{5}}{2} \end{array}\right]$ then  $P^{-1}\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]P=\left[\begin{array}{cc} \lambda_1 & 0  \\
0 & \lambda_2 \end{array}\right]$. Finally, $\left[\begin{array}{rr} 1 & 1  \\
1 & 0 \end{array}\right]^{n-2}=P\left[\begin{array}{cc} \lambda_1 & 0  \\
0 & \lambda_2 \end{array}\right]^{n-2}P^{-1}=\frac{1}{\sqrt{5}}\left[\begin{array}{cc} \lambda_1^{n-1}-\lambda_2^{n-1} & \lambda_1^{n-2}-\lambda_2^{n-2}  \\
\lambda_1^{n-2}-\lambda_2^{n-2} & \lambda_1^{n-3}-\lambda_2^{n-3}
\end{array}\right]$ and hence
$$a_n=\frac{1}{\sqrt{5}}(\lambda_1^{n-1}-\lambda_2^{n-1} +
\lambda_1^{n-2}-\lambda_2^{n-2})=\frac{1}{\sqrt{5}}\left[\left(\frac{1+\sqrt{5}}{2}\right)^n-\left(\frac{1-\sqrt{5}}{2}\right)^n\right]$$
after calculation.
\end{example}

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $A=\left[\begin{array}{rr} -2 & 12 \\ -1 & 5 \end{array}\right]$.
\begin{enumerate}
\item Find eigenvalues and corresponding eigenvectors of $A$.
\item Find a diagonal matrix $D$ and an invertible matrix $P$ such that $A=PDP^{-1}$.
\item Compute $A^{100}$.
\end{enumerate}
\item Let $A$ be an $n\times n$ square matrix with $n$ distinct positive eigenvalues. Show that there is an $n\times n$ matrix $B$ such that $B^2=A$. \\\textit{Hint}: If $A$ is a diagonal matrix, then $B$ can be easily obtained. Use diagonalization for general case.
\item Let $A$ be an $n\times n$ diagonalizable matrix. Show that there is an $n\times n$ matrix $B$ such that $A=B^3$.
\end{enumerate}
\end{exercise}

\section{Symmetric Matrices and Quadratic Forms}
For a random vector $\vX=\left[\begin{array}{c} X_1 \\ X_2 \\ \vdots \\ X_n \end{array}\right]$, its variance-covariance matrix $\Sigma$ is defined to be an $n\times n$ matrix whose $(i,j)$-entry $\Sigma_{ij}$ is given by the covariance between $X_i$ and $X_j$. It is known that the variance-covariance matrix of a random vector has many desirable properties, including positive definiteness. In this section, we study properties of symmetric and positive definite matrices and their roles in geometric interpretation of multidimensional data.

\begin{defi}\label{symmortho} Let $A$ be a square matrix.
\begin{enumerate}
\item $A$ is said to be \underline{symmetric}\index{matrix!symmetric} if $A=A^t$.
\item $A$ is said to be \underline{orthogonal}\index{matrix!orthogonal} if $A^t=A^{-1}$.
\end{enumerate}
\end{defi}

\begin{thm}\label{all real}
If $A$ is a symmetric matrix, then all eigenvalues of $A$ are real.
\end{thm}

\begin{thm}\label{orthogonal diag} If $A$ is a symmetric matrix, then there exists an orthogonal matrix $U$ such that $U^tAU=D$, where $D$ is a diagonal matrix. Moreover, $A=\sum_{i=1}^n \lambda_i \vu_i\vu_i^t$, where $\vu_i$ is the $i^{\text{th}}$ column of $U$ and $\lambda_i$ is the $i^{\text{th}}$ diagonal element of $D$.
\end{thm}

\begin{remark}
Let $A$ be an $n\times n$ symmetric matrix. By Theorem \ref{all real}, we can list all eigenvalues of $A$ in a decreasing order, say, $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_n$. By Theorem \ref{orthogonal diag}, one can write $A=UDU^t$, and this is called a \underline{spectral decomposition}\index{spectral decomposition} of $A$.
\end{remark}

\begin{defi} A \underline{quadratic form}\index{quadratic form} $Q$ is a map from $\mathbb{R}^n$ to $\mathbb{R}$ of the form $Q(\vx)=\vx^t A \vx$, where $A$ is a $n\times n$ matrix and $\vx\in \mathbb{R}^n$. A quadratic form $Q(\vx)=\vx^t A\vx$, or the matrix $A$, is said to be \underline{positive definite}\index{positive definite} (respectively, \underline{positive semidefinite}\index{positive semidefinite}) if $\vx^t A\vx>0$ (respectively, $\vx^t A\vx\geq 0$) for all $\vx\neq \veczero$. Similarly, a quadratic form $Q(\vx)=\vx^t A\vx$, or the matrix $A$, is said to be \underline{negative definite}\index{negative definite} (respectively, \underline{negative semidefinite}\index{negative semidefinite}) if $\vx^t A\vx<0$ (respectively, $\vx^t A\vx\leq 0$) for all $\vx\neq \veczero$.
\end{defi}

\begin{remark}
For a quadratic form $Q(\vx)=\vx^t A \vx$, we may assume that $A$ is symmetric. See Exercise \ref{singular exercises}.\ref{symmqf}. So from now on, when it comes to a quadratic form $Q$ associated with a matrix $A$, we always assume that $A$ is symmetric.
\end{remark}

\begin{thm} \quad \label{posdef charact}
\begin{enumerate}
\item $A$ is positive definite (respectively, positive semidefinite) if and only if $-A$ is negative definite (respectively, negative semidefinite).
\item \label{semi square} $A$ is positive semidefinite if and only if there exists an $M$ such that $A=M^tM$.
\item \label{semi becomes pos} $A$ is positive definite if and only if $A$ is positive semidefinite and $\det A>0$.
\end{enumerate}
\end{thm}

$M$ in Theorem \ref{posdef charact}.\ref{semi square} can be assumed to be a lower triangular matrix.

\begin{thm}[Cholesky Decomposition]
Let $A$ be a positive semidefinite matrix. Then there is a lower triangular matrix $L$ such that $A=LL^t$. In particular, if $A$ is positive definite, then all diagonal entries in $L$ are positive.
\end{thm}

\begin{problem}\label{pd easy criterion}
Let $A$ be a symmetric matrix. Show that $A$ is positive definite (respectively, positive semidefinite) if and only if all eigenvalues of $A$ are positive (respectively, nonnegative).
\end{problem}

\begin{answer}
Let $\lambda_1,\ldots,\lambda_n$ be eigenvalues of $A$. Suppose that $\lambda_i>0$ for all $i$, $1\leq i \leq n$. By Theorem \ref{orthogonal diag}, $A$ has a spectral decomposition, that is, there exists an orthogonal matrix $U$ such that $A=UDU^t$, where $D$ is a diagonal matrix whose diagonal entries are $\lambda_1,\ldots,\lambda_n$. Let $R$ be a diagonal matrix with diagonal entries $\sqrt{\lambda_1},\ldots,\sqrt{\lambda_n}$, then $D=R^2=RR^t$ and it follows that $A=(UR)(UR)^t$. Since $\det A>0$ (see Exercise \ref{char eq exer}.\ref{prod equals det}), by Theorem \ref{posdef charact}, $A$ is positive definite. See Exercise \ref{singular exercises}.\ref{definite eigen} for the other implication.
\end{answer}

\begin{example}[Classical Multidimensional Scaling]\index{Classical Multidimensional Scaling}
Let $\bm{X}_i$ ($1\leq i \leq N$), $\bar{\bm{X}}$, $X$, and $B$ be as in Remark \ref{trace variance}. Let $d_{ij}$ denote the distance between $\bm{X}_i$ and $\bm{X}_j$, then it is obvious that $d_{ij}$ is obtained from $X$. For example, if one defines an $n\times n$ matrix $S=[s_{ij}]_{i,j=1}^n$ by $S=B^tB$, then
\begin{equation}\label{dij sij}
\begin{aligned}d_{ij}^2&=\|\bm{X}_i-\bm{X}_j\|^2\\
&=\|\bm{X}_i-\bar{\bm{X}}-(\bm{X}_j-\bar{\bm{X}})\|^2\\
&=(\red{\bm{X}_i-\bar{\bm{X}}}-\blue{(\bm{X}_j-\bar{\bm{X}})})\cdot (\red{\bm{X}_i-\bar{\bm{X}}}-\blue{(\bm{X}_j-\bar{\bm{X}})})\\
&=\red{\|\bm{X}_i-\bar{\bm{X}}\|^2}+\blue{\|\bm{X}_j-\bar{\bm{X}}\|^2}-2\red{(\bm{X}_i-\bar{\bm{X}})}\cdot \blue{(\bm{X}_j-\bar{\bm{X}})}\\
&=\red{s_{ii}}+\blue{s_{jj}}-2s_{ij}.
\end{aligned}
\end{equation}
Conversely, consider a problem of obtaining a $p\times N$ matrix $X$ from the distance information $\{d_{ij}:1\leq i,j\leq N\}$. First we note that $X$ is not uniquely determined, since distance remains invariant under, for example, identical translations of $\bm{X}_i$'s,  so we assume that $\bar{\bm{X}}=\bm{0}$. Letting $S=[s_{ij}]_{i,j=1}^n=X^tX$, we get
$$\sum_{i=1}^N s_{ij}=\sum_{i=1}^n \langle \bm{X}_i, \bm{X}_j \rangle = \langle N\bar{\bm{X}},\bm{X}_j \rangle=0$$
and similarly
$$\sum_{j=1}^N s_{ij}=\sum_{j=1}^n \langle \bm{X}_i, \bm{X}_j \rangle = \langle \bm{X}_i, N\bar{\bm{X}} \rangle=0.$$
It follows from (\ref{dij sij}) that
$$\sum_{j=1}^N d_{ij}^2=Ns_{ii}+tr(S), \quad\sum_{i=1}^N d_{ij}^2=tr(S)+Ns_{jj},\quad \sum_{i,j=1}^N d_{ij}^2=2Ntr(S)$$
from which we obtain
\begin{equation}\label{s in d}
\begin{aligned}
tr(S)&=\frac{1}{2N}\sum_{i,j=1}^N d_{ij}^2,\\
s_{ii}&=\frac{1}{N}\left(\sum_{j=1}^N d_{ij}^2-tr(S)\right)=\frac{1}{N}\left(\sum_{j=1}^N d_{ij}^2-\frac{1}{2N}\sum_{i,j=1}^N d_{ij}^2\right), \text{and} \\
s_{jj}&=\frac{1}{N}\left(\sum_{i=1}^N d_{ij}^2-tr(S)\right)=\frac{1}{N}\left(\sum_{i=1}^N d_{ij}^2-\frac{1}{2N}\sum_{i,j=1}^N d_{ij}^2\right).
\end{aligned}
\end{equation}
From (\ref{dij sij}) and (\ref{s in d}), it follows that
\begin{align*}s_{ij}&=-\frac{1}{2}\left(d_{ij}^2-s_{ii}-s_{jj}\right)\\
&=-\frac{1}{2}\left(d_{ij}^2-\frac{1}{N}\sum_{j=1}^N d_{ij}^2-\frac{1}{N}\sum_{i=1}^N d_{ij}^2+\frac{1}{N^2}\sum_{i,j=1}^N d_{ij}^2\right),
\end{align*}
which means that $S=X^tX$ is obtained using $d_{ij}$'s. To recover a $p\times N$ matrix $X$ from $S$, we write $S=UDU^t$, where
$U=\left[\begin{array}{cccc} | & | &   & | \\ \vu_1 & \vu_2 & \cdots & \vu_N \\ | & | &  & | \end{array}\right]$ is an $N\times N$ orthogonal matrix and $D=\left[\begin{array}{cccc} \lambda_1 & & &  \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_N\end{array}\right]$. Note that $S$ is positive semidefinite and $\lambda_i\geq 0$ for $1\leq i \leq N$. Since $\rank(D)=\rank(S)=\rank(X)$ is at most $p$, it follows that $\lambda_{p+1}=\lambda_{p+2}=\cdots=\lambda_{N}=0$ and that $S=U_0D_0U_0^t$, where $U_0=\left[\begin{array}{cccc} | & | &   & | \\ \vu_1 & \vu_2 & \cdots & \vu_p \\ | & | &  & | \end{array}\right]$ and $D_0=\left[\begin{array}{cccc} \lambda_1 & & &  \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_p\end{array}\right]$. Finally, one can take $X=D_0^{1/2}U_0^t$, where $D_0^{1/2}=\left[\begin{array}{cccc} \sqrt{\lambda_1} & & &  \\ & \sqrt{\lambda_2} & & \\ & & \ddots & \\ & & & \sqrt{\lambda_p} \end{array}\right]$.
\end{example}

The following is another criterion for positive definiteness. See Exercise \ref{singular exercises}.\ref{2by2posdefdet} for a proof of a special case.

\begin{thm}[Sylvester's Criterion]
Let $A$ be an $n\times n$ symmetric matrix. For $1\leq k \leq n$, let $\Delta_k$ denote the determinant of the upper left $k\times k$ submatrix of $A$. Then
\begin{enumerate}
\item $A$ is positive definite if and only if $\Delta_k>0$ for all $k$, $1\leq k \leq n$.
\item $A$ is negative definite if and only if $\Delta_k<0$ for all odd $k$ and $\Delta_k>0$ for all even $k$.
\end{enumerate}
\end{thm}

In many applications, we need to maximize a quadratic form under certain conditions. We start with the next theorem.

\begin{thm}\label{quad opt}
Let $A$ be a symmetric matrix with eigenvalues $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_n$. Let $M=\max\{\vx^t A\vx: \|\vx\|=1\}$ and $m=\min\{\vx^t A\vx: \|\vx\|=1\}$, then $M=\vxi_1^t A \vxi_1$ and $m=\vxi_n^t A\vxi_n$, where $\vxi_i$ is a unit eigenvector of $A$ corresponding to $\lambda_i$.
\end{thm}

\begin{thm}
Let $A$, $\lambda_i$, and $\vxi_i$ be as in Theorem \ref{quad opt}. Then the maximum value of $$\{\vx^t A\vx: \|\vx\|=1, \langle \vx,\vxi_1 \rangle=0\}$$ is the second largest eigenvalue, $\lambda_2$, and this is obtained when $\vx=\vxi_2$. In general, for $2\leq i \leq n$, the maximum value of $$\{\vx^t A\vx: \|\vx\|=1, \langle \vx,\vxi_1 \rangle=\langle \vx,\vxi_2 \rangle=\cdots =\langle \vx,\vxi_{i-1} \rangle=0\}$$ is $\lambda_i$, and this is obtained when $\vx=\vxi_i$.
\end{thm}

\begin{example}\label{ellipse rotation graph} Let $A$ be a $2\times 2$ positive definite matrix, $\vmu=\left[\begin{array}{c} \mu_1 \\ \mu_2 \end{array}\right]\in \mathbb{R}^2$, and $c>0$. Consider the problem of determining the trace of points $\vx=\left[\begin{array}{c} x_1\\ x_2 \end{array}\right]\in \mathbb{R}^2$ such that $(\vx-\vmu)^tA(\vx-\vmu)=c$. First, we assume that $\vmu=\veczero$. If $A=\left[\begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array}\right]$ is a diagonal matrix with $\lambda_1\geq\lambda_2>0$, then the equation $\vx^tA\vx=c$ becomes $\lambda_1x_1^2+\lambda_2x_2^2=c$ and the trace turns out to be an ellipse centered at the origin with half length of minor axis equal to $\frac{\sqrt{c}}{\sqrt{\lambda_1}}$ and half length of major axis equal to $\frac{\sqrt{c}}{\sqrt{\lambda_2}}$ (see Figure \ref{positive ellipse} (a)).

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1.2, yscale=1.2, >=triangle 45]

\def\lamone{4} ;
\def\lamtwo{1} ;
\def\cc{2} ;
\draw [->] (-3,0) -- (3,0) ;
\draw [->] (0,-3) -- (0,3) ;

\node [above right] at (3,0) {$x_1$} ; 
\node [above right] at (0,3) {$x_2$} ; 
\node [below left] at (0,0) {$O$} ;
\node [below] at (0,-3) {(a)} ;
\node [above] at ({\cc/sqrt(\lamone)/2},0) {$\frac{\sqrt{c}}{\sqrt{\lambda_1}}$} ;
\node [left] at (0,{\cc/sqrt(\lamtwo)/2}) {$\frac{\sqrt{c}}{\sqrt{\lambda_2}}$} ;
\draw [red, thick] (0,0) -- ({\cc/sqrt(\lamone)},0) ;
\draw [red, thick] (0,0) -- (0,{\cc/sqrt(\lamtwo)}) ;

\draw [blue, thick,  domain=0:360, samples=60] 
 plot ({\cc*cos(\x)/sqrt(\lamone)}, {\cc*sin(\x)/sqrt(\lamtwo)} );

\begin{scope}[xshift=7cm]
\def\lamone{4} ;
\def\lamtwo{1} ;
\def\cc{2} ;
\draw [->,dashed] (-3,0) -- (3,0) ;
\draw [->,dashed] (0,-3) -- (0,3) ;
\draw [->] ({-3*sqrt(3)/2},-3/2) -- ({3*sqrt(3)/2},3/2) ;
\draw [->] (3/2,{-3*sqrt(3)/2}) -- (-3/2,{3*sqrt(3)/2}) ;

\node [above] at ({3*sqrt(3)/2},3/2) {$\ve_1$} ; 
\node [above] at (-3/2,{3*sqrt(3)/2}) {$\ve_2$} ; 
\node [above right] at (3,0) {$x_1$} ; 
\node [above right] at (0,3) {$x_2$} ; 
\node [below left] at (0,-0.1) {$O$} ;
\node [below] at (0,-3) {(b)} ;

\draw [red, thick] (0,0) -- ({sqrt(3)/2*\cc*cos(0)/sqrt(\lamone)-\cc/2*sin(0)/sqrt(\lamtwo)}, {\cc/2*cos(0)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(0)/sqrt(\lamtwo)} ) ; 
\draw [red, thick] (0,0) -- ({sqrt(3)/2*\cc*cos(90)/sqrt(\lamone)-\cc/2*sin(90)/sqrt(\lamtwo)}, {\cc/2*cos(90)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(90)/sqrt(\lamtwo)} ) ; 

\node [above] at ({-0.1+1/2*sqrt(3)/2*\cc*cos(0)/sqrt(\lamone)-1/2*\cc/2*sin(0)/sqrt(\lamtwo)}, {1/2*\cc/2*cos(0)/sqrt(\lamone)+1/2*sqrt(3)/2*\cc*sin(0)/sqrt(\lamtwo)} ) {$\frac{\sqrt{c}}{\sqrt{\lambda_1}}$} ;
\node [left] at (0.1,{0.1+\cc/sqrt(\lamtwo)/2}) {$\frac{\sqrt{c}}{\sqrt{\lambda_2}}$} ;

\draw [blue, thick,  domain=0:360, samples=60] 
 plot ({sqrt(3)/2*\cc*cos(\x)/sqrt(\lamone)-\cc/2*sin(\x)/sqrt(\lamtwo)}, {\cc/2*cos(\x)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(\x)/sqrt(\lamtwo)} );
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Positive definite matrix and ellipse}
\label{positive ellipse}
\end{figure}

In general, let $A=UDU^t$ be a spectral decomposition of $A$ with $D=\left[\begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array}\right]$, $\lambda_1\geq \lambda_2>0$, and $U=\left[\begin{array}{cc} | & | \\ \ve_1 & \ve_2  \\ | & |  \end{array}\right]$, where $\ve_i$ is a unit eigenvector corresponding to $\lambda_i$ for $i=1,2$. Let $\vy=U^t\vx$. Since $\ve_1$ and $\ve_2$ are orthogonal to each other,  $U^t\ve_1=\left[\begin{array}{c} 1 \\ 0  \end{array}\right]$ and $U^t\ve_2=\left[\begin{array}{c} 0 \\ 1  \end{array}\right]$, so $\vy$ can be viewed as a new coordinate system that transforms $\ve_1$ and $\ve_2$ to $(1,0)$ and $(0,1)$, respectively. Since the equation $\vx^tA\vx=c$ becomes $\vy^tD\vy=c$, one obtains a standard ellipse in this new coordinate system (see Figure \ref{positive ellipse} (b)). Finally, for general $\vmu$, the trace is given by simple translation (see Figure \ref{positive ellipse general}).

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1.2, yscale=1.2, >=triangle 45]

\def\lamone{4} ;
\def\lamtwo{1} ;
\def\cc{2} ;
\draw [->,dashed] (-1,0) -- (6,0) ;
\draw [->,dashed] (0,-1) -- (0,5) ;
\node [above right] at (6,0) {$x_1$} ; 
\node [above right] at (0,5) {$x_2$} ; 
\node [below left] at (0,0) {$O$} ;

\begin{scope}[xshift=3cm, yshift=2cm]
\node [right] at (0,0) {\scriptsize{$(\mu_1,\mu_2)$}} ;

\draw [->] ({-3*sqrt(3)/2},-3/2) -- ({3*sqrt(3)/2},3/2) ;
\draw [->] (3/2,{-3*sqrt(3)/2}) -- (-3/2,{3*sqrt(3)/2}) ;

\node [above] at ({3*sqrt(3)/2},3/2) {$\ve_1$} ; 
\node [above] at (-3/2,{3*sqrt(3)/2}) {$\ve_2$} ; 

\draw [red, thick] (0,0) -- ({sqrt(3)/2*\cc*cos(0)/sqrt(\lamone)-\cc/2*sin(0)/sqrt(\lamtwo)}, {\cc/2*cos(0)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(0)/sqrt(\lamtwo)} ) ; 
\draw [red, thick] (0,0) -- ({sqrt(3)/2*\cc*cos(90)/sqrt(\lamone)-\cc/2*sin(90)/sqrt(\lamtwo)}, {\cc/2*cos(90)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(90)/sqrt(\lamtwo)} ) ; 
\draw [fill] (0,0) circle (2pt) ;
\draw [dashed] (0,0) -- (0,-2) ;
\draw [dashed] (0,0) -- (-3,0) ;
\node [below] at (0,-2) {$\mu_1$} ; 
\node [left] at (-3,0) {$\mu_2$} ; 
\node [right, blue] at (1.3,-1) {\scriptsize{$(\vx-\vmu)^tA(\vx-\vmu)=c$}} ; 

\node [above] at ({-0.1+1/2*sqrt(3)/2*\cc*cos(0)/sqrt(\lamone)-1/2*\cc/2*sin(0)/sqrt(\lamtwo)}, {1/2*\cc/2*cos(0)/sqrt(\lamone)+1/2*sqrt(3)/2*\cc*sin(0)/sqrt(\lamtwo)} ) {$\frac{\sqrt{c}}{\sqrt{\lambda_1}}$} ;
\node [left] at (0.1,{0.1+\cc/sqrt(\lamtwo)/2}) {$\frac{\sqrt{c}}{\sqrt{\lambda_2}}$} ;

\draw [blue, thick,  domain=0:360, samples=60] 
 plot ({sqrt(3)/2*\cc*cos(\x)/sqrt(\lamone)-\cc/2*sin(\x)/sqrt(\lamtwo)}, {\cc/2*cos(\x)/sqrt(\lamone)+sqrt(3)/2*\cc*sin(\x)/sqrt(\lamtwo)} );
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Ellipse described by the equation $(\vx-\vmu)^tA(\vx-\vmu)=c$. $\lambda_1>\lambda_2$ are eigenvalues of a positive definite matrix $A$ and $\ve_1$, $\ve_2$ are eigenvectors corresponding to $\lambda_1$, $\lambda_2$, respectively.}
\label{positive ellipse general}
\end{figure}
\end{example}

\begin{remark}
Suppose that $X_1,X_2,\ldots,X_n$ denote a random sample from a multivariate normal population. When making an inference on the population mean, $\vmu_0$, the statistic
$$T^2=(\bar{\vX}-\vmu_0)^t\left(\frac{\vS}{n}\right)^{-1}(\bar{\vX}-\vmu_0)=n(\bar{\vX}-\vmu_0)^t \vS^{-1}(\bar{\vX}-\vmu_0),$$
called \underline{Hotelling's $T^2$},\index{Hotelling's $T^2$} plays a crucial role. The arguments used in Example \ref{ellipse rotation graph}, for example, can be used to determine the confidence region of $\vmu_0$ in bivariate normal case. 
\end{remark}

\begin{thm}[Singular Value Decomposition]\label{SVD} Let $A$ be an $m\times n$ matrix with $m\leq n$. Then there are an $m\times m$ orthogonal matrix $U$, an $n\times n$ orthogonal matrix $V$, and an $m\times n$ rectangular diagonal matrix $\Sigma$ such that $A=U\Sigma V^T$ and $\Sigma$ is of the form
$$\Sigma=\left[\begin{array}{ccccccc} \sigma_1 & & & &&& \\ & \sigma_2 & &&&& \\ & & \ddots &&&& \\ & & & \sigma_m&&& \end{array}\right]$$ with
$$\sigma_1\geq \sigma_2\geq \cdots \geq \sigma_m\geq 0.$$
\end{thm}

\begin{remark}\quad
\begin{enumerate}
\item The factorization in Theorem \ref{SVD} is called a \underline{singular value decomposition}\index{singular value decomposition} of $A$.
\item The numbers $\sigma_1,\ldots,\sigma_m$, called the \underline{singular values}\index{singular values} of $A$, are square roots of eigenvalues of $AA^t$ (see Exercise \ref{evev exer}.\ref{evpositive}) and hence uniquely determined.
\item The columns of $U$ are eigenvectors of $AA^t$ and the columns of $V$ are eigenvectors of $A^tA$.
\item The rank of $A$ is $r$ if and only if $\sigma_1\geq \cdots \geq \sigma_r>\sigma_{r+1}=0$.
\end{enumerate}
\end{remark}

The next theorem states how to approximate a given matrix with a matrix with a smaller rank. Recall the definitions of the Hilbert-Schmidt norm and the operator norm of a matrix (see Remark \ref{norms}).

\begin{thm}[Eckart-Young Theorem] \label{EckartYoung}
Let $m\leq n$. Let $A$ be an $m\times n$ matrix with a singular value decomposition $A=U\Sigma V^t$ and $\rank(A)=r$. Let $k< r$ and $A_k=U\Sigma_k V^t$, where $\Sigma_k$ is a truncation of $\Sigma$ defined by
$$\Sigma_{k}=\left[\begin{array}{cccccccccc} \sigma_1 & & & && &&&& \\ & \sigma_2 & &&&&&&& \\ & & \ddots &&&&&& \\ & & & \sigma_k&&&&&& \\ &&&& 0 &&&&& \\ &&&&&\ddots &&&& \\ &&&&&& 0 &&& \end{array}\right].$$
Then
$$\min\{\|A-B\|_{HS}:  \rank (B) \leq k\}=\|A-A_k\|_{HS}=\sqrt{\sigma_{k+1}^2+\sigma_{k+2}^2+\cdots+\sigma_{r}^2}$$
and
$$\min\{\|A-B\|_{op}: \rank (B) \leq k \}=\|A-A_k\|_{op}=\sigma_{k+1}.$$
\end{thm}

\begin{example}[Principal Component Analysis]\index{Principal Component Analysis}\label{example pca}
For $1\leq i \leq N$, let $\bm{X}_i$ be a multivariate data point in $\mathbb{R}^p$ and let $X=\left[\begin{array}{cccc} | & | &   & | \\ \bm{X}_1 & \bm{X}_2 & \cdots & \bm{X}_N \\ | & | &  & | \end{array}\right]$, as in Remark \ref{trace variance}. For simplicity, we assume that the mean $\displaystyle{\bar{\bm{X}}=\frac{1}{N}\sum_{i=1}^N \bm{X}_i=\bm{0}}$ -- otherwise, one can simply replace $\bm{X}_i$ by $\bm{X}_i-\bar{\bm{X}}$. Let $k<p$. The goal of Principal Component Analysis is to find a $k$-dimensional subspace $W_0$ of $\mathbb{R}^p$ and the corresponding projection matrix $P_{W_0}$ so that $\{P_{W_0}\bm{X}_1,\ldots,P_{W_0}\bm{X}_N\}$ preserves as much information of the distribution of $\{\bm{X}_1,\ldots,\bm{X}_N\}$ as possible, by which we mean that the total variance (see Remark \ref{trace variance}) of $\{P_{W_0}\bm{X}_1,\ldots,P_{W_0}\bm{X}_N\}$ is kept as large as possible (see Figure \ref{pca figure}). Let $W$ be a $k$-dimensional subspace of $\mathbb{R}^p$. Define $\bm{Y}_i=P_W\bm{X}_i$ and $Y=\left[\begin{array}{cccc} | & | &   & | \\ \bm{Y}_1 & \bm{Y}_2 & \cdots & \bm{Y}_N \\ | & | &  & | \end{array}\right]$, then it follows that $Y=P_WX$. By Problem \ref{proj matrix prop},
$$\|Y\|_{HS}^2=tr(\red{Y^t}\blue{Y})=tr(\red{X^tP_W^t}\blue{P_WX})=tr(X^tP_WX)=tr(X^tY)$$
and
\begin{equation}\label{trace ortho}
\begin{aligned}
\|X-Y\|_{HS}^2&=tr((X-Y)^t(X-Y))\\
&=tr(X^tX)-tr(X^tY)-tr(Y^tX)+tr(Y^tY)\\
(\text{Exercise }\ref{basics of matrix exer}.\ref{tracedef})&=tr(X^tX)-tr(X^tY)-tr((Y^tX)^t)+tr(Y^tY)\\
&=tr(X^tX)-2tr(X^tY)+tr(Y^tY)\\
&=\|X\|_{HS}^2-\|Y\|_{HS}^2.
\end{aligned}
\end{equation}
Since $$\bar{\bm{Y}}=\frac{1}{N}\sum_{i=1}^N \bm{Y}_i=\frac{1}{N}\sum_{i=1}^N P_W\bm{X}_i=P_W\bar{\bm{X}}=\bm{0},$$ by (\ref{trace ortho}), the total variance of $\{\bm{Y}_1,\ldots,\bm{Y}_N\}$ is given by
\begin{equation}\label{total relation}\frac{1}{N-1}\|Y\|^2_{HS}=\frac{1}{N-1}\left(\|X\|^2_{HS}-\|X-Y\|^2_{HS}\right)\end{equation} and is maximized when $Y$ is chosen so that $\|X-Y\|_{HS}$ is minimized. To this end, let $X=U\Sigma V^t$ be a singular value decomposition of $X$ as in Theorem \ref{EckartYoung}, and let $P=UQ U^t$, where
$$Q=\left[\begin{array}{ccccccc} 1 &&&&&&  \\ & 1 &&&&& \\ && \ddots &&&& \\ &&& 1 &&& \\ &&&& 0 && \\ &&&&& \ddots & \\ &&&&&& 0 \end{array}\right]$$
is a $p\times p$ diagonal matrix whose only nonzero elements are $1$'s placed at the first $k$ diagonal entries. Note that $P^2=P=P^t$. By Exercise \ref{ortho exer}.\ref{ptp2p}, if we define $W_0=\{P\vx:\vx\in \mathbb{R}^p\}$, then it follows that $P_{W_0}=P$, $\dim W_0=\rank(P_{W_0})=k$, and $P_{W_0}X=U\Sigma_kV^t$, where $\Sigma_k$ is as in Theorem \ref{EckartYoung}. By Eckart-Young Theorem, the largest possible total variance is obtained by $\{P_{W_0}\bm{X}_1,\ldots,P_{W_0}\bm{X}_N\}$.
\end{example}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.85, yscale=0.85, >=triangle 45]

\draw [dashed] (-3,0) -- (3,0) ;
\draw [dashed] (0,-3) -- (0,3) ;

\draw [fill] (1,2) circle (2pt) ;
\draw [fill] (-1,-2) circle (2pt) ;
\draw [fill] (-0.5,1) circle (2pt) ;
\draw [fill] (0.5,-1) circle (2pt) ;
\draw [fill] (-2.5,-1) circle (2pt) ;
\draw [fill] (1.5,1) circle (2pt) ;
\draw [fill] (1,0) circle (2pt) ;

\node [above] at (1,2) {$\bm{X}_i$} ;

\begin{scope}[xshift=7.5cm]
\def\aa{3}
\def\bb{1}
%\def\aa{1.379517}
%\def\bb{1.690254}

\draw [blue, very thick] plot [domain=-3:3] (\x, {\bb/\aa*\x}) ;
\node [above] at (3, {\bb/\aa*3}) {$W_1$} ;

\draw [dashed] (-3,0) -- (3,0) ;
\draw [dashed] (0,-3) -- (0,3) ;

\draw [dashed, red] (1,2) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-1,-2) --  (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-0.5,1) --  (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (0.5,-1) --  (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-2.5,-1) --  (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1.5,1) --  (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1,0) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [fill] (1,2) circle (2pt) ;
\draw [fill] (-1,-2) circle (2pt) ;
\draw [fill] (-0.5,1) circle (2pt) ;
\draw [fill] (0.5,-1) circle (2pt) ;
\draw [fill] (-2.5,-1) circle (2pt) ;
\draw [fill] (1.5,1) circle (2pt) ;
\draw [fill] (1,0) circle (2pt) ;

\node [above] at (1,2) {$\bm{X}_i$} ;

\node [red, below right] at (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) {$P_{W_1}\bm{X}_i$} ;

\end{scope}

\begin{scope}[yshift=-7.5cm]
\def\aa{3}
\def\bb{-1}

\draw [blue, very thick] plot [domain=-3:3] (\x, {\bb/\aa*\x}) ;
\node [above] at (3, {\bb/\aa*3}) {$W_2$} ;

\draw [dashed] (-3,0) -- (3,0) ;
\draw [dashed] (0,-3) -- (0,3) ;

\draw [dashed, red] (1,2) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-1,-2) --  (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-0.5,1) --  (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (0.5,-1) --  (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-2.5,-1) --  (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1.5,1) --  (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1,0) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [fill] (1,2) circle (2pt) ;
\draw [fill] (-1,-2) circle (2pt) ;
\draw [fill] (-0.5,1) circle (2pt) ;
\draw [fill] (0.5,-1) circle (2pt) ;
\draw [fill] (-2.5,-1) circle (2pt) ;
\draw [fill] (1.5,1) circle (2pt) ;
\draw [fill] (1,0) circle (2pt) ;

\node [above] at (1,2) {$\bm{X}_i$} ;

\node [red, above] at (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) {$P_{W_2}\bm{X}_i$} ;

\end{scope}

\begin{scope}[xshift=7.5cm, yshift=-7.5cm]
\def\aa{1.379517}
\def\bb{1.690254}

\draw [blue, very thick] plot [domain=-2.5:2.5] (\x, {\bb/\aa*\x}) ;
\node [above] at (2.5, {\bb/\aa*2.5}) {$W_0$} ;

\draw [dashed] (-3,0) -- (3,0) ;
\draw [dashed] (0,-3) -- (0,3) ;

\draw [dashed, red] (1,2) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-1,-2) --  (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-1*\aa/sqrt((\aa)^2+(\bb)^2)-2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-0.5,1) --  (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-0.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (0.5,-1) --  (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(0.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (-2.5,-1) --  (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(-2.5*\aa/sqrt((\aa)^2+(\bb)^2)-1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1.5,1) --  (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1.5*\aa/sqrt((\aa)^2+(\bb)^2)+1*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [dashed, red] (1,0) --  (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) ;
\draw [fill=red, red] (({1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+0*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) circle (2pt);

\draw [fill] (1,2) circle (2pt) ;
\draw [fill] (-1,-2) circle (2pt) ;
\draw [fill] (-0.5,1) circle (2pt) ;
\draw [fill] (0.5,-1) circle (2pt) ;
\draw [fill] (-2.5,-1) circle (2pt) ;
\draw [fill] (1.5,1) circle (2pt) ;
\draw [fill] (1,0) circle (2pt) ;

\node [above] at (1,2) {$\bm{X}_i$} ;

\node [red, right] at (({1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\aa/sqrt((\aa)^2+(\bb)^2)}, {(1*\aa/sqrt((\aa)^2+(\bb)^2)+2*\bb/sqrt((\aa)^2+(\bb)^2))*\bb/sqrt((\aa)^2+(\bb)^2)}) {$P_{W_0}\bm{X}_i$} ;

\end{scope}

\end{tikzpicture}
\end{center}
\caption{Principal component analysis when $p=2$ and $k=1$. $W=W_0$ yields the largest total variance of $\{P_{W}\bm{X}_1,\ldots,P_{W}\bm{X}_N\}$ (\red{red dots}).}
\label{pca figure}
\end{figure}

\begin{remark}
Let $X$ and $Y$ be as in Example \ref{example pca}. Since
$$\|X-Y\|_{HS}^2=\sum_{i=1}^N \|\bm{X}_i-\bm{Y}_i\|^2=\sum_{i=1}^N \|\bm{X}_i-P_W\bm{X}_i\|^2,$$ by (\ref{total relation}),
the total variance of $\{\bm{Y}_1,\ldots,\bm{Y}_N\}$ is maximized when the sum of the squares of the distances between $\bm{X}_i$ and $P_W\bm{X}_i$ is minimized. See Figure \ref{pca}.
\end{remark}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\draw [fill=yellow!30, yellow!30] (-5,-3) -- (-2,2) -- (5,3) -- (2,-2) --  cycle;

\node [above right] at (5,3) {$W$} ;
%\draw [->] (-4,0) -- (5,0) ;
%\node [above right] at (5,0) {$x$} ;
%\draw [->, domain=-2.2:3] plot (\x, {4*\x/3});
%\node [right] at (3,4*3/3) {$y$} ;
%\draw [->] (0,-3) -- (0,4.5) ;
%\node [above right] at (0,4.5) {$z$} ;

\draw [thick] (-2,4) -- (1.5,-3);
\node [above right] at (-2,4) {$W^\perp$};

\draw [blue, ->, dashed] (1,2) -- (1.5,1);
\draw [fill=black] (1,2) circle (2pt);
\draw [fill=red, red] (1.5,1) circle (2pt);
\node [above] at (1,2) {$\bm{X}_1$};
\node [right] at (1.5,1) {$\red{P_W\bm{X}_1}$};
\node [right] at (1.25,1.5) {\blue{$d_1$}} ;

\draw [blue, ->, dashed] (1.5,4) -- (3,1);
\draw [fill=black] (1.5,4) circle (2pt);
\draw [fill=red, red] (3,1) circle (2pt);
\node [above] at (1.5,4) {$\bm{X}_2$};
\node [right] at (3,1) {$\red{P_W\bm{X}_2}$};
\node [right] at (2.25,2.5) {\blue{$d_2$}} ;


\draw [blue, ->, dashed] (3,-3) -- (1.8,-0.6);
\draw [fill=black] (3,-3) circle (2pt);
\draw [fill=red, red] (1.8,-0.6) circle (2pt);
\node [right] at (3,-3) {$\bm{X}_3$};
\node [right] at (1.8,-0.6) {$\red{P_W\bm{X}_3}$};
\node [right] at (2.4,-1.8) {\blue{$d_3$}} ;

\draw [blue, ->, dashed] (-0.5,2) -- (0.3,0.4);
\draw [fill=black] (-0.5,2) circle (2pt);
\draw [fill=red, red] (0.3,0.4) circle (2pt);
\node [above] at (-0.5,2) {$\bm{X}_i$};
\node [right] at (0.3,0.4) {$\red{P_W\bm{X}_i}$};
\node [right] at (-0.1,1.2) {\blue{$d_i$}} ;

\draw [blue, ->, dashed] (-3.5,3.5) -- (-1.5,-0.5);
\draw [fill=black] (-3.5,3.5) circle (2pt);
\draw [fill=red, red] (-1.5,-0.5) circle (2pt);
\node [above] at (-3.5,3.5) {$\bm{X}_N$};
\node [left] at (-1.5,-0.5) {$\red{P_W\bm{X}_N}$};
\node [right] at (-2.5,1.5) {\blue{$d_N$}} ;

\node [below left] at (0,0) {$\bm{0}$};
\draw [fill=black] (0,0) circle (2pt);

\end{tikzpicture}
\end{center}
\caption{Principal component analysis when $p=3$ and $k=2$. The largest total variance of $\{P_{W}\bm{X}_1,\ldots,P_{W}\bm{X}_N\}$ is obtained when $\|X-\red{P_WX}\|_{HS}=\sqrt{\sum_{i=1}^N \blue{d_i}^2}$ is minimized.}
\label{pca}
\end{figure}

\begin{exercise}\label{singular exercises}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $\lambda$ and $\mu$ be distinct eigenvalues of a symmetric matrix $A$. Let $\vxi$ (respectively, $\veta$) be an eigenvector of $A$ corresponding to $\lambda$ (respectively, $\mu$). Show that $\langle \vxi,\veta \rangle=0$.\\
\textit{Hint}: Compare $\lambda\langle \vxi,\veta\rangle$ and  $\mu\langle \vxi,\veta\rangle$. You might want to use Problem \ref{transfer}.

\item\label{definite eigen} Let $\lambda$ be an eigenvalue of a positive definite (respectively, positive semidefinite) matrix $A$. Show that $\lambda > 0$ (respectively, $\lambda \geq 0$).

\item Let $A$ be an $n\times n$ positive semidefinite matrix and $c>0$ be a constant. Show that $A+cI_n$ is invertible. \\\textit{Hint}: Show that $0$ cannot be an eigenvalue of $A+cI_n$. You may want to use Exercise \ref{evev exer}.\ref{poly eigenvalue}.

\item\label{symmqf} Let $Q(\vx)=\vx^t A \vx$ be a quadratic form. Show that there is a \textit{symmetric} matrix $B$ such that $Q(\vx)=\vx^t B \vx$.\\\textit{Hint}: Since $Q(\vx)=\vx^t A \vx$ is a scalar, $Q(\vx)=(\vx^t A \vx)^t=\vx^t A^t \vx$.

\item Let $M=\left[\begin{array}{cr} 4-b & -2 \\ a & b \end{array}\right]$. Determine $a$ and $b$ so that $M$ is positive semidefinite.

\item\label{2by2posdefdet} Let $A=\left[\begin{array}{cc} a & b \\ b & c \end{array}\right]$. If $a>0$ and $ac-b^2>0$, show that $A$ is positive definite.

\item\label{gram equiv} Let $\bm{y}_1,\ldots,\bm{y}_n$ be linearly independent vectors in $\mathbb{R}^m$ (so $m\geq n$ by Exercise \ref{matalgexer}.\ref{npdep}). Let $\langle \cdot, \cdot \rangle$ denote the inner product in $\mathbb{R}^n$ and define $v_{ij}=\langle \bm{y}_i, \bm{y}_{j} \rangle$. Show that the matrix $V=[v_{ij}]$ is positive definite.

\item\label{compsymm} The goal of this exercise is to show that the compound symmetry\index{compound symmetry} structure of the variance-covariance matrix for longitudinal studies is positive definite. Let $0\leq \rho<1$. Show that $V=\left[\begin{array}{cccccc}1 & \rho & \rho & \rho & \cdots & \rho \\ \rho & 1 & \rho & \rho & \cdots & \rho  \\ \rho & \rho & 1 & \rho & \cdots & \rho \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho & \rho & \rho & \rho & \cdots & 1 \end{array}\right]$ is positive definite. 
\\\textit{Hint}: For $1\leq i \leq n$, define $\vy_i \in \mathbb{R}^{n+1}$ by
$$\vy_1=\frac{1}{\sqrt{1+\alpha^2}}\left[\begin{array}{c} \alpha \\ 1 \\ 0 \\0 \\ \vdots \\ 0  \end{array}\right], \vy_2=\frac{1}{\sqrt{1+\alpha^2}}\left[\begin{array}{c} \alpha \\ 0 \\ 1 \\0 \\ \vdots \\ 0  \end{array}\right],\cdots,\vy_n=\frac{1}{\sqrt{1+\alpha^2}}\left[\begin{array}{c} \alpha \\ 0 \\ 0 \\0 \\ \vdots \\ 1  \end{array}\right],$$
where $\alpha=\sqrt{\frac{\rho}{1-\rho}}$ so that $\frac{\alpha^2}{1+\alpha^2}=\rho$. Use Exercise \ref{singular exercises}.\ref{gram equiv}.

\item Show that $\begingroup\renewcommand*{\arraystretch}{1.3}\left[\begin{array}{rrr}1 & -\frac{2}{3} & -\frac{2}{3} \\ -\frac{2}{3} & 1 &  -\frac{2}{3} \\ -\frac{2}{3} & -\frac{2}{3} & 1  \end{array}\right]\endgroup$ is not positive definite. This shows that the condition $\rho\geq 0$ is indispensable in Exercise \ref{singular exercises}.\ref{compsymm}. \\\textit{Hint}: You may want to use Sylvester's Criterion.

\item Let $S$ be an $n\times n$ positive definite matrix. For $\vx,\vy\in \mathbb{R}^n$, define $[\vx,\vy]_S$ by $\langle \vx, S\vy\rangle$. Show that $[\cdot,\cdot]$ satisfies all properties in Theorem \ref{inner product properties}.\\
\textit{Hint}: You may want to use Theorem \ref{posdef charact} and Problem \ref{transfer}.

\item Let $S$ be a square matrix such that $S^2=S=S^t$. Show that $\rank(S)=tr(S)$.
\\\textit{Hint}: Consider a diagonalization of $S$. You may find Exercise \ref{evev exer}.\ref{proj 1 or 0} useful.

\item This problem is about the optimization of the \underline{Rayleigh quotient}\index{Rayleigh quotient}. Let $A$ be an $n\times n$ positive definite matrix with eigenvalues $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_n$.
\begin{enumerate}
\item Show that $\max\{\frac{\vx^t A\vx}{\vx^t\vx}: \vx\neq \veczero\}=\lambda_1$ and $\min\{\frac{\vx^t A\vx}{\vx^t\vx}: \vx\neq \veczero\}=\lambda_n$.
\item Let $B$ be an $n\times n$ positive definite matrix. Show that
$$\max\left\{\frac{\vx^t A\vx}{\vx^tB\vx}: \vx\neq \veczero\right\}$$
is the largest eigenvalue of $AB^{-1}$. \\\textit{Hint}: By Theorem \ref{posdef charact}.\ref{semi square}, there is a matrix $C$ such that $B=C^tC$ and it follows that
$$\max\left\{\frac{\vx^t A\vx}{\vx^tB\vx}: \vx\neq \veczero\right\}=\max\left\{\frac{\vy^t (C^{-1})^tAC^{-1}\vy}{\vy^t\vy}: \vy\neq \veczero\right\}.$$
Use part (a) above. You may want to use Problem \ref{ABBA same eigen} and Problem \ref{inverse transpose}.
\end{enumerate}

\item Let $A$ be an $m\times n$ matrix with $m\leq n$. Show that there are an $m\times n$ matrix $U$, an $n\times n$ orthogonal matrix $V$, and an $n\times n$ diagonal matrix $\Sigma$ such that $A=U\Sigma V^T$ and columns of $U$ are orthonormal (this factorization is called the \underline{reduced singular value decomposition}\index{reduced singular value decomposition} of $A$).

\end{enumerate}
\end{exercise}

\chapter{Multivariable Calculus} 

\section{Partial Derivatives and Interchange of Operations}

Let $f:\mathbb{R}^n\to \mathbb{R}$. $f$ is said to be \underline{continuous}\index{continuous} at $\bm{x}_0$ if for $\epsilon>0$, there exists $\delta>0$ such that $\|\bm{x}-\bm{x}_0\|<\delta$ implies that $|f(\bm{x})-f(\bm{x}_0)|<\epsilon$. Here $\|\bm{x}-\bm{x}_0\|$ denotes the distance between $\bm{x}$ and $\bm{x}_0$ in $\mathbb{R}^n$: see Definition \ref{rngeo} for more detail regarding the geometry of $\mathbb{R}^n$. \\\\
Let $\bm{x}=(x_1,\ldots,x_n)$. The \underline{partial derivative}\index{partial derivative} $\frac{\partial f}{\partial x_i}$ ($D_i f$ and $f_{x_i}$ are also used) of $f$ with respect to $x_i$ is a map from $\mathbb{R}^n$ to $\mathbb{R}$ defined by
$$\frac{\partial f}{\partial x_i}(\bm{x})=\lim_{h\to 0}\frac{f(x_1,\ldots,x_{i-1},x_i+h,x_{i+1},\ldots,x_n)-f(x_1,\ldots,x_i,\ldots,x_n)}{h}.$$
Roughly speaking, $\frac{\partial f}{\partial x_i}$ is obtained by differentiating $f$ with respect to the $i^{\text{th}}$ variable $x_i$, treating other variables as constants.\\
One can also consider higher order partial derivatives. For example,
\begin{align*}\frac{\partial^2 f}{\partial x_i\partial x_j}(\bm{x})&=\frac{\partial}{\partial x_i}\left(\frac{\partial f}{\partial x_j}\right)(\bm{x})\\
&=\lim_{h\to 0}\frac{\frac{\partial f}{\partial x_j}(x_1,\ldots,x_{i-1},x_i+h,x_{i+1},\ldots,x_n)-\frac{\partial f}{\partial x_j}(x_1,\ldots,x_i,\ldots,x_n)}{h}.\end{align*}
In general, the order of differentiation matters and it can happen that $\frac{\partial^2 f}{\partial x_i\partial x_j}\neq \frac{\partial^2 f}{\partial x_j\partial x_i}$. Under some mild conditions, however, we have that $\frac{\partial^2 f}{\partial x_i\partial x_j}= \frac{\partial^2 f}{\partial x_j\partial x_i}$, as the next result shows.

\begin{thm}\label{switch partials}
Let $f:\mathbb{R}^n\to \mathbb{R}$. If the second partial derivatives $\frac{\partial^2 f}{\partial x_j\partial x_i}$ and $\frac{\partial^2 f}{\partial x_i\partial x_j}$ are continuous, then $\frac{\partial^2 f}{\partial x_j\partial x_i}=\frac{\partial^2 f}{\partial x_i\partial x_j}$.
\end{thm}

In many applications (e.g. moment generating function), we need to differentiate a function defined by integration and a natural question arises: is the derivative of an integral the same as the integral of a derivative? The following result deals with differentiation under integral sign.

\begin{thm}[Leibniz Integral Rule]\index{Leibniz Integral Rule} Let $f(t,x)$ be a function such that $f_t(x)=f(t,x)$ is integrable for all $t$ and $\frac{\partial f}{\partial t}$ is continuous in $t$ for each $x$. Then
$$\frac{d}{dt}\int_{a(t)}^{b(t)}f(t,x)\,dx=f(t,b(t))b'(t)-f(t,a(t))a'(t)+\int_{a(t)}^{b(t)}\frac{\partial f}{\partial t}(t,x)\,dx.$$
In particular, if $a(t)=a$ and $b(t)=b$ are constant functions, then
\begin{equation}\label{leib} \frac{d}{dt}\int_{a}^{b}f(t,x)\,dx=\int_{a}^{b}\frac{\partial f}{\partial t}(t,x)\,dx.\end{equation}
\end{thm}

\begin{remark}
(\ref{leib}) is valid even when $a$ and/or $b$ is/are $\pm\infty$.
\end{remark}

\begin{example}
Let $X$ be a continuous random variable with the pdf $f$. For a real number $t$, define the \underline{moment generating function}\index{moment generating function} $\varphi_X(t)$ of $X$ by
\begin{equation}\label{def of mgf}\varphi_X(t)=E(e^{tX})=\int_{-\infty}^\infty e^{tx}f(x)\,dx.\end{equation}
Differentiating (\ref{def of mgf}) with respect to $t$ and applying Leibniz Integral Rule,
$$\left.\frac{d}{dt}\varphi_X(t)\right|_{t=0}=\left.\int_{-\infty}^\infty xe^{tx}f(x)\,dx\right|_{t=0}=\int_{-\infty}^\infty xf(x)\,dx=E(X).$$
Differentiating (\ref{def of mgf}) twice with respect to $t$, we get
$$\left.\frac{d^2}{dt^2}\varphi_X(t)\right|_{t=0}=\left.\int_{-\infty}^\infty x^2e^{tx}f(x)\,dx\right|_{t=0}=\int_{-\infty}^\infty x^2f(x)\,dx=E(X^2).$$ In general, it follows that
\begin{equation}\label{mgtderi}\left.\frac{d^n}{dt^n}\varphi_X(t)\right|_{t=0}=E(X^n).\end{equation}
\end{example}

\begin{remark}
The concept of the moment generating function extends to discrete random variables and (\ref{mgtderi}) is still valid provided that the term-by-term differentiation
\begin{equation}\label{sumtbt}\frac{d^n}{dt^n}\left(\sum_k e^{tk}P(X=k)\right)=\sum_k\frac{d^n}{dt^n}\left( e^{tk}P(X=k)\right)\end{equation}
is valid. In fact, under some conditions (which include uniform convergence of the right hand side of  (\ref{sumtbt})), one can show that (\ref{sumtbt}) is true.
\end{remark}

\begin{example}[See Example \ref{def of exp dist} and Exercise \ref{improperexer}.\ref{gammaexp}] Let $X\sim Exp(\lambda)$, then
$$\varphi_X(t)=\int_0^\infty e^{tx}\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\,dx=\frac{1}{\lambda}\int_0^\infty e^{(t-\frac{1}{\lambda})x}\,dx=\frac{1}{1-\lambda t} $$
for $t<\frac{1}{\lambda}$, so
$$\varphi_X'(0)=\left.\frac{\lambda}{(1-\lambda t)^2}\right|_{t=0}=\lambda\quad\text{and}\quad \varphi_X''(0)=\left.\frac{2\lambda^2}{(1-\lambda t)^3}\right|_{t=0}=2\lambda^2.$$
It follows that $E(X)=\lambda$ and $Var(X)=E(X^2)-E(X)^2=2\lambda^2-\lambda^2=\lambda^2$.
\end{example}

We close this section with a couple of results that justifies the interchange of the order of limit and integration.

\begin{thm}[Lebesgue's Dominated Convergence Theorem]\index{Lebesgue's Dominated Convergence Theorem}
Let $(f_n)$ be a sequence of functions on $[a,b]$. Suppose that there exists a function $g$ such that
\begin{enumerate}
\item $|f_n(x)|\leq g(x)$ for all $x\in [a,b]$ and $n$ and
\item $\displaystyle{\int_{a}^b} g(x)\,dx<\infty$.
\end{enumerate}
Then $$\lim_{n\to \infty}\int_a^b f_n(x)\,dx=\int_a^b \left(\lim_{n\to \infty}f_n(x)\right)\,dx.$$
\end{thm}

\begin{thm}[Lebesgue's Monotone Convergence Theorem]\index{Lebesgue's Monotone Convergence Theorem}
Let $(f_n)$ be a sequence of functions on $[a,b]$ such that $0\leq f_k(x) \leq f_{k+1}(x)$ for all $k$ and for all $x\in [a,b]$. Then $$\lim_{n\to \infty}\int_a^b f_n(x)\,dx=\int_a^b \left(\lim_{n\to \infty}f_n(x)\right)\,dx.$$
\end{thm}

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item Let $f(x,y)=\ln (x^2y+y^3)$. Compute $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$. Show that $\frac{\partial^2 f}{\partial x\partial y}=\frac{\partial^2 f}{\partial y\partial x}$.
\end{enumerate}
\end{exercise}

\section{Finding Extreme Values}
As in the case of one variable function, there is a multivariate Taylor series expansion. Before we proceed, we need a multivariate version of the first and second derivatives.

\begin{defi}
Let $f:\mathbb{R}^n\to \mathbb{R}$. The \underline{gradient}\index{gradient} $\nabla f$ of $f$ is a (row) $n$-vector defined by $(\frac{\partial f}{\partial x_1}(\bm{x}),\ldots,\frac{\partial f}{\partial x_n}(\bm{x}))$. The \underline{Hessian}\index{Hessian} $H(f)(\bm{x})$ of $f$ is an $n\times n$ matrix given by the Jacobian of $\nabla f$, that is,
$$H(f)(\bm{x})=\left[\begin{array}{cccc} \frac{\partial^2 f}{\partial x_1^2}(\bm{x}) & \frac{\partial^2 f}{\partial x_1 \partial x_2}(\bm{x}) & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n}(\bm{x}) \\ \frac{\partial^2 f}{\partial x_2\partial x_1}(\bm{x}) & \frac{\partial^2 f}{\partial x_2^2}(\bm{x}) & \cdots & \frac{\partial^2 f}{\partial x_2\partial x_n}(\bm{x}) \\ \vdots & \vdots & \ddots & \vdots \\  \frac{\partial^2 f}{\partial x_n\partial x_1}(\bm{x}) & \frac{\partial^2 f}{\partial x_n\partial x_2}(\bm{x}) & \cdots & \frac{\partial^2 f}{\partial x_n^2}(\bm{x}) \end{array}\right].$$
\end{defi}

\begin{remark}\quad
\begin{enumerate}
\item A point $\vx_0$ is called a \underline{critical point}\index{critical point} of $f$ if $\nabla f(\vx_0)=(0,0,\ldots,0)$.
\item By Theorem \ref{switch partials}, if the second partial  derivatives of $f$ are continuous, then $H(f)$ is symmetric (see Definition \ref{symmortho}).
\end{enumerate}
\end{remark}

\begin{thm}\label{multi taylor}
Let $f:\mathbb{R}^n\to \mathbb{R}$. If all second order partial derivatives of $f$ exist, then
$$f(\vx)=f(\vx_0)+\nabla f(\vx_0)(\vx-\vx_0)+\frac{1}{2}(\vx-\vx_0)^t Hf(\vx_1)(\vx-\vx_0)$$
for some point $\vx_1$ lying on the line segment that connects $\vx_0$ and $\vx$.
\end{thm}

\begin{remark}
Theorem \ref{multi taylor} can be viewed as a multivariate version of Taylor expansion:
$$f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_1)(x-x_0)^2$$
for some $x_1$ between $x_0$ and $x$.
\end{remark}

Now we are ready to introduce a multivariate version of the second derivative test.

\begin{thm}[Multivariate Second Derivative Test]\index{multivariate second derivative test}
Let $f:\mathbb{R}^n\to \mathbb{R}$ and suppose that $\nabla f(\vx_0)=(0,0,\ldots,0)$.
\begin{enumerate}
\item If $Hf(\vx_0)$ is positive definite, then $f$ achieves a local minimum at $\vx_0$.
\item If $Hf(\vx_0)$ is negative definite, then $f$ achieves a local maximum at $\vx_0$.
\end{enumerate}
\end{thm}

\begin{example}
Let $f(x,y)=x^3+y^3-3xy$. Since $\nabla f=(3x^2-3y, 3y^2-3x)$, we have two critical points $\vx_0=(0,0)$ and $\vx_1=(1,1)$. Since
$Hf(\vx)=\left[\begin{array}{cc} 6x & -3 \\ -3 & 6y \end{array}\right]$, we have
$Hf(\vx_0)=\left[\begin{array}{rr} 0 & -3 \\ -3 & 0 \end{array}\right]$ and $Hf(\vx_1)=\left[\begin{array}{rr} 6 & -3 \\ -3 & 6 \end{array}\right]$.
Using Problem \ref{pd easy criterion}, one can easily check that $Hf(\vx_0)$ is neither positive definite nor negative definite, while $Hf(\vx_1)$ is positive definite. Therefore, we conclude that $f$ has a local minimum at $(1,1)$.
\end{example}

\begin{example}\label{same with sdt}
Recall Example \ref{lsexample}. We will find $\beta_0$ and $\beta_1$ that minimize (\ref{lsformula}) using the Multivariate Second Derivative Test. Let $f(\beta_0,\beta_1)=\sum_{i=1}^n |y_i-(\beta_0+\beta_1 x_i)|^2=\sum_{i=1}^n (y_i-(\beta_0+\beta_1 x_i))^2$, then
$$\frac{\partial f}{\partial \beta_0}=\sum_{i=1}^n 2(y_i-(\beta_0+\beta_1 x_i))(-1)$$
and
$$\frac{\partial f}{\partial \beta_1}=\sum_{i=1}^n 2(y_i-(\beta_0+\beta_1 x_i))(-x_i),$$
so the equation$\nabla f=(0,0)$ gives (using the notations as in Example \ref{lsexample})
$$\begin{cases}n\bar{y}-n\beta_0-n\beta_1\bar{x} =0  \\ \sum_{i=1}^n x_iy_i-n\beta_0\bar{x}-\beta_1\sum_{i=1}^n x_i^2=0\end{cases},$$
which in turn yields $\beta_1=\frac{SXY}{SXX}$ and $\beta_0=\bar{y}-\frac{SXY}{SXX}\bar{x}$. To show that this is a local minimum, we compute $H(f)=\left[\begin{array}{cc} 2n & 2n\bar{x} \\ 2n\bar{x} & 2\sum_{i=1}^n x_i^2 \end{array}\right]$. Since $\sum_{i=1}^n x_i^2-n\bar{x}^2=\sum_{i=1}^n (x_i-\bar{x})^2$, by Exercise \ref{singular exercises}.\ref{2by2posdefdet}, $H(f)$ is easily shown to be positive definite.
\end{example}

\begin{exercise}\quad
\begin{enumerate}[\bfseries 1.]
\item For a real number $x$ and a positive real number $y$, define
$$f(x,y)=\frac{1}{y}e^{-\frac{(x-1)^2}{2y^2}}.$$
\begin{enumerate}
\item Compute the gradient vector $\nabla f$ and the Hessian matrix $H(f)$.
\item Find a local maximum of $f$.
\end{enumerate}

\end{enumerate}
\end{exercise}

\section{Matrix Differentiation}
In this section, we deal with differentiation that involves matrices. Just as the differentiation of a single variable function plays an important role in finding extreme values of the function, matrix differentiation gives a convenient way to compute the extreme values of a multivariable function, which is essential in multiple linear regressions. We start with a convention that will be used throughout the section.

\begin{defi}
Let $\vy=\vy(\vx)=\left[\begin{array}{c} y_1(\vx) \\ \vdots \\ y_m(\vx) \end{array}\right]$ such that each $y_i$ is a differentiable function of $\vx=\left[\begin{array}{c} x_1 \\ \vdots \\ x_n \end{array}\right]$. We define $\frac{\partial \vy}{\partial \vx}$ to be an $m\times n$ matrix given by
$$\frac{\partial \vy}{\partial \vx} = \left[\begin{array}{cccc} \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{array}\right].$$
In particular, if $m=1$, then $\frac{\partial y}{\partial \vx}$ is nothing but the gradient $\nabla y$ of $y$.
\end{defi}

It is easy to verify that the operator $\frac{\partial}{\partial \vx}$ is linear. To be more precise, if $\vy,\vz$ are $n\times 1$ vectors whose components are functions of $\vx$, and if $c$ is a constant, then
\begin{equation}\label{mat-linear}\frac{\partial (\vy+\vz)}{\partial \vx}=\frac{\partial \vy}{\partial \vx}+\frac{\partial \vz}{\partial \vx}\quad\text{and}\quad \frac{\partial (c\vy)}{\partial \vx}=c\frac{\partial \vy}{\partial \vx}.\end{equation}

\begin{example}\label{matdiffCR}
Let $A=[a_{ij}]$ be an $m\times n$ constant matrix and $\vx=\left[\begin{array}{c} x_1 \\ \vdots \\ x_n \end{array}\right]$. Let $\vy=A\vx$ so that $\vy$ is an $m\times 1$ vector with component $y_i(\vx)=\sum_{j=1}^na_{ij}x_j$, $1\leq j\leq m$. Since $\frac{\partial y_i}{\partial x_j}=a_{ij}$, we see that $\frac{\partial \vy}{\partial \vx} = A$.
\end{example}

\begin{problem}\label{xtranc}
Let $\bm{c}=\left[\begin{array}{c} c_1 \\ \vdots \\ c_n \end{array}\right]$ be an $n\times 1$ constant vector and $\vx=\left[\begin{array}{c} x_1 \\ \vdots \\ x_n \end{array}\right]$. Let $y=\vx^t\bm{c}$ (note that $y$ is a scalar). Compute $\frac{\partial y}{\partial \vx}$.
\end{problem}

\begin{answer} First we note that the result must be a $1\times n$ vector. Since $y=\sum_{j=1}^n c_jx_j$, we get $\frac{\partial y}{\partial x_j}=c_j$, it follows that $\frac{\partial y}{\partial \vx}=\bm{c}^t$.
\end{answer}

The next result can be regarded as a matrix version of the Chain Rule.

\begin{thm}
Let $\vy=A\vx$, where $A$ is an $m\times n$ constant matrix and $\vx$ is an $n\times 1$ vector. Let $\vz$ be an $\ell\times 1$ vector, then
$$\frac{\partial \vy}{\partial \vz}=A\frac{\partial \vx}{\partial \vz}.$$
\end{thm}

Next, we consider differentiation of a quadratic form.\index{quadratic form!differentiation}

\begin{thm}\label{Ainside}
Let $y=\vx^tA\vx$, where $A$ is an $n\times n$ constant matrix and $\vx$ is an $n\times 1$ vector. Then $\frac{\partial y}{\partial \vx}=\vx^t(A+A^t)$. In particular, if $A$ is symmetric, then $\frac{\partial y}{\partial \vx}=2\vx^tA$.
\end{thm}

\begin{problem}\label{normdiff}
Let $y=\|A\vx -\vb\|^2$, where $A$ is an $m\times n$ constant matrix and $\vx, \vb$ are $m\times 1$ vectors. Compute $\frac{\partial y}{\partial \vx}$.
\end{problem}

\begin{answer}
Note that $y=(A\vx-\vb)^t(A\vx-\vb)=\vx^tA^tA\vx-\vx^tA^t\vb-\vb^tA\vx+\vb^t\vb$.  By (\ref{mat-linear}), Example \ref{matdiffCR}, Problem \ref{xtranc}, and Theorem \ref{Ainside}, we obtain
\begin{align*}
\frac{\partial y}{\partial \vx}&=\frac{\partial (\vx^tA^tA\vx)}{\partial \vx}-\frac{\partial(\vx^tA^t\vb)}{\partial \vx}-\frac{\partial(\vb^tA\vx)}{\partial \vx}+\frac{\partial (\vb^t\vb)}{\partial \vx}\\
&=2\vx^tA^tA-\vb^t A-\vb^tA+\bm{0}\\
&=2\vx^tA^tA-2\vb^t A.
\end{align*}
\end{answer}

\begin{remark}
Least squares estimators for multiple linear regressions can be found using Problem \ref{normdiff}. To be more precise, it is known that $y=\|A\vx -\vb\|^2$ is maximized when $\frac{\partial y}{\partial \vx}=\bm{0}^t$. In addition, if $A^tA$ is invertible, then $\|A\vx -\vb\|^2$ is maximized when $\vx=(A^tA)^{-1}A^tb$. See Exercise \ref{matdiffexer}.\ref{diffcalc2}
\end{remark}

There is also a matrix version of the Product Rule.

\begin{thm}\label{matdiffPR}
Let $\vx,\vy$ be $n\times 1$ vectors and both $\vx$ and $\vy$ are functions of $\vz$. Let $w=\vy^t\vx$, then $\frac{\partial w}{\partial \vz}=\vx^t\frac{\partial \vy}{\partial \vz}+\vy^t\frac{\partial \vx}{\partial \vz}$.
\end{thm}

\begin{problem}
Let $w=\vy^tA\vx$, where $\vy$ is an $m\times 1$ vector, $A$ is an $m\times n$ constant matrix, and $\vx$ is an $n\times 1$ vector. If both $\vy$ and $\vx$ are functions of $\vz$, show that
$$\frac{\partial w}{\partial \vz}=\vx^tA^t\frac{\partial \vy}{\partial \vz}+\vy^tA\frac{\partial \vx}{\partial \vz}.$$
\end{problem}

\begin{answer}
Note that $w=(A^t\vy)^t\vx$. By Theorem \ref{matdiffPR},
$$\frac{\partial w}{\partial \vz}=\vx^t\frac{\partial (A^t\vy)}{\partial \vz}+(A^t\vy)^t\frac{\partial \vx}{\partial \vz}.$$ The result follows by Example \ref{matdiffCR}.
\end{answer}

\begin{exercise}\quad\label{matdiffexer}
\begin{enumerate}[\bfseries 1.]
\item Let $y=\vx^t\vx$, where $\vx$ is an $n\times 1$ vector. Show that $\frac{\partial y}{\partial \vx}=2\vx^t$.
\item Let $y=\vx^t\vx$, where $\vx$ is an $n\times 1$ vector. If $\vx$ is a function of $\vz$, show that $\frac{\partial y}{\partial \vz}=2\vx^t\frac{\partial \vx}{\partial \vz}$.
\item \label{diffcalc2} Let $A$ be an $m\times n$ constant matrix and $\bm{b}$ be an $m\times 1$ constant vector. Let $\vx$ be an $n\times 1$ vector and $y=\|A\vx-\bm{b}\|^2$. Show that $\frac{\partial y}{\partial \vx}=\bm{0}^t$ if and only if $A^tA\vx=A^tb$.\\
\textit{Hint}: See Problem \ref{normdiff}.
\item Let $A$ be an $m\times n$ constant matrix and $\bm{b}$ be an $m\times 1$ constant vector. Let $W$ be a constant $m\times m$ positive semidefinite matrix. Compute $\frac{\partial y}{\partial \vx}$, where
$$y=(A\vx-\vb)^tW(A\vx-\vb).$$
\textit{Hint}: You may want to use Problem \ref{normdiff} together with Theorem \ref{posdef charact}.
\end{enumerate}
\end{exercise}

\section{Double Integrals}
Suppose that a function $f:\mathbb{R}^2\to \mathbb{R}$ is nonnegative over a region $D$ in $\mathbb{R}^2$ (see Figure \ref{double integral}). The volume under the surface of $z=f(x,y)$ and above the region $D$ is denoted by $$\iint_D f(x,y)\,dxdy$$
and called the \underline{double integral of $f$ over $D$}\index{double integral}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]

\fill [fill=yellow!30] plot coordinates{(0.95,1.9) (1,-1) (4.5,-1) (4.5,2)} ;

\fill [fill=yellow] plot [smooth cycle] coordinates{(1,-1)(2.5,-0.5)(4.5,-1)(3.5,-2)(1.5,-2)};

\draw plot [smooth cycle] coordinates{(1,-1)(2.5,-0.5)(4.5,-1)(3.5,-2)(1.5,-2)};

\draw [->] (-2,0) -- (6,0) ;
\node [above right] at (6,0) {$x$} ;
\draw [->, domain=-1.5:3] plot (\x, {4*\x/3});
 \node [above right] at (3,4) {$y$} ;
\draw [->] (0,-2) -- (0,4.5) ;
\node [above right] at (0,4.5) {$z$} ;

\shade [ball color=yellow] plot [smooth cycle] coordinates{(1,2)(2.5,3)(4.5,2)(3.5,1)(1.5,1)};

\draw [dashed] plot [smooth cycle] coordinates{(1,2)(2.5,2.5)(4.5,2)(3.5,1)(1.5,1)};
\draw plot [smooth cycle] coordinates{(1,2)(2.5,3)(4.5,2)(3.5,1)(1.5,1)};

\draw [dashed] (0.95,1.9) -- (1,-1) ;
\draw [dashed] (4.5,2) -- (4.5,-1) ;
\draw [dashed] (3.5,1) -- (3.5,-2) ;
\draw [dashed] (1.5,1) -- (1.5,-2) ;
\draw [dashed] (2.5,-0.5) -- (2.5,2.5) ;

\node at (2.5,-1.2) {$D$} ;
\node at (4.5,3) {$z=f(x,y)$} ;
\end{tikzpicture}
\end{center}
\caption{Double integral as volume under surface $z=f(x,y)$}
\label{double integral}
\end{figure}

\begin{example}
For $R>0$, let $D=\{(x,y):x^2+y^2\leq R^2\}$. The function $f(x,y)=\sqrt{R^2-(x^2+y^2)}$ represents the upper hemisphere of radius $R$ centered at $(0,0,0)$ in $\mathbb{R}^3$ and $\displaystyle{\iint_D \sqrt{R^2-(x^2+y^2)}\,dxdy}$ denotes its volume over $D$ (see Figure \ref{hemisphere}). Therefore,  $$\iint_D \sqrt{R^2-(x^2+y^2)}\,dxdy=\frac{2}{3}\pi R^3.$$

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7, >=triangle 45]

\shade [ball color=yellow] (3,0) arc(0:180:3) -- (0,0) ellipse (3 and 1) -- cycle ;
\draw (3,0) arc(0:180:3) ;
\draw [dashed]  (0,0) ellipse (3 and 1) ;
%\draw [->] (-4,0) -- (5,0) ;
%\node [above right] at (5,0) {$x$} ;
%\draw [->] (-0.6,-2.4) -- (1,4) ;
%\node [above right] at (1,4) {$y$} ;
%\draw [->] (0,-2) -- (0,4.5) ;
%\node [above right] at (0,4.5) {$z$} ;

\draw [->] (-4,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->, domain=-1.5:2.5] plot (\x, {4*\x/3});
 \node [above right] at (2.5,4*2.5/3) {$y$} ;
\draw [->] (0,-2) -- (0,4.5) ;
\node [above right] at (0,4.5) {$z$} ;
\node [below right] at (3,0) {$R$} ;
\node [above left] at (0,3) {$R$} ;
\node [above right] at (2.5,1.5) {$z=\sqrt{R^2-(x^2+y^2)}$} ;
\end{tikzpicture}
\end{center}
\caption{Graph of $z=\sqrt{R^2-(x^2+y^2)}$}
\label{hemisphere}
\end{figure}
\end{example}

As in the case of definite integral of a single variable function, the double integral of function that has both positive and negative parts should be interpreted as \textit{signed} volume.

\begin{example}
When $f(x,y)=c$ is a constant function, $\displaystyle{\iint_D f(x,y)\,dxdy=c\cdot\text{area}(D)}$, where $\text{area}(D)$ denotes the area of the region $D$.
\end{example}

As stated in the next two theorems, under certain conditions, double integrals can be calculated using iterated integrals.

\begin{thm}[Tonelli's Theorem]\index{Tonelli's Theorem} If $f$ is nonnegative, then
$$\iint_{\mathbb{R}^2} f(x,y)\,dxdy=\int_{-\infty}^\infty \left(\int_{-\infty}^\infty f(x,y)\,dy\right)\,dx=\int_{-\infty}^\infty \left(\int_{-\infty}^\infty f(x,y)\,dx\right)\,dy.$$
\end{thm}

\begin{thm}[Fubini's Theorem]\index{Fubini's Theorem} If $\displaystyle{\iint_{\mathbb{R}^2} |f(x,y)|\,dxdy}<\infty$, then
$$\iint_{\mathbb{R}^2} f(x,y)\,dxdy=\int_{-\infty}^\infty \left(\int_{-\infty}^\infty f(x,y)\,dy\right)\,dx=\int_{-\infty}^\infty \left(\int_{-\infty}^\infty f(x,y)\,dx\right)\,dy.$$
\end{thm}

\begin{remark}
Since Tonelli's Theorem applies only to nonnegative functions, when dealing with the double integral of a function that has both positive and negative parts, one usually relies on Fubini's Theorem. When applying Fubini's Theorem, to show that $\displaystyle{\iint_{\mathbb{R}^2} |f(x,y)|\,dxdy}<\infty$, one can use Tonelli's Theorem. To be more precise, since $|f(x,y)|$ is nonnegative, by Tonelli's Theorem it follows that
$$\iint_{\mathbb{R}^2} |f(x,y)|\,dxdy=\int_{\mathbb{R}} \left(\int_{\mathbb{R}} |f(x,y)|\,dy\right)\,dx=\int_{\mathbb{R}} \left(\int_{\mathbb{R}} |f(x,y)|\,dx\right)\,dy.$$
So if one can show that one of $\displaystyle{\int_{\mathbb{R}} \left(\int_{\mathbb{R}} |f(x,y)|\,dy\right)\,dx}$ or $\displaystyle{\int_{\mathbb{R}} \left(\int_{\mathbb{R}} |f(x,y)|\,dx\right)\,dy}$ is finite, then the double integral $\displaystyle{\iint_{\mathbb{R}^2} f(x,y)\,dxdy}$ can be evaluated via iterated integrals.
\end{remark}

\begin{example}\label{rectangle double integral}
Consider $\displaystyle{\iint_D(3y+x)\,dxdy}$, where $D=\{(x,y): -3\leq x \leq 1, 1\leq y\leq 3\}$. For $(x,y)\in D$, we have $|3y+x|\leq 3|y|+|x|\leq 12$, so by Exercise \ref{double integral exercise}.\ref{double integral order}, we see that $\displaystyle{\iint_D|3y+x|\,dxdy\leq 12\cdot\text{area}(D)}$. By Fubini's Theorem, it follows that
$$\iint_D(3y+x)\,dxdy=\int_1^3\left(\int_{-3}^1 \left(3y+x\right)\,dx\right)dy =\int_1^3 (12y-4)\,dy=40.$$
\end{example}

\begin{problem}
Let $\lambda$ be a constant. Define $f:\mathbb{R}^2\to \mathbb{R}$ by
$$f(x,y)=\begin{cases} \lambda x, &0<x<y<1, \\ 0, &\text{otherwise}.\end{cases}$$
Determine $\lambda$ so that $\displaystyle{\iint_{\mathbb{R}^2} f(x,y)\,dxdy=1}$.
\end{problem}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=3, yscale=3, >=triangle 45]
\fill [fill=yellow] (0,1) -- (1,1) -- (0,0) --  cycle ;
\draw [->] (-0.5,0) -- (2,0) ;
\node [above right] at (2,0) {$x$} ;
\draw [->] (0,-0.5) -- (0,2) ;
\node [above right] at (0,2) {$y$} ;
\node [below right] at (0,0) {$O$} ;

\draw [dashed] (1,-0.5)--(1,1.5) ;
\draw [dashed] (-0.5,1)--(1.5,1) ;

\draw [dashed] (-0.5,-0.5) -- (1.5,1.5) ;
\node [right] at (1.5,1.5) {$y=x$};
\node [above left] at (0,1) {$1$};
\node [below right] at (1,0) {$1$};

\draw [red, ultra thick] (0,0.3) -- (0.3,0.3);
\node [left] at (0,0.3) {$y$};
\draw [red, dashed] (0.3,0) -- (0.3,0.3);
\node [below] at (0.3,0) {$\red{y}$};

\draw [dashed] (0.6,0) -- (0.6,0.6) ;
\draw [blue, ultra thick] (0.6,1) -- (0.6,0.6) ;

\node [below] at (0.6,0) {$x$};
\node [left] at (0,0.6) {$\blue{x}$};

\draw [blue, dashed] (0,0.6) -- (0.6,0.6);

\node at (0.25,0.75) {$D$};

\end{tikzpicture}
\end{center}
\caption{$D=\{(x,y): 0<x<y<1\}$}
\label{R2 triangle}
\end{figure}

\begin{answer}
Let $D=\{(x,y): 0<x<y<1\}$ (see Figure \ref{R2 triangle}). Since $f$ is zero outside $D$, $\displaystyle{\iint_{\mathbb{R}^2} f(x,y)\,dxdy=\iint_{D} f(x,y)\,dxdy= \lambda \iint_{D}x\,dxdy}$. Note that, in $D$, $y$ ranges from $0$ to $1$ and corresponding $x$ ranges from $\red{0}$ to $\red{y}$, so
$$\iint_{D}x\,dxdy=\int_0^1\left(\int_{\red{0}}^{\red{y}}x\,dx\right)\,dy=\int_0^1\frac{y^2}{2}\,dy=\left[\frac{y^3}{6}\right]_0^1=\frac{1}{6}$$ and it follows that $\lambda=1$. Note that we could compute $\displaystyle{\iint_{D}x\,dxdy}$ using the other iterated integrals. Since $x$ in $D$ ranges from $0$ to $1$ and corresponding $y$ ranges from $\blue{x}$ to $\blue{1}$, we get
$$\iint_{D}x\,dxdy=\int_0^1\left(\int_{\blue{x}}^{\blue{1}}x\,dy\right)\,dx=\int_0^1x(1-x)\,dx=\left[\frac{x^2}{2}-\frac{x^3}{3}\right]_0^1=\frac{1}{6}.$$
\end{answer}

\begin{problem}
Let $f:\mathbb{R}^2\to \mathbb{R}$ be defined by
$$f(x,y)=\begin{cases} 2e^{-x-y}, &0<x<y, \\ 0, &\text{otherwise}.\end{cases}$$
Compute $\displaystyle{\iint_D f(x,y)\,dxdy}$ and $\displaystyle{\iint_E f(x,y)\,dxdy}$, where
$$D=\{(x,y):y<1\}\quad\text{and}\quad E=\{(x,y):x<1\}.$$
\end{problem}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=2, yscale=2, >=triangle 45]

\fill [yellow!30] (0,0) -- (2,2) -- (0,2) -- cycle ;
\fill [yellow] (0,0) -- (1,1) -- (0,1) -- cycle ;
\draw [->] (-0.5,0) -- (2,0) ;
\draw [->] (0,-0.5) -- (0,2) ;
\draw [dashed] (0,0) -- (2,2) ;
\draw [dashed] (1,0) -- (1,1) -- (0,1) ;
\draw [blue, ultra thick] (0,0.7) -- (0.7,0.7) ;
\draw [red, ultra thick] (0.7,0.7) -- (0.7,1) ;
\draw [red, dashed] (0.7,0) -- (0.7,0.7) ;

\node[above] at (0.3,0.4)  {$\mathscr{D}$} ; 
\node[right] at (2,2)  {$y=x$} ; 
\node[above] at (1,1.6)  {$0<x<y$} ; 
\node[below] at (1,0)  {$1$} ;
\node[left] at (0,1)  {$1$} ;
\node[above right] at (0,2)  {$y$} ; 
\node[below left] at (0,0)  {$O$} ; 
\node[above right] at (2,0) {$x$};

\begin{scope}[xshift=4cm]
\fill [yellow!30] (0,0) -- (2,2) -- (0,2) -- cycle ;
\fill [yellow] (0,0) -- (1,1) -- (1,2) -- (0,2) -- cycle ;
\draw [->] (-0.5,0) -- (2,0) ;
\draw [->] (0,-0.5) -- (0,2) ;
\draw [dashed] (0,0) -- (2,2) ;
\draw [dashed] (1,0) -- (1,2) ;
\draw [red, ultra thick] (0.7,0.7) -- (0.7,2) ;
\draw [red, dashed] (0.7,0) -- (0.7,0.7) ;

\node[above] at (0.5,1.2)  {$\mathscr{E}$} ; 
\node[right] at (2,2)  {$y=x$} ; 
\node[below] at (1,0)  {$1$} ;
\node[above right] at (0,2)  {$y$} ; 
\node[below left] at (0,0)  {$O$} ; 
\node[above right] at (2,0) {$x$};
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Iterated integrals}
\label{inf triangles}
\end{figure}

\begin{answer}
Let $R=\{(x,y):0<x<y\}$, $\mathscr{D}=D\cap R$, and $\mathscr{E}=E\cap R$ (see Figure \ref{inf triangles}). Then 
$$\iint_D f(x,y)\,dxdy=\iint_{\mathscr{D}} f(x,y)\,dxdy=\iint_{\mathscr{D}} 2e^{-x-y}\,dxdy$$ 
and  
$$\iint_E f(x,y)\,dxdy=\iint_{\mathscr{E}} f(x,y)\,dxdy=\iint_{\mathscr{E}} 2e^{-x-y}\,dxdy.$$
Here
\begin{align*}
\iint_{\mathscr{D}} 2e^{-x-y}\,dxdy&=\int_0^1 \int_{\blue{0}}^{\blue{y}} 2e^{-x-y}\,dxdy\\
&=\int_0^1 2e^{-y}\left[\int_0^{y}e^{-x}\,dx\right]dy\\
&=\int_0^1 2e^{-y}(1-e^{-y})dy\\
&=\left[-2e^{-y}+e^{-2y}\right]_0^1\\
&=1-2e^{-1}+e^{-2}.
\end{align*}

Note that one could have used the other iterated integrals to compute $\displaystyle{\iint_{\mathscr{D}} 2e^{-x-y}\,dxdy}$, that is, 
\begin{align*}\iint_{\mathscr{D}} 2e^{-x-y}\,dxdy&=\int_0^1 \int_{\red{x}}^{\red{1}}2e^{-x-y}\,dydx\\
&=\int_0^1 2e^{-x}\left[\int_{x}^{1}e^{-y}\,dy\right]dx\\
&=\int_0^1 2e^{-x}(e^{-x}-e^{-1})dx\\
&=\left[-e^{-2x}+2e^{-1}e^{-x}\right]_0^1\\
&=e^{-2}+1-2e^{-1}.
\end{align*}

For the other integral, we have
\begin{align*} \iint_{\mathscr{E}} 2e^{-x-y}\,dxdy&=\int_0^1 \int_{\red{x}}^{\red{\infty}} 2e^{-x-y}\,dydx\\
&=\int_0^1 2e^{-x}\left[\int_{x}^\infty e^{-y}\,dy \right]dx\\
&=\int_0^1 2e^{-2x}dx\\
&=1-e^{-2}.
\end{align*}
\end{answer}

\begin{example} \label{area_under_roc}
Consider the area under the ROC curve (see Example \ref{sens_spec}) of a binary classification: Figure \ref{AUROC}. 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=5, yscale=5, >=triangle 45]
\draw [->] (-0.1,0) -- (1.1,0) ;
\node [above right] at (1.1,0) {$1-spec(T)$} ;
\draw [->] (0,-0.1) -- (0,1.1) ;
\node [above right] at (0,1.1) {$sens(T)$} ;
\fill[fill=yellow] (0,0) -- plot [domain=0:1, samples=80] (\x, {(1-(1-\x)^3)^(1/3)}) -- (1,0) -- cycle;
\draw [thick, blue, domain=0:1, samples=80] plot (\x, {(1-(1-\x)^3)^(1/3)}) ;
\draw [dashed] (1,0) -- (1,1) -- (0,1) ;
\node [below] at (1,0) {$1$} ;
\node [left] at (0,1) {$1$} ;
\end{tikzpicture}
\end{center}
\caption{The area under the ROC curve. It can be interpreted as the probability that the underlying binary classifier will give a randomly chosen positive instance a higher score than an independently chosen negative instance.}
\label{AUROC}
\end{figure}

With the same notation as in Example \ref{sens_spec}, it can be shown (area under a parametric curve) that the area under the ROC curve is given by
\begin{align*}&\int_{\infty}^{-\infty}sens(y)\frac{d}{dy}\left(1-spec(y)\right)\,dy\\
=&\int_{-\infty}^{\infty}\left(\int_y^\infty f_+(x)\,dx\right)f_-(y)\,dy\\
=&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} I_{x>y}(x,y) f_+(x)f_-(y)\,dx\,dy.
\end{align*}
Let $X$ be the score of an instance that belongs to \textit{positive} and $Y$ be the score of an instance that is independent of the former and belongs to \textit{negative}. Since the joint density of $X$ and $Y$ is $f_+(x)f_-(y)$, the probability that the score $X$ is greater than $Y$ is given by
$$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} I_{x>y}(x,y) f_+(x)f_-(y)\,dx\,dy.$$
That is, the area under the ROC curve can be interpreted as the probability that the underlying classifier will give a randomly chosen positive instance a higher score than an independently chosen negative instance.

\end{example}


Our next result can be viewed as a higher-dimensional analogue of Theorem \ref{1dcov}.

\begin{thm}[Change of Variables]\index{Change of Variables Formula} Let $\varphi:\mathscr{D}\subseteq \mathbb{R}^2 \to D\subseteq \mathbb{R}^2$ be a differentiable one-to-one map of $\mathscr{D}$ onto $D$ (see Figure \ref{changeofvariables}). Then
$$\iint_D f(x,y)\,dxdy=\iint_{\mathscr{D}}f\left(\varphi_1(u,v),\varphi_2(u,v)\right)\left|\det J_\varphi \right|\,dudv,$$
where $\det J_\varphi$ denotes the determinant (see Definition \ref{detdefi}) of the Jacobian matrix $\left[\begin{array}{rr}\frac{\partial \varphi_1}{\partial u} & \frac{\partial \varphi_1}{\partial v} \\ \frac{\partial \varphi_2}{\partial u} & \frac{\partial \varphi_2}{\partial v}\end{array}\right]$.
\end{thm}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill [fill=yellow] plot [smooth cycle] coordinates{(0.5,2)(2,3)(4,2)(3,1)(1,1)};
\draw plot [smooth cycle] coordinates{(0.5,2)(2,3)(4,2)(3,1)(1,1)};
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\node at (2,1.8) {$D=\varphi(\mathscr{D})$} ;
\draw [<-] (4.5,2.5) .. controls (6,3) .. (7.5,2.5);
\node [above] at (6,3) {$\varphi=(\varphi_1,\varphi_2)$} ;

\begin{scope}[xshift=8cm]
\fill [fill=cyan] plot [smooth cycle] coordinates{(1,1)(1.2,2)(1,3.5)(4,3)(3,1)};
\draw plot [smooth cycle] coordinates{(1,1)(1.2,2)(1,3.5)(4,3)(3,1)};
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$u$} ;
\draw [->] (0,-1) -- (0,5) ;
\node [above right] at (0,5) {$v$} ;
\node [below left] at (0,0) {$O$} ;
\node at (2.5,2.25) {$\mathscr{D}=\varphi^{-1}(D)$} ;
\end{scope}

\node [below] at (6,-1) {$\displaystyle{\iint_D f(x,y)\,dxdy=\iint_{\mathscr{D}}f\left(\varphi_1(u,v),\varphi_2(u,v)\right)\left|\det J_\varphi \right|\,dudv}$} ;
\end{tikzpicture}
\end{center}
\caption{Change of variables formula in $\mathbb{R}^2$}
\label{changeofvariables}
\end{figure}

\begin{example}
We compute $\displaystyle{\iint_D (2x+4y)\,dxdy}$, where $D$ is a rectangle with vertices at $(0,1)$, $(1,2)$, $(3,0)$, and $(2,-1)$ (see Figure \ref{rhombus}). It is natural to consider change of variables $u=y-x$ and $v=y+x$ so that $x=\varphi_1(u,v)=\frac{v-u}{2}$, $y=\varphi_2(u,v)=\frac{v+u}{2}$, and $D$ is mapped onto $\mathscr{D}$ under $\varphi^{-1}$, where $\mathscr{D}=\{(u,v):-3\leq u\leq 1, 1\leq v\leq 3\}$. Since $J_\varphi =\left[\begin{array}{rr} -\frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]$, it follows that $|\det J_\varphi|=|-\frac{1}{2}|=\frac{1}{2}$ and that
\begin{align*}\iint_D (2x+4y)\,dxdy&=\iint_{\mathscr{D}} \left(2\cdot\frac{v-u}{2}+4\cdot\frac{v+u}{2}\right)\,dudv \\
&=\iint_{\mathscr{D}} \left(3v+u\right)\,dudv \\
(\text{Example }\ref{rectangle double integral})&=40.\end{align*}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill [fill=yellow] (0,1) -- (1,2) -- (3,0) -- (2,-1) -- cycle ;
\draw [dashed] (-0.5,0.5) -- (0,1) ;
\draw [dashed] (-0.5,1.5) -- (0,1) ;
\draw [dashed] (0.5,2.5) -- (1,2) ;
\draw [dashed] (1.5,-1.5) -- (2,-1) ;
\draw [dashed] (3.5,-0.5) -- (3,0) ;
\node [right] at (3.5,-0.5) {$y+x=3$} ;
\draw [dashed] (2.5,-1.5) -- (2,-1) ;
\node [right] at (2.5,-1.5) {$y+x=1$} ;
\draw [dashed] (3,0) -- (3.5,0.5) ;
\node [right] at (3.5,0.5) {$y-x=-3$} ;
\draw [dashed] (1,2) -- (1.5,2.5) ;
\node [right] at (1.5,2.5) {$y-x=1$} ;
\draw (0,1) -- (1,2) -- (3,0) -- (2,-1) -- cycle ;
\draw [->] (-1,0) -- (6,0) ;
\node [above right] at (6,0) {$x$} ;
\draw [->] (0,-2) -- (0,4) ;
\node [above right] at (0,4) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\node at (1.5,0.5) {$D$} ;

\draw [<-] (4.5,2) .. controls (6,2.5) .. (7.5,2);
\node [above] at (6,2.5) {$\varphi=(\varphi_1,\varphi_2)$} ;

\begin{scope} [xshift=11cm]
\fill [fill=cyan] (-3,1) -- (1,1) -- (1,3) -- (-3,3) -- cycle ;
\draw (-3,1) -- (1,1) -- (1,3) -- (-3,3) -- cycle ;
\draw [->] (-4,0) -- (2,0) ;
\node [above right] at (2,0) {$u$} ;
\draw [->] (0,-2) -- (0,4) ;
\node [above right] at (0,4) {$v$} ;
\node [below left] at (0,0) {$O$} ;
\node at (-1,2) {$\mathscr{D}$} ;
\draw [dashed] (1,1) -- (1,0) ;
\node [below] at (1,0) {$1$} ;
\draw [dashed] (-3,1) -- (-3,0) ;
\node [below] at (-3,0) {$-3$} ;
\node [above left] at (0,3) {$3$} ;
\node [below left] at (0,1) {$1$} ;
\end{scope}

\end{tikzpicture}
\end{center}
\caption{Change of variables in $\mathbb{R}^2$}
\label{rhombus}
\end{figure}

\end{example}

\begin{example}
We compute $\displaystyle{\iint_D e^{x^2+y^2}\,dxdy}$, where $D$ is the region between two arcs as described in Figure \ref{ring}. With change of variables $x=\varphi_1(r,\theta)=r\cos\theta$ and $y=\varphi_2(r,\theta)=r\sin\theta$, $D$ is mapped onto $\mathscr{D}$ under $\varphi^{-1}$, where $\mathscr{D}=\{(r,\theta):1\leq r\leq 4, \frac{\pi}{6}\leq v\leq \frac{\pi}{3}\}$. Since $J_\varphi =\left[\begin{array}{rr} \cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta \end{array}\right]$, it follows that $|\det J_\varphi|=|r(\cos^2\theta+\sin^2\theta)|=r$ and that
\begin{align*}\iint_D e^{x^2+y^2}\,dxdy&=\iint_{\mathscr{D}}e^{r^2}r\,dr d\theta \\
&=\int_\frac{\pi}{6}^\frac{\pi}{3}\left(\int_1^4 re^{r^2}\,dr\right)d\theta \\
(\text{Integration by Substitution with }u=r^2)&=\int_\frac{\pi}{6}^\frac{\pi}{3} \frac{1}{2}\left(e^{16}-e\right)d\theta \\
&=\frac{\pi}{12}\left(e^{16}-e\right).
\end{align*}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill [fill=yellow] ({cos(30)},{sin(30)}) -- ({cos(30)},{sin(30)}) arc(30:60:1) -- ({4*cos(60)},{4*sin(60)}) -- ({4*cos(60)},{4*sin(60)}) arc(60:30:4)-- cycle ;
\draw [dashed] (1,0) arc(0:30:1) ;
\draw [dashed] (4,0) arc(0:30:4) ;
\node [below] at (1,0) {$1$} ;
\node [below] at (4,0) {$4$} ;
\draw [dashed] ({cos(60)},{sin(60)})-- (0,0) -- ({cos(30)},{sin(30)}) ;
\draw [dashed] ({4*cos(30)},{4*sin(30)}) -- ({5*cos(30)},{5*sin(30)}) ;
\draw [dashed] ({4*cos(60)},{4*sin(60)}) -- ({5*cos(60)},{5*sin(60)}) ;
\draw ({cos(30)},{sin(30)}) -- ({cos(30)},{sin(30)}) arc(30:60:1) -- ({4*cos(60)},{4*sin(60)}) -- ({4*cos(60)},{4*sin(60)}) arc(60:30:4)-- cycle ;
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\node [right] at ({5*cos(30)},{5*sin(30)}) {$\theta=\frac{\pi}{6}$} ;
\node [right] at ({5*cos(60)},{5*sin(60)}) {$\theta=\frac{\pi}{3}$} ;
\node at ({2.7*cos(45)},{2.7*sin(45)}) {$D$} ;

\draw [<-] (5,4) .. controls (6.5,4.5) .. (8,4);
\node [above] at (6.5,4.5) {$\varphi=(\varphi_1,\varphi_2)$} ;

\begin{scope}[xshift=9cm, yscale=2]
\fill [fill=cyan] (1,{pi/6}) -- (1,{pi/3}) -- (4,{pi/3}) -- (4,{pi/6}) -- cycle ;
\draw (1,{pi/6}) -- (1,{pi/3}) -- (4,{pi/3}) -- (4,{pi/6}) -- cycle ;
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$r$} ;
\draw [->] (0,-0.5) -- (0,2.5) ;
\node [above right] at (0,2.5) {$\theta$} ;
\node [below left] at (0,0) {$O$} ;
\node at (2.5,pi/4) {$\mathscr{D}$} ;
\draw [dashed]  (1,{pi/6}) -- (1,0) ;
\node [below] at (1,0) {$1$} ;
\draw [dashed] (4,{pi/6}) -- (4,0) ;
\node [below] at (4,0) {$4$} ;
\draw [dashed] (0,pi/6) -- (1,pi/6) ;
\node [left] at (0,pi/6) {$\frac{\pi}{6}$} ;
\draw [dashed] (0,pi/3) -- (1,pi/3) ;
\node [left] at (0,pi/3) {$\frac{\pi}{3}$} ;
\end{scope}
\end{tikzpicture}
\end{center}
\caption{Area between two arcs}
\label{ring}
\end{figure}

\end{example}

\begin{problem}\label{sector example}
Compute $\displaystyle{\iint_D e^{-(x^2+y^2)}\,dxdy}$, where $D$ is the part of the unit disc in the first quadrant (see Figure \ref{fan}).
\end{problem}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=3, yscale=3, >=triangle 45]
\fill [fill=yellow] (0,0) -- (1,0) arc(0:90:1) -- cycle ;
\node [below] at (1,0) {$1$} ;
\node [left] at (0,1) {$1$} ;
\draw (1,0) arc(0:90:1) ;
\draw [->] (-0.3,0) -- (1.3,0) ;
\node [above right] at (1.3,0) {$x$} ;
\draw [->] (0,-0.3) -- (0,1.3) ;
\node [above right] at (0,1.3) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\node at (0.4,0.4) {$D$} ;

\begin{scope}[xshift=2.5cm]
\fill [fill=yellow] (0,0) -- (1,0) arc(0:90:1) -- cycle ;
\fill [fill=white] (0,0) -- (0.2,0) arc(0:90:0.2) -- cycle ;
\node [below] at (1,0) {$1$} ;
\node [left] at (0,1) {$1$} ;
\draw (1,0) arc(0:90:1) ;
\draw (0.2,0) arc(0:90:0.2) ;
\node [below] at (0.2,0) {$\epsilon$} ;
\node [left] at (0,0.2) {$\epsilon$} ;
\draw [->] (-0.3,0) -- (1.3,0) ;
\node [above right] at (1.3,0) {$x$} ;
\draw [->] (0,-0.3) -- (0,1.3) ;
\node [above right] at (0,1.3) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\node at (0.4,0.4) {$D_\epsilon$} ;
\end{scope}
\end{tikzpicture}
\end{center}
\caption{A sector in the first quadrant}
\label{fan}
\end{figure}

\begin{answer} First we note that the change of variables $x=\varphi_1(r,\theta)=r\cos\theta$ and $y=\varphi_2(r,\theta)=r\sin\theta$ is \textit{not} one-to-one from $\mathscr{D}=\{(r,\theta): 0\leq r\leq 1, 0\leq \theta \leq \frac{\pi}{2}\}$ onto $D$, so we cannot apply the Change of Variables formula directly. However, we see that
$$\iint_D e^{-(x^2+y^2)}\,dxdy=\lim_{\epsilon\to 0}\iint_{D_\epsilon} e^{-(x^2+y^2)}\,dxdy,$$
where $D_\epsilon$ is a subset of $D$ constructed by taking a small sector around the origin from $D$ (see Figure \ref{fan}). Using the change of variables $x=\varphi_1(r,\theta)=r\cos\theta$ and $y=\varphi_2(r,\theta)=r\sin\theta$, we see that $\{(r,\theta):\epsilon \leq r \leq 1, 0\leq \theta \leq \frac{\pi}{2}\}$ is mapped onto $D_\epsilon$, so it follows that
$$\iint_{D_\epsilon} e^{-(x^2+y^2)}\,dxdy=\int_0^\frac{\pi}{2} \left(\int_\epsilon^1  e^{-r^2}rdr\right)d\theta=\frac{\pi}{4}\left(e^{-\epsilon^2}-e^{-1}\right)$$
and that $$\iint_D e^{-(x^2+y^2)}\,dxdy=\lim_{\epsilon\to 0}\frac{\pi}{4}\left(e^{-\epsilon^2}-e^{-1}\right)=\frac{\pi}{4}\left(1-e^{-1}\right).$$
\end{answer}

\begin{example}\label{liouville}
For $b>0$, let $\displaystyle{I(b)=\int_0^b e^{-x^2}\,dx}$. It is well known (Liouville's Theorem on Differential Algebra)\index{Liouville's Theorem} that the antiderivatives of $e^{-x^2}$ cannot be expressed in terms of elementary functions. By Tonelli's Theorem,
$$I(b)^2=\int_0^b e^{-x^2}\,dx \int_0^b e^{-y^2}\,dy=\iint_{[0,b]\times [0,b]}e^{-(x^2+y^2)}\,dxdy.$$
Note that $R_1\subseteq [0,b]\times [0,b] \subseteq R_2$, where $R_1$ and $R_2$ are sectors described in Figure \ref{sector}. By Exercise \ref{double integral exercise}.\ref{double integral order}, it follows that $$ \iint_{R_1}e^{-(x^2+y^2)}\,dxdy \leq I(b)^2 \leq \iint_{R_2}e^{-(x^2+y^2)}\,dxdy.$$ By Problem \ref{sector example}, we get
$$\iint_{R_1}e^{-(x^2+y^2)}\,dxdy =\int_0^\frac{\pi}{2}\int_0^{b}  e^{-r^2}rdrd\theta=\frac{\pi}{4}\left(1-e^{-b^2}\right).$$
Similarly,
$$\iint_{R_2}e^{-(x^2+y^2)}\,dxdy =\int_0^\frac{\pi}{2}\int_0^{\sqrt{2}b}  e^{-r^2}rdrd\theta=\frac{\pi}{4}\left(1-e^{-2b^2}\right)$$
and it follows that $\displaystyle{\lim_{b\to\infty}\iint_{R_1}e^{-(x^2+y^2)}\,dxdy =\lim_{b\to\infty}\iint_{R_2}e^{-(x^2+y^2)}\,dxdy =\frac{\pi}{4}}$. By Squeeze Theorem, we finally get $\displaystyle{\int_0^\infty e^{-x^2}\,dx=\lim_{b\to\infty}I(b)=\frac{\sqrt{\pi}}{2}}$.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1, >=triangle 45]
\fill [fill=green] (0,0) -- (3,0) arc(0:90:3) -- cycle ;
\fill [fill=yellow] (3,3) -- (3,0) arc(0:90:3) -- cycle ;
\fill [fill=cyan] (3,0) -- ({3*sqrt(2)},0) arc(0:90:{3*sqrt(2)}) -- (0,3) -- (3,3) -- cycle ;
\draw [->] (-1,0) -- (5,0) ;
\node [above right] at (5,0) {$x$} ;
\draw [->] (0,-1) -- (0,5) ;
\node [above right] at (0,5) {$y$} ;
\node [below left] at (0,0) {$O$} ;
\draw (3,0) arc(0:90:3) ;
\draw (3,0) -- (3,3) -- (0,3) ;
\draw ({3*sqrt(2)},0) arc(0:90:{3*sqrt(2)}) ;
\node [below] at (3,0) {$b$} ;
\node [below] at ({3*sqrt(2)},0) {$\sqrt{2} b$} ;
\node [left] at (0,3) {$b$} ;
\node [left] at (0,{3*sqrt(2)}) {$\sqrt{2} b$} ;
\node at (2.5,0.6) {$R_1$} ;
\node at (3.5,1) {$R_2$} ;
\end{tikzpicture}
\end{center}
\caption{Calculation of $\displaystyle{\int_0^\infty e^{-x^2}\,dx}$}
\label{sector}
\end{figure}

\end{example}

\begin{exercise} \label{double integral exercise} \quad
\begin{enumerate}[\bfseries 1.]

\item\label{double integral order} (see Exercise \ref{integral exercise}.\ref{integral order preserving}) Suppose that $f(x,y)\geq g(x,y)$ for all $(x,y)\in D$. Show that $\displaystyle{\iint_D f(x,y)\,dxdy\geq \iint_D g(x,y)\,dxdy}$.

\item Let $D$ be the region in $\mathbb{R}^2$ enclosed by two curves $y=x^2$ and $y=\sqrt{x}$. Compute $\displaystyle{\iint_D \sqrt{y}\,dxdy}$.

\item\label{normaldensity} Prove that $\displaystyle{\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\,dx=1}$. In general, prove that $\displaystyle{\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx=1}$ for all $\mu$ and $\sigma$, $\sigma>0$. This shows that the normal density is indeed a pdf.

\item Compute the double integral $\displaystyle{\int_0^\infty \int_{-\infty}^\infty (x^2+y^2)e^{-(x^2+y^2)}\,dxdy}$.

\item Show that $\Gamma(\frac{1}{2})=\sqrt{\pi}$ (see Example \ref{gammafcndef} for the definition of the gamma function). \\\textit{Hint}: Recall that $\Gamma(\frac{1}{2})=\displaystyle{\int_0^\infty x^{-\frac{1}{2}}e^{-x}\,dx}$. Use Integration by Substitution with $z=\sqrt{x}$, then the result follows from Example \ref{liouville}.

\item Compute $\displaystyle{\iint_D e^{3(x+y)}\,dxdy}$, where $D$ is the parallelogram with vertices at $(1,0)$, $(0,2)$, $(1,3)$, and $(2,1)$.
\end{enumerate}
\end{exercise}


\begin{thebibliography}{9}

\bibitem{lay}
  David C. Lay,
  \emph{Linear Algebra and Its Applications}.
  Addison Wesley, 3rd Edition, 2006.

\bibitem{HornJohnson}
  Roger A. Horn and Charles R. Johnson,
   \emph{Matrix Analysis},
   Cambridge University Press, 1990.

\end{thebibliography}

%\bibliography{jungjinthesisbib}
%\bibliographystyle{alpha}

\printindex

\end{document}
